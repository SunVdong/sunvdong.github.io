[{"categories":null,"content":"欢迎来到我的友情链接页面！以下是一些我推荐的网站和博客，如果你也有兴趣交换友链，请随时联系我！ ","date":"2025-03-03","objectID":"/links/:0:0","series":null,"tags":null,"title":"友情链接","uri":"/links/#"},{"categories":null,"content":" 我的信息 网站名称: 我的灵感记录 网站地址: https://www.vdong.xyz 网站描述: 这是一个关于互联网技术和生活的的博客，记录生活点滴，分享技术感受。 ","date":"2025-03-03","objectID":"/links/:1:0","series":null,"tags":null,"title":"友情链接","uri":"/links/#我的信息"},{"categories":null,"content":" 友情链接列表 帅东 一个PHP大牛~💤 帅强 嘴硬的人~ ","date":"2025-03-03","objectID":"/links/:2:0","series":null,"tags":null,"title":"友情链接","uri":"/links/#友情链接列表"},{"categories":null,"content":" 申请友链如果你希望与我交换友链，请确保你的网站符合以下条件： 网站内容健康、合法，且无恶意广告。 网站内容与我的网站主题相关或互补。 请在你的网站上也添加我的友链。 申请格式： 网站名称: 你的网站名称 网站地址: 你的网站地址 网站描述: 你的网站描述 头像/Logo: 你的网站Logo链接 请将你的信息发送至：sunvdong@qq.com 或在评论区留言，我会尽快处理！ 感谢你的支持与关注！希望我们能共同进步，分享更多有价值的内容！ ","date":"2025-03-03","objectID":"/links/:3:0","series":null,"tags":null,"title":"友情链接","uri":"/links/#申请友链"},{"categories":["技术"],"content":"nginx 与大多数应用程序一样，记录了大量与客户端交互、系统事件和潜在错误相关的数据。然而，只有通过正确的配置、管理和分析才能充分发挥这些数据的潜力。 ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:0:0","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#"},{"categories":["技术"],"content":" 找到日志与大多数 Web 服务器一样，Nginx 仔细地将其活动记录在两个不同的日志文件中： /var/log/nginx/access.log ：此文件记录每个传入请求，捕获关键详细信息，例如客户端的 IP 地址、请求的时间戳、请求的资源 (URI)、响应的 HTTP 状态代码以及客户端的用户代理（浏览器和操作）系统）。 /var/log/nginx/error.log : 该文件充当诊断工具，记录请求处理和其他 Nginx 操作期间遇到的错误和问题。它记录时间戳、错误级别、错误消息和任何相关上下文等信息，以帮助进行故障排除。 ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:1:0","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#找到日志"},{"categories":["技术"],"content":" 定位不同环境下的Nginx日志 Linux 发行版在大多数 Linux 发行版中，Nginx 日志文件通常位于/var/log/nginx/目录中。您会发现它们分别名为access.log和error.log。 如果您在默认位置找不到日志文件，则需要检查特定的 Nginx 配置。首先确定 Nginx 配置文件的位置（通常是/etc/nginx/nginx.conf ）： shell nginx -t 如果配置文件有效，此命令应输出配置文件的位置： shell ## output nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful 打开配置文件并查找error_log和access_log指令以查明它们各自的位置。 /etc/nginx/nginx.conf shell error_log /var/log/nginx/error.log http { . . . access_log /var/log/nginx/access.log; . . . } 默认情况下，Nginx 全局应用error_log指令，而access_log通常放置在http块中。 Docker容器由于 Docker 容器是短暂的，因此直接在容器中存储日志是不切实际的。官方 Nginx Docker 镜像通过创建从/var/log/nginx/access.log和/var/log/nginx/error.log到容器的标准输出 ( /dev/stdout ) 和标准错误 ( /dev/stderr的符号链接来解决这个问题/dev/stderr ) 分别流。这使得Docker 的日志记录机制能够收集和管理日志。 您可以在 Dockerfile 中找到相关行： mainline/debian/Dockerfile dockerfile . . . ln -sf /dev/stdout /var/log/nginx/access.log \\ \u0026\u0026 ln -sf /dev/stderr /var/log/nginx/error.log \\ nginx 容器运行后，访问相应的地址会生成一些日志，使用 docker logs 命令可以查看相应的日志，其中混合了访问日志和错误日志。 要仅查看访问日志，请将标准错误重定向到/dev/null ： shell docker logs -f nginx-server 2\u003e/dev/null 同样，要仅查看错误日志，请将标准输出重定向到/dev/null ： shell docker logs -f nginx-server 1\u003e/dev/null ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:1:1","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#定位不同环境下的nginx日志"},{"categories":["技术"],"content":" 定位不同环境下的Nginx日志 Linux 发行版在大多数 Linux 发行版中，Nginx 日志文件通常位于/var/log/nginx/目录中。您会发现它们分别名为access.log和error.log。 如果您在默认位置找不到日志文件，则需要检查特定的 Nginx 配置。首先确定 Nginx 配置文件的位置（通常是/etc/nginx/nginx.conf ）： shell nginx -t 如果配置文件有效，此命令应输出配置文件的位置： shell ## output nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful 打开配置文件并查找error_log和access_log指令以查明它们各自的位置。 /etc/nginx/nginx.conf shell error_log /var/log/nginx/error.log http { . . . access_log /var/log/nginx/access.log; . . . } 默认情况下，Nginx 全局应用error_log指令，而access_log通常放置在http块中。 Docker容器由于 Docker 容器是短暂的，因此直接在容器中存储日志是不切实际的。官方 Nginx Docker 镜像通过创建从/var/log/nginx/access.log和/var/log/nginx/error.log到容器的标准输出 ( /dev/stdout ) 和标准错误 ( /dev/stderr的符号链接来解决这个问题/dev/stderr ) 分别流。这使得Docker 的日志记录机制能够收集和管理日志。 您可以在 Dockerfile 中找到相关行： mainline/debian/Dockerfile dockerfile . . . ln -sf /dev/stdout /var/log/nginx/access.log \\ \u0026\u0026 ln -sf /dev/stderr /var/log/nginx/error.log \\ nginx 容器运行后，访问相应的地址会生成一些日志，使用 docker logs 命令可以查看相应的日志，其中混合了访问日志和错误日志。 要仅查看访问日志，请将标准错误重定向到/dev/null ： shell docker logs -f nginx-server 2\u003e/dev/null 同样，要仅查看错误日志，请将标准输出重定向到/dev/null ： shell docker logs -f nginx-server 1\u003e/dev/null ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:1:1","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#linux-发行版"},{"categories":["技术"],"content":" 定位不同环境下的Nginx日志 Linux 发行版在大多数 Linux 发行版中，Nginx 日志文件通常位于/var/log/nginx/目录中。您会发现它们分别名为access.log和error.log。 如果您在默认位置找不到日志文件，则需要检查特定的 Nginx 配置。首先确定 Nginx 配置文件的位置（通常是/etc/nginx/nginx.conf ）： shell nginx -t 如果配置文件有效，此命令应输出配置文件的位置： shell ## output nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful 打开配置文件并查找error_log和access_log指令以查明它们各自的位置。 /etc/nginx/nginx.conf shell error_log /var/log/nginx/error.log http { . . . access_log /var/log/nginx/access.log; . . . } 默认情况下，Nginx 全局应用error_log指令，而access_log通常放置在http块中。 Docker容器由于 Docker 容器是短暂的，因此直接在容器中存储日志是不切实际的。官方 Nginx Docker 镜像通过创建从/var/log/nginx/access.log和/var/log/nginx/error.log到容器的标准输出 ( /dev/stdout ) 和标准错误 ( /dev/stderr的符号链接来解决这个问题/dev/stderr ) 分别流。这使得Docker 的日志记录机制能够收集和管理日志。 您可以在 Dockerfile 中找到相关行： mainline/debian/Dockerfile dockerfile . . . ln -sf /dev/stdout /var/log/nginx/access.log \\ \u0026\u0026 ln -sf /dev/stderr /var/log/nginx/error.log \\ nginx 容器运行后，访问相应的地址会生成一些日志，使用 docker logs 命令可以查看相应的日志，其中混合了访问日志和错误日志。 要仅查看访问日志，请将标准错误重定向到/dev/null ： shell docker logs -f nginx-server 2\u003e/dev/null 同样，要仅查看错误日志，请将标准输出重定向到/dev/null ： shell docker logs -f nginx-server 1\u003e/dev/null ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:1:1","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#docker容器"},{"categories":["技术"],"content":" 配置 Nginx 访问日志默认情况下，Nginx 访问日志以 combined 格式生成，无特殊说明时，该格式的定义为： nginx '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\"'; 此配置会生成类似于以下内容的访问日志条目： shell 172.17.0.1 - - [06/Aug/2024:16:37:59 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0 (X11; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0\" \"-\" 172.17.0.1 ：提出请求的客户端的IP地址。 -：如果使用身份验证，则这是经过身份验证的用户名；否则，它是连字符 (-)。 [06/Aug/2024:16:37:59 +0000] ：处理请求的本地时间。 \"GET / HTTP/1.1\" ：请求方法、路径和HTTP协议版本。 200 ：返回给客户端的HTTP状态码。 615 ：响应正文的大小（以字节为单位）。 \"-\" ：引用页面的 URL（如果有）。 \"Mozilla/5.0 (X11; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0\" : 客户端提供的浏览器和操作系统信息。 \"-\" ：如果请求通过代理，该变量包含原始客户端IP地址。 您可以使用主 Nginx 配置文件 ( /etc/nginx/nginx.conf ) 中的 log_format 指令或/etc/nginx/sites-enabled中特定于主机的配置中的log_format指令来定制访问日志格式。 如果 Nginx 直接在您的主机上运行，您可以相应地编辑相关文件。对于 Docker 实例，执行以下命令从nginx映像中提取 Nginx 配置文件并将其保存到主机上的nginx.conf文件中： shell docker run --rm --entrypoint=cat nginx /etc/nginx/nginx.conf \u003e nginx.conf 准备好再次启动容器后，请确保将修改后的文件从主机挂载到容器内的/etc/nginx/nginx.conf shell docker run --name nginx-server -v ./nginx.conf:/etc/nginx/nginx.conf:ro -d nginx ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:2:0","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#配置-nginx-访问日志"},{"categories":["技术"],"content":" 自定义访问日志格式可以使用 log_format 指令来自定义访问日志的格式： nginx log_format \u003cname\u003e '\u003cformatting_variables\u003e'; 你需要做的是设置一个格式名，然后使用提供的核心变量和日志变量设置日志的结构，下面是它的示例： nginx . . . http { . . . log_format custom '$remote_addr - $remote_user [$time_local] $status ' '\"$host\" \"$request\" $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; . . . } 要应用此格式，只需修改配置文件中的 access_log 指令： nginx access_log /var/log/nginx/access.log custom; ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:2:1","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#自定义访问日志格式"},{"categories":["技术"],"content":" 设置记录日志的条件大流量下，Nginx 的访问日志可能变得相当大，条件日志记录允许您根据特定条件有选择地过滤日志条目，以减少日志量并提高性能。 语法如下： shell access_log /path/to/access.log \u003clog_format\u003e if=\u003ccondition\u003e; \u003ccondition\u003e是 Nginx 对每个请求进行计算的布尔表达式。如果计算结果为true ，则写入日志条目；否则，它会被跳过。 以下示例演示如何从访问日志中排除成功 (2xx) 和重定向 (3xx) 状态代码的日志： nginx http { map $status $loggable { ~^[23] 0; # Match 2xx and 3xx status codes default 1; # Log everything else } access_log /var/log/nginx/access.log combined if=$loggable; } 一些实际应用： 仅记录错误响应（4xx 和 5xx）以进行故障排除。 排除已知为机器人的特定用户代理或 IP 地址。 仅记录对应用程序特定部分的请求。 记录一定百分比的请求以降低记录成本，同时仍然捕获代表性样本（请参阅此处了解一些采样技术）。 ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:2:2","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#设置记录日志的条件"},{"categories":["技术"],"content":" 禁用访问日志如果您已经通过 Web 应用程序收集请求日志，则可能需要使用特殊的off值或重定向到/dev/null来禁用 Nginx 访问日志。 nginx access_log off; access_log /dev/null; ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:2:3","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#禁用访问日志"},{"categories":["技术"],"content":" 构建Nginx访问日志在云原生分布式系统和微服务领域，结构化日志记录因其相对于传统纯文本日志的众多优势而获得了巨大的关注。 例如， Caddy （Nginx 的替代品）会生成如下所示的访问日志： json { \"level\": \"info\", \"ts\": 1646861401.5241024, \"logger\": \"http.log.access\", \"message\": \"handled request\", \"request\": { \"remote_ip\": \"127.0.0.1\", \"remote_port\": \"41342\", \"client_ip\": \"127.0.0.1\", \"proto\": \"HTTP/2.0\", \"method\": \"GET\", \"host\": \"localhost\", \"uri\": \"/\", \"headers\": { \"User-Agent\": [\"curl/7.82.0\"], \"Accept\": [\"*/*\"], \"Accept-Encoding\": [\"gzip, deflate, br\"], }, \"tls\": { \"resumed\": false, \"version\": 772, \"cipher_suite\": 4865, \"proto\": \"h2\", \"server_name\": \"example.com\" } }, \"bytes_read\": 0, \"user_id\": \"\", \"duration\": 0.000929675, \"size\": 10900, \"status\": 200, \"resp_headers\": { \"Server\": [\"Caddy\"], \"Content-Encoding\": [\"gzip\"], \"Content-Type\": [\"text/html; charset=utf-8\"], \"Vary\": [\"Accept-Encoding\"] } } 让我们探讨一下如何将 Nginx 访问日志带入这个现代时代。 虽然 Nginx 本身并不生成 JSON 日志，但您可以用log_format指令与escape=json参数（确保正确转义 JSON 中无效的字符）来实现此目的。 nginx . . . http { . . . log_format custom_json escape=json '{' '\"level\":\"info\",' '\"ts\": \"$time_iso8601\",' '\"message\": \"handled request $request_method $request_uri\",' '\"request\": {' '\"id\": \"$http_x_request_id\",' '\"remote_ip\": \"$remote_addr\",' '\"remote_port\": \"$remote_port\",' '\"protocol\": \"$server_protocol\",' '\"method\": \"$request_method\",' '\"host\": \"$host\",' '\"uri\": \"$request_uri\",' '\"headers\": {' '\"user-agent\": \"$http_user_agent\",' '\"accept\": \"$http_accept\",' '\"accept-encoding\": \"$http_accept_encoding\",' '\"traceparent\": \"$http_traceparent\",' '\"tracestate\": \"$http_tracestate\"' '}' '},' '\"bytes_read\": $request_length,' '\"duration_msecs\": $request_time,' '\"size\": $bytes_sent,' '\"status\": $status,' '\"resp_headers\": {' '\"content_length\": \"$sent_http_content_length\",' '\"content_type\": \"$sent_http_content_type\"' '}' '}'; access_log /var/log/nginx/access.log custom_json; . . . } 在此配置中： 我们定义了一种名为custom_json的新日志格式，并使用escape=json启用 JSON 转义 在 JSON 结构中，我们捕获各种信息： 基本信息：例如日志级别、时间戳 (ts) 和消息。 请求信息：嵌套在请求对象下的详细请求信息，包括使用$http_\u003cheader_name\u003e的标头。 指标信息：读取字节数、响应时间和响应大小等指标。 响应信息：如状态、特定响应标头像$sent_http_\u003cheader_name\u003e 最后，将custom_json格式应用于访问日志。 保存配置并重新启动 Nginx 后，使用一些虚构的分布式跟踪标头发出请求： shell curl -v -H \"traceparent: 00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01\" \\ -H \"tracestate: rojo=00f067aa0ba902b7\" \\ -H \"X-Request-Id: f45a82a7-7066-40d4-981d-145952c290f8\" \\ http://localhost 您将观察到干净的 JSON 格式的新访问日志条目： json { \"level\": \"info\", \"ts\": \"2024-08-07T11:57:31+00:00\", \"message\": \"handled request GET /\", \"request\": { \"id\": \"f45a82a7-7066-40d4-981d-145952c290f8\", \"remote_ip\": \"172.17.0.1\", \"remote_port\": \"39638\", \"protocol\": \"HTTP/1.1\", \"method\": \"GET\", \"host\": \"localhost\", \"uri\": \"/\", \"headers\": { \"user-agent\": \"curl/8.6.0\", \"accept\": \"*/*\", \"accept-encoding\": \"\", \"traceparent\": \"00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01\", \"tracestate\": \"rojo=00f067aa0ba902b7\" } }, \"bytes_read\": 229, \"duration_msecs\": 0.000, \"size\": 853, \"status\": 200, \"resp_headers\": { \"content_length\": \"615\", \"content_type\": \"text/html\" } } ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:3:0","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#构建nginx访问日志"},{"categories":["技术"],"content":" 配置Nginx错误日志Nginx 错误日志是诊断和解决 Web 服务器问题的重要工具。它捕获各种 Nginx 操作期间发生的错误、警告和其他重要事件。让我们探讨如何配置和管理这个宝贵的资源。 error_log指令控制 Nginx 的错误日志记录行为。它接受两个参数：日志文件的路径和日志的最低严重级别。 shell error_log /var/log/nginx/error.log \u003cseverity_level\u003e; 严重级别按照从低到高分为以下几种： debug ：高度详细的消息，主要用于故障排除和开发。 info ：有关服务器操作的一般信息消息。 notice ：值得注意的事件，但不一定是错误。 warn ：可能表明潜在问题的意外事件。 error ：处理过程中遇到的实际错误。 crit ：需要注意的关键情况。 alert ：需要立即采取措施的错误。 emerg ：导致系统无法使用的严重错误。 如果您没有在 Nginx 配置中显式配置错误严重性级别，您将看到error级别及其之上的所有级别（ crit 、 alert和emerg ）的消息。然而，官方 Nginx docker 镜像的默认级别设置为notice 。 ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:4:0","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#配置nginx错误日志"},{"categories":["技术"],"content":" Nginx 错误日志格式 Nginx 错误日志遵循为人类可读性和易于工具解析而设计的格式。一般格式为： nginx YYYY/MM/DD HH:MM:SS [\u003cseverity_level\u003e] \u003cpid\u003e#\u003ctid\u003e: *\u003ccid\u003e \u003cmessage\u003e 这里： \u003cpid\u003e ：进程 ID \u003ctid\u003e : 线程 ID \u003ccid\u003e ：连接 ID 示例： nginx 2024/08/07 17:41:58 [error] 29#29: *1 open() \"/usr/share/nginx/html/make\" failed (2: No such file or directory), client: 172.17.0.1, server: localhost, request: \"GET /make HTTP/1.1\", host: \"localhost\" ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:4:1","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#nginx-错误日志格式"},{"categories":["技术"],"content":" 将错误记录到多个文件与访问日志类似，您可以配置 Nginx 将错误记录到多个文件，即使具有不同的严重级别： shell error_log /var/log/nginx/error.log info; error_log /var/log/nginx/emerg_error.log emerg; 在此设置中，除调试级别消息之外的所有事件都将记录到error.log中，而紧急事件将记录到名为emerg_error.log的单独文件中。 ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:4:2","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#将错误记录到多个文件"},{"categories":["技术"],"content":" 禁用错误日志如果您需要完全禁用 Nginx 错误日志（尽管通常不推荐），您可以将其重定向到/dev/null 。在撰写本文时似乎没有特殊的off值。 shell error_log /dev/null; ","date":"2024-11-12","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/:4:3","series":null,"tags":["nginx"],"title":"Nginx日志指南","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E6%8C%87%E5%8D%97/#禁用错误日志"},{"categories":["技术"],"content":"保留提交记录和tag的git仓库迁移","date":"2024-07-16","objectID":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/","series":null,"tags":["git"],"title":"保留提交记录和tag的git仓库迁移","uri":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/"},{"categories":["技术"],"content":" 问题场景1将 Git 仓库从一个服务器迁移到另一个服务器，并保留所有的分支和提交记录。例如：我有一个git仓库（1.1.1.5:8888/a.git），有多个分支且做了多次提交了，现在需要通过u盘迁移至另一个仓库(2.2.2..5:8888/b.git)，我应该怎么操作呢。 ","date":"2024-07-16","objectID":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/:1:0","series":null,"tags":["git"],"title":"保留提交记录和tag的git仓库迁移","uri":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/#问题场景1"},{"categories":["技术"],"content":" 操作步骤 克隆源仓库到本地 git git clone --mirror http://1.1.1.5:8888/a.git 使用 --mirror 参数会确保你克隆的是镜像仓库，包括所有的分支、标签和提交记录。 将仓库复制到目标服务器 在目标服务器上初始化新的仓库 git cd a.git git remote set-url origin http://2.2.2.5:8888/b.git git push --mirror git remote set-url origin 命令将远程仓库的地址更改为新的地址。git push --mirror 命令会将所有的分支、标签和提交记录推送到新的远程仓库。 ","date":"2024-07-16","objectID":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/:2:0","series":null,"tags":["git"],"title":"保留提交记录和tag的git仓库迁移","uri":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/#操作步骤"},{"categories":["技术"],"content":" 问题场景2一个项目中有多个 git 仓库，我希望合并到一起，一个 git 仓库一个目录， 同时我希望保留提交记录和tags 等。 ","date":"2024-07-16","objectID":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/:3:0","series":null,"tags":["git"],"title":"保留提交记录和tag的git仓库迁移","uri":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/#问题场景2"},{"categories":["技术"],"content":" 操作步骤 在源机器上 git git clone --bare https://example.com/repo1.git repo1.git git clone --bare https://example.com/repo2.git repo2.git cp -r repo1.git /path/to/usb-drive/ cp -r repo2.git /path/to/usb-drive/ 在目标机器上 git cp -r /path/to/usb-drive/repo1.git /path/to/temp-directory/ cp -r /path/to/usb-drive/repo2.git /path/to/temp-directory/ mkdir combined-repo cd combined-repo git init git remote add -f repo1 /path/to/temp-directory/repo1.git git fetch repo1 git remote add -f repo2 /path/to/temp-directory/repo2.git git fetch repo2 git checkout -b repo1-master repo1/master git checkout master git merge --allow-unrelated-histories repo1-master mkdir repo1 git mv * repo1/ git commit -m \"Move repo1 content to repo1 directory\" git checkout -b repo2-master repo2/master git checkout master git merge --allow-unrelated-histories repo2-master mkdir repo2 git mv * repo2/ git commit -m \"Move repo2 content to repo2 directory\" 这样，你就可以通过U盘将多个仓库合并到一个目标仓库中，并保留所有的提交记录和标签。 ","date":"2024-07-16","objectID":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/:4:0","series":null,"tags":["git"],"title":"保留提交记录和tag的git仓库迁移","uri":"/posts/%E4%BF%9D%E7%95%99%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E5%92%8Ctag%E7%9A%84git%E4%BB%93%E5%BA%93%E8%BF%81%E7%A7%BB/#操作步骤-1"},{"categories":["技术"],"content":"使用文件的inode号码删除无法打出名字的文件","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/"},{"categories":["技术"],"content":"在操作 UNIX和Linux系统时，偶尔由于误操作等会出现一些无法打出文件名（例如文件名包含非法字符）的文件，由于无法打出名字，所以不容易使用 rm 命令将其删除。 ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:0:0","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#"},{"categories":["技术"],"content":" 解决方案这是我们可以使用文件的inode号码将其删除，具体操作如下: 找到文件的inode号码 使用 ls -i 命令查看文件的inode号码。例如： sh ls -i 输出结果会显示文件的inode号码，例如： sh 1234567 文件名 使用inode号码删除文件 使用 find 命令和inode号码删除文件。例如： sh find . -inum 1234567 -delete ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:1:0","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#解决方案"},{"categories":["技术"],"content":" 原理在UNIX和Linux文件系统中，inode（索引节点）是一个数据结构，用于存储文件的元数据。每个文件和目录在文件系统中都有一个唯一的inode。 当 unix 创建文件时，unix 做了两件事。 Unix 在存储设备上保留一块空间用来存储数据。 Unix 创建一个**索引节点（index node）**或 **i 节点（i-node）**的结构。用来存放文件的基本信息（包括步骤1中指向存储数据空间的指针）。 ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:2:0","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#原理"},{"categories":["技术"],"content":" inode的组成部分具体来讲，inode包含了文件的以下信息： 文件类型：如普通文件、目录、符号链接等。 文件权限：包括读、写、执行权限。 文件所有者（用户ID）。 文件所属组（组ID）。 文件大小。 文件的时间戳： 创建时间（ctime） 最后一次修改时间（mtime） 最后一次访问时间（atime） 链接计数：硬链接到该文件的数量。 指向数据块的指针：存储文件内容的数据块的地址。 ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:2:1","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#inode的组成部分"},{"categories":["技术"],"content":" inode不包含的信息：inode不包含文件名信息。文件名与inode号码之间的映射由目录项（directory entry）来维护。每个目录项将一个文件名映射到一个inode号码。 ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:2:2","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#inode不包含的信息"},{"categories":["技术"],"content":" inode的作用 文件系统结构：inode提供了一种高效的方式来管理文件系统中的文件和目录。通过inode，文件系统可以快速地找到文件的元数据和实际内容。 权限和所有权管理：inode存储了文件的权限和所有权信息，确保文件系统的安全性和访问控制。 文件操作：当执行文件操作时，如打开、读取、写入文件，操作系统通过inode来访问和修改文件的数据和元信息。 ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:2:3","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#inode的作用"},{"categories":["技术"],"content":" 查看inode信息在Linux系统中，可以使用以下命令查看文件的inode信息： 查看文件的inode号码 sh ls -i 文件名 # eg： ls -i example.txt # 输出示例： 1234567 example.txt 详细查看文件的元数据 sh stat 文件名 # eg： stat example.txt #输出示例： File: example.txt Size: 1234 Blocks: 8 IO Block: 4096 regular file Device: 801h/2049d Inode: 1234567 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/ username) Gid: ( 1000/ group) Access: 2024-07-05 12:34:56.000000000 +0000 Modify: 2024-07-04 11:33:55.000000000 +0000 Change: 2024-07-03 10:32:54.000000000 +0000 Birth: - ","date":"2024-07-05","objectID":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/:2:4","series":null,"tags":["unix","文件"],"title":"使用文件的inode号码删除无法打出名字的文件","uri":"/posts/unixs/%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6%E7%9A%84inode%E5%8F%B7%E7%A0%81%E5%88%A0%E9%99%A4%E6%97%A0%E6%B3%95%E6%89%93%E5%87%BA%E5%90%8D%E5%AD%97%E7%9A%84%E6%96%87%E4%BB%B6/#查看inode信息"},{"categories":["技术"],"content":"Docker容器内部抓包","date":"2024-07-02","objectID":"/posts/docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E6%8A%93%E5%8C%85/","series":null,"tags":["docker","调试","命名空间"],"title":"Docker容器内部抓包","uri":"/posts/docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E6%8A%93%E5%8C%85/"},{"categories":["技术"],"content":"很多docker容器为了轻量化，都不包含一些基础命令，如ip ，address，tcpdump 等，这给调试容器的网络带来了麻烦。 其实我们可以通过 命令进入容器的网络命名空间，使用宿主机的命令调试容器网络。 用法如下： 查看 docker 容器的 pid shell docker inspect -f {{.State.Pid}} nginx 进入容器的网络命名空间 shell nsenter -n -t \u003c上一步中的Pid\u003e # 退出 exit 然后就可以使用 ip 或者 tcpdump 抓包了。 ","date":"2024-07-02","objectID":"/posts/docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E6%8A%93%E5%8C%85/:0:0","series":null,"tags":["docker","调试","命名空间"],"title":"Docker容器内部抓包","uri":"/posts/docker%E5%AE%B9%E5%99%A8%E5%86%85%E9%83%A8%E6%8A%93%E5%8C%85/#"},{"categories":["技术"],"content":"数据库分布在多台设备上的好处： 可拓展性： 数据量、读取负载、写入负载超出了单台机器处理能力。 容错、高可用：单台机器（或多台、网络、整个数据中心）出现故障仍然能继续工作。 延迟：全球范围内部署多个服务器，用户从最近的数据中心获取服务。 扩展负载 垂直扩展（vertical scaling or scaling up）：买更强大的机器。 方法1: 共享内存架构，许多处理器，内存和磁盘可以在同一个操作系统下相互连接，快速的相互连接允许任意处理器访问内存或磁盘的任意部分。所有的组件都可以看作一台单独的机器（ps：大型机中，尽管任意处理器可以访问内存的任意部分，但是总有一些内存区域与处理器更近，称为非均匀内存访问，解决方案是分区，使每个处理器主要访问临近内存）。缺点，成本的增长是非线性的。 方法2: 共享磁盘架构，使用多台具有独立处理器和内存的机器，但将数据存储在机器之间共享的磁盘阵列上，这些磁盘通过快速网络连接（ps：网络附属存储 NAS ，或存储区网络 SAN）。这种架构常用于数据仓库，竞争和锁定的开销限制可伸缩性。 水平拓展（horizontal scaling or scaling out）: 也是一种无共享架构，买更多的机器。在这种架构中，运行数据库软件的每台机器 / 虚拟机都称为 节点（node）。每个节点只使用各自的处理器，内存和磁盘。节点之间的任何协调，都是在软件层面使用传统网络实现的。（ps：它带来了额外的复杂度）。 复制和分区 复制（Replication）： 不同的节点上保存数据的相同副本。提供了冗余，也有助于改善性能。 分区（Partitioning）：将一个大型数据库，拆分成较小的子集（称为分区，partitions），从而将不同的分区指派给不同的节点（nodes，也叫 分片， 即sharding）。 它们经常同时使用。 ","date":"2024-04-15","objectID":"/posts/ddia/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E6%A6%82%E8%BF%B0/:0:0","series":null,"tags":["分布式数据"],"title":"分布式数据-概述","uri":"/posts/ddia/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E6%A6%82%E8%BF%B0/#"},{"categories":["技术"],"content":" 查看别名 Powershell Get-Alias ","date":"2024-01-11","objectID":"/posts/powershell%E6%96%B0%E5%A2%9E%E5%88%AB%E5%90%8D/:1:0","series":null,"tags":["powershell"],"title":"Powershell新增别名","uri":"/posts/powershell%E6%96%B0%E5%A2%9E%E5%88%AB%E5%90%8D/#查看别名"},{"categories":["技术"],"content":" 创建永久别名在 Powershell 中使用 Set-Alias 和 New-Alias 定义的别名，在此session关闭后即会失效，防止此现象，可以将定义别名的命令写入 Windows Powershell profile 文件中。查看此文件位置: Powershell $profile 一般该文件在没有创建时是不存在的，使用以下命令创建： Powershell New-Item -Type file -Force $profile 可以在该文件中定义函数和别名等，如下： Powershell function py-v{ python --version } Set-Alias -Name py-v -Value py-v set-Alias -Name vim -Value nvim Invoke-Expression (\u0026starship init powershell) 重新打开 Powershell ，就会加载 $profile 文件。 ","date":"2024-01-11","objectID":"/posts/powershell%E6%96%B0%E5%A2%9E%E5%88%AB%E5%90%8D/:2:0","series":null,"tags":["powershell"],"title":"Powershell新增别名","uri":"/posts/powershell%E6%96%B0%E5%A2%9E%E5%88%AB%E5%90%8D/#创建永久别名"},{"categories":["技术"],"content":"今天发现早上登录服务器，查看日志有大量ssh登录的爆破，所以先限制root用户不允许远程登录，再使用Fail2Ban自动封锁ip限制一下。 记录如下： ","date":"2023-11-27","objectID":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/:0:0","series":null,"tags":["unix","fail2ban"],"title":"Fail2Ban防护SSH爆破","uri":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/#"},{"categories":["技术"],"content":" 限制root用户不允许远程登录 shell vi /etc/ssh/sshd_config 修改 PermitRootLogin yes 为 PermitRootLogin no 。 然后重启 ssh 服务 shell systemctl restart ssh ","date":"2023-11-27","objectID":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/:1:0","series":null,"tags":["unix","fail2ban"],"title":"Fail2Ban防护SSH爆破","uri":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/#限制root用户不允许远程登录"},{"categories":["技术"],"content":" 使用Fail2Ban自动封锁ip shell // install yum install fail2ban -y 修改配置文件 shell cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local vi /etc/fail2ban/jail.local 根据需要进行配置： ignoreip: 允许指定一组 IP 地址，这些地址不会被 fail2ban 封锁。 bantime: 定义 IP 地址被封锁的时间（秒）。 maxretry: 指定在多少次失败尝试后触发封锁。 findtime: 定义检查失败尝试的时间窗口。 backend: 设置 fail2ban 用于封锁 IP 的后端（例如 iptables）。 须在 ssh 选项下增加 enabled=true 如下： shell [sshd] # To use more aggressive sshd modes set filter parameter \"mode\" in jail.local: # normal (default), ddos, extra or aggressive (combines all). # See \"tests/files/logs/sshd\" or \"filter.d/sshd.conf\" for usage example and details. #mode = normal enabled = true port = ssh logpath = %(sshd_log)s backend = %(sshd_backend)s 设置开机自动，同时现在启动 shell systemctl enable --now fail2ban.service 常用命令： shell # 查看状态 serviceName 对应配置文件的中 [sshd] 选项 fail2ban-client status \u003cserviceName\u003e # 手动 ban ip fail2ban-client set sshd banip 222.186.16.210 # 手动删除被 ban 的 ip fail2ban-client set sshd delignoreip 222.186.16.210 # 查看日志 tail /var/log/fail2ban.log ps：当然也可以限制只允许密钥登录。 参考： centos8与Fail2Ban联合使用 ssh只允许密钥登录 fail2ban防御ddos攻击 ","date":"2023-11-27","objectID":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/:2:0","series":null,"tags":["unix","fail2ban"],"title":"Fail2Ban防护SSH爆破","uri":"/posts/unixs/fail2ban%E9%98%B2%E6%8A%A4ssh%E7%88%86%E7%A0%B4/#使用fail2ban自动封锁ip"},{"categories":["技术"],"content":"很多内网环境需要离线安装软件，就需要我们在可以上网的服务器上将需要安装的软件的相应安装包及其依赖下载下来，传输到离线机器，再安装。其中下载和处理依赖关系可能比较麻烦，所以本文特做记录。 ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:0:0","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#"},{"categories":["技术"],"content":" centos","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:0","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#centos"},{"categories":["技术"],"content":" 查看依赖 shell # 查看依赖列表 yum deplist docker-ce ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:1","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#查看依赖"},{"categories":["技术"],"content":" 下载全量依赖repotrack 是一个 yum-utils 包中的工具，用于下载指定软件包及其所有依赖关系，以便在没有网络连接的环境中安装它们。 shell yum install yum-utils -y // 下载 docker-ce 全量依赖包 repotrack docker-ce ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:2","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#下载全量依赖"},{"categories":["技术"],"content":" 下载主软件包和基于你现在的操作系统所缺少的依赖关系包如果依赖较多且在线环境和离线环境相同，就不需要下载全量依赖，只需要下载主软件包和基于你当前环境缺少的依赖。更多时候我们使用这种方法。 方法1：使用 yumdownloader shell yum install yum-utils -y // --resolve 下载依赖包 --destdir 指定目录，默认当前目录 yumdownloader --resolve --destdir=/tmp nginx 方法2：yum 的 downloadonly 插件 shell // 安装 yum 的 downloadonly 插件 yum install yum-download -y yum -y install nginx --downloadonly --downloaddir=. ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:3","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#下载主软件包和基于你现在的操作系统所缺少的依赖关系包"},{"categories":["技术"],"content":" 安装依赖方法1：localinstall 可以尝试解决并安装所有依赖 （推荐） shell yum localinstall ./rpm-path/*.rpm 方法2： rpm shell rmp -ivh \u003cpackage-name-1.rpm\u003e \u003cpackage-name-2.rpm\u003e ... # -U 升级或安装 -v 详细模式 -h 显示进度 --force 强制安装 --nodeps 忽略依赖性检查 rpm -Uvh --force --nodeps *.rpm 使用 rpm 请确保按照正确的顺序安装，以满足依赖关系。 如果缺少依赖，重复下载、传输、安装的步骤，直到所有依赖都被满足。 ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:4","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#安装依赖"},{"categories":["技术"],"content":" 依赖问题排查如果遇到如下问题 shell ---\u003e 软件包 audit-libs.x86_64.0.2.8.4-4.el7 将被 升级 --\u003e 正在处理依赖关系 audit-libs(x86-64) = 2.8.4-4.el7，它被软件包 audit-2.8.4-4.el7.x86_64 需要 Loading mirror speeds from cached hostfile Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7\u0026arch=x86_64\u0026repo=os\u0026infra=stock error was 14: curl#6 - \"Could not resolve host: mirrorlist.centos.org; 未知的错误\" 即，audit-libs-2.8.4 将被升级，但是缺少依赖，可以使用如下命令查看系统已经安装的软件包， shell rpm -qa | grep audit # output audit-libs-python-2.8.4-4.el7.x86_64 audit-libs-2.8.4-4.el7.x86_64 audit-2.8.4-4.el7.x86_64 然后去这个网址下载升级后的软件包（注意对应的 centos 版本，这里是 centos 7）: https://centos.pkgs.org/7/centos-x86_64/audit-2.8.5-4.el7.x86_64.rpm.html 一般下载链接为： http://mirror.centos.org/centos/7/os/x86_64/Packages/audit-2.8.5-4.el7.x86_64.rpm 然后安装即可。 参考：yum 下载全量依赖 rpm 包及离线安装 ","date":"2023-11-17","objectID":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/:1:5","series":null,"tags":["unix","centos","yum"],"title":"离线安装包及依赖处理","uri":"/posts/unixs/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%8C%85%E5%8F%8A%E4%BE%9D%E8%B5%96%E5%A4%84%E7%90%86/#依赖问题排查"},{"categories":["技术"],"content":"F12 打开开发者模式，选择需要截图的节点，ctrl+shift+p 打开运行栏，搜索 shot，选择 Capture node screenshot，如下图： 如果截取手机或平板模式的图，先点击左上角的 Toggle device toolbar （ Ctrl + Shift + M ）选择合适的设备，再进行上面步骤。 ","date":"2023-11-09","objectID":"/posts/chrome%E7%9A%84%E4%B8%80%E7%A7%8D%E6%88%AA%E5%9B%BE%E6%96%B9%E6%B3%95/:0:0","series":null,"tags":["折腾"],"title":"Chrome的一种截图方法","uri":"/posts/chrome%E7%9A%84%E4%B8%80%E7%A7%8D%E6%88%AA%E5%9B%BE%E6%96%B9%E6%B3%95/#"},{"categories":["技术"],"content":" 场景本地开发代码，需要同步到 10.21.30.81 上， 但是本地只能通过 ssh 连接到 10.21.30.80 上，不能 ssh 连接到 10.21.30.81 上，因此需要将代码通过SFTP传到 80 上，再在 80 上 scp 到 81上，太麻烦了，网络如下： ","date":"2023-11-09","objectID":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/:1:0","series":null,"tags":["SSH隧道"],"title":"PhpStorm实现跨网传输源代码","uri":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/#场景"},{"categories":["技术"],"content":" SSH 隧道在本机上执行 shell ssh -L 2222:10.21.30.81:22 root@10.21.30.80 以上，本机上的 2222 端口会被监听，访问本机的 2222 端口，相当于访问 10.21.30.81 的 22 端口，即 ssh隧道建立在本机和 10.21.30.80之间。这个命令可以让你通过本地端口 2222 来访问远程主机 10.21.30.81 上的 SSH 服务。 在编辑器 Phpstrom 中配置 Tools-\u003eDeployment-\u003eConfiguration, 添加 SFTP， 使用 SSH 连接 127.0.0.1:2222，即可实现代码自动上传。 与之类似的，执行 shell ssh -L 33060:10.21.30.81:3306 root@10.21.30.80 连接本地的 127.0.0.1:33060， 即可访问 10.21.30.81:3306 的 mysql 数据库。 ","date":"2023-11-09","objectID":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/:2:0","series":null,"tags":["SSH隧道"],"title":"PhpStorm实现跨网传输源代码","uri":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/#ssh-隧道"},{"categories":["技术"],"content":" 参考连接SSH隧道简明教程 SSH 端口转发 ","date":"2023-11-09","objectID":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/:3:0","series":null,"tags":["SSH隧道"],"title":"PhpStorm实现跨网传输源代码","uri":"/posts/phpstorm%E5%AE%9E%E7%8E%B0%E8%B7%A8%E7%BD%91%E4%BC%A0%E8%BE%93%E6%BA%90%E4%BB%A3%E7%A0%81/#参考连接"},{"categories":["技术"],"content":" Shell解析yml代码实现如下： shell # 解析 yml , 数组自动添加序号 # ### 例如： ### yml 如下： ### global: ### input: ### - \"main.c\" ### - \"main.h\" ### flags: [ \"-O3\", \"-fpic\" ] ### sample_input: ### - { property1: value, property2: \"value2\" } ### - { property1: \"value3\", property2: 'value 4' } ### ### 执行： eval $(parse_yaml \"config.yml\"） ### ### 解析结果如下，相当于生成了如下代码: ### global_input_1=\"main.c\" ### global_input_2=\"main.h\" ### global_flags_1=\"-O3\" ### global_flags_2=\"-fpic\" ### global_sample_input_1_property1=\"value\" ### global_sample_input_1_property2=\"value2\" ### global_sample_input_2_property1=\"value3\" ### global_sample_input_2_property2=\"value 4\" ### ### 最后可以使用间接变量引用，访问变量 ### function parse_yaml { local prefix=$2 local s='[[:space:]]*' w='[a-zA-Z0-9_]*' fs=$(echo -e '\\034') sed -ne \"s|,$s\\]$s\\$|]|\" \\ -e \":1;s|^\\($s\\)\\($w\\)$s:$s\\[$s\\(.*\\)$s,$s\\(.*\\)$s\\]|\\1\\2: [\\3]\\n\\1 - \\4|;t1\" \\ -e \"s|^\\($s\\)\\($w\\)$s:$s\\[$s\\(.*\\)$s\\]|\\1\\2:\\n\\1 - \\3|;p\" $1 | \\ sed -ne \"s|,$s}$s\\$|}|\" \\ -e \":1;s|^\\($s\\)-$s{$s\\(.*\\)$s,$s\\($w\\)$s:$s\\(.*\\)$s}|\\1- {\\2}\\n\\1 \\3: \\4|;t1\" \\ -e \"s|^\\($s\\)-$s{$s\\(.*\\)$s}|\\1-\\n\\1 \\2|;p\" | \\ sed -ne \"s|^\\($s\\):|\\1|\" \\ -e \"s|^\\($s\\)-$s[\\\"']\\(.*\\)[\\\"']$s\\$|\\1$fs$fs\\2|p\" \\ -e \"s|^\\($s\\)-$s\\(.*\\)$s\\$|\\1$fs$fs\\2|p\" \\ -e \"s|^\\($s\\)\\($w\\)$s:$s[\\\"']\\(.*\\)[\\\"']$s\\$|\\1$fs\\2$fs\\3|p\" \\ -e \"s|^\\($s\\)\\($w\\)$s:$s\\(.*\\)$s\\$|\\1$fs\\2$fs\\3|p\" | \\ awk -F$fs '{ indent = length($1)/2; vname[indent] = $2; for (i in vname) {if (i \u003e indent) {delete vname[i]; idx[i]=0}} if(length($2)== 0){ vname[indent]= ++idx[indent] }; if (length($3) \u003e 0) { vn=\"\"; for (i=0; i\u003cindent; i++) { vn=(vn)(vname[i])(\"_\")} gsub(/\\s*#.*$/, \"\", $3); printf(\"%s%s%s=\\\"%s\\\"\\n\", \"'$prefix'\",vn, vname[indent], $3); } }' } 用法： yml 文件内容如下： yml # yml elasticsearch: - setup/elastic/ilm-policies/my-ilm-policy.request - setup/elastic/index-templates/switch.request mysql: host: 127.0.0.1 解析代码如下： shell elastic_set_file=\"${BASH_SOURCE[0]%/*}\"/elastic/setup-elasticsearch.yml if [[ ! -f \"${elastic_set_file:-}\" ]]; then sublog \"No setup-elastic.yml file found, skipping\" continue fi # 解析 yml 并添加前缀，避免变量名已存在 eval $(parse_yaml \"$elastic_set_file\" \"config_\") # 会生成如下变量 config_elasticsearch_1=\"setup/elastic/ilm-policies/my-ilm-policy.request\" config_elasticsearch_2=\"setup/elastic/index-templates/switch.request\" config_mysql_host=\"127.0.0.1\" ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:1:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#shell解析yml"},{"categories":["技术"],"content":" bash 动态变量名（间接变量引用） shell # 假设有两个变量 var1=\"Hello\" index=1 # 使用间接变量引用来构造动态变量名 dynamic_var=\"var${index}\" # 使用动态变量名访问变量的值 echo \"${!dynamic_var}\" # 输出：Hello # 例如：有多个递增变量 # global_input_1=\"main.c\" # global_input_2=\"main.h\" # ... # global_input_100=\"xxx.c\" for idx in {1..100}; do variable_name=\"global_input_$idx\" if [ ! -v \"$variable_name\" ]; then break if file_name=\"${!variable_name}\" # more code done ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:2:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#bash-动态变量名间接变量引用"},{"categories":["技术"],"content":" bash 参数扩展参数扩展是 Bash 中强大的字符串操作工具，可以用于各种字符串处理任务。 以下是一些常用的参数扩展写法示例： ${varname#substring}：删除最短匹配前缀。 ${varname##substring}：删除最长匹配前缀。 ${varname%suffix}：删除最短匹配后缀。 ${varname%%substring}：删除最长匹配后缀。 ${varname/old/new}：将第一个匹配的 “old” 子字符串替换为 “new”。 ${varname//old/new}：将所有匹配的 “old” 子字符串替换为 “new”。 ${#varname}：获取字符串的长度。 ${varname[offset,length]}：从字符串中提取子字符串，从 offset 位置开始，长度为 length。 ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:3:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#bash-参数扩展"},{"categories":["技术"],"content":" bash 迭代关联数组或关联列表 shell declare -A users_passwords users_passwords=( [logstash_internal]=\"${LOGSTASH_INTERNAL_PASSWORD:-}\" [kibana_system]=\"${KIBANA_SYSTEM_PASSWORD:-}\" [metricbeat_internal]=\"${METRICBEAT_INTERNAL_PASSWORD:-}\" [filebeat_internal]=\"${FILEBEAT_INTERNAL_PASSWORD:-}\" [heartbeat_internal]=\"${HEARTBEAT_INTERNAL_PASSWORD:-}\" [monitoring_internal]=\"${MONITORING_INTERNAL_PASSWORD:-}\" [beats_system]=\"${BEATS_SYSTEM_PASSWORD=:-}\" ) for user in \"${!users_passwords[@]}\"; do echo \"User '$user'\" if [[ -z \"${users_passwords[$user]:-}\" ]]; then echo 'No password defined, skipping' continue fi # code more ... done ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:4:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#bash-迭代关联数组或关联列表"},{"categories":["技术"],"content":" 数组展开 shell args=(\"http://example.com\" \"-H\" \"User-Agent: MyUserAgent\" \"--data\" \"param1=value1\" \"--header\" \"Content-Type: application/json\") curl \"${args[@]}\" # 与下边形式等价，避免了拼接字符串 curl http://example.com -H \"User-Agent: MyUserAgent\" --data \"param1=value1\" --header \"Content-Type: application/json\" ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:5:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#数组展开"},{"categories":["技术"],"content":" 拓展链接优雅的shell ","date":"2023-09-25","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/:6:0","series":null,"tags":["unix","shell"],"title":"Shell编程-2","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-2/#拓展链接"},{"categories":["技术"],"content":"数据在不共享内存的进程传递，就需要编码为字节序列。有多种方式。 兼容性 实际描述了 编码数据的进程 和 解码数据的进程 之间的关系。它对 可演化性（允许你升级系统的部分，而不必全部升级） 非常重要。 数据可以通过多种方式从一个流程流向另一个流程（通常先编码为字节序列，再解码）。谁编码，谁解码？以下是常见场景： 数据库 服务调用 异步消息传递 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:0:0","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#"},{"categories":["技术"],"content":" 数据库中的数据流写入数据 —\u003e 编码数据；读取数据 — \u003e 解码数据。 单进程连接数据库，现在写入的数据，未来才读取。向后兼容 是必须的。 多个进程更常见，可能是不同的应用程序或服务连接一个数据库，也可能是一个服务的几个实例（并行运行，并行的目的是可伸缩性和容错性）。在这个场景下，可能发生，一个数据库中的值被较新的代码写入，被旧代码读取。因此 数据库也需要 向前兼容。 当你向数据库新增了一个字段（即修改了数据库的表结构，或者说修改了数据的模式 schema ），较新的代码写入包含新字段内容的数据到数据库中，旧代码（不知道有新字段）在读取、更新、写回记录到数据库时，理想状态时需保值新字段的值不变。编码格式需要支持，在应用层面也需要小心。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:1:0","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#数据库中的数据流"},{"categories":["技术"],"content":" 数据的生命周期超出代码的生命周期代码的版本更新频繁，数据库的数据版本可能变化可能大多并不频繁。 将数据迁移到一个新的模式是昂贵的，大多数数据库尽可能避免它，大多数关系数据库（除了mysql，并非真的必要时，它也经常会重写整个表）允许一些简单的模式变更，如添加一个默认为空的新列，而非重写现有数据。 LinkedIn 的文档数据库 Espresso 使用 Avro 存储，允许它使用 Avro 的模式演变规则。 模式演变允许整个数据库看起来好像是用单个模式编码的，即使底层存储可能包含用各种历史版本的模式编码的记录。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:1:1","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#数据的生命周期超出代码的生命周期"},{"categories":["技术"],"content":" 数据归档创建数据快照时，即使源数据库中的原始编码包含来自不同时代的模式版本的混合，数据转储通常也将使用最新模式进行编码。既然你不管怎样都要拷贝数据，那么你可以对这个数据拷贝进行一致的编码。 由于数据转储是一次写入的，而且以后是不可变的，所以 Avro 对象容器文件等格式非常适合。这也是一个很好的机会，可以将数据编码为面向分析的列式格式，例如 Parquet。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:1:2","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#数据归档"},{"categories":["技术"],"content":" 服务中的数据流：REST 与 RPC当你需要通过网络在进程间通信时，有很多方法，最常见的是采用 cs 架构，即 服务端 通过网络暴露 API，客户端通过网络向服务端的 API 发出请求。 服务器公开的 API 被称为 服务 。 Web 是怎么工作的，客户端（即浏览器）想服务器发出请求，通过 GET 请求下载 HTML，CSS，JavaScript，图片等，通过 POST 请求发送数据给服务器。API 包含一组协议和数据格式（HTTP，URLs, SSL/TLS, HTML，等）。浏览器、web服务器，网站作者都同意这些标准。 除了浏览器，还有很多其他客户端，例如，移动端或电脑端的app，浏览器中的客户端 JavaScript 程序可以使用XMLHttpRequest成为客户端。 另外服务器本身也可以作为另一个服务的客户端（如 Web 应用服务器充当数据库的客户端）。这种方法通常用于将大型应用程序按照功能区域分解为较小的服务，这样当一个服务需要来自另一个服务的某些功能或数据时，就会向另一个服务发出请求。这种构建应用程序的方式传统上被称为 面向服务的体系结构（service-oriented architecture，SOA） ，最近被改进和更名为 微服务架构 。 服务类似于数据库，相同点，允许客户端提交和查询数据。不同之处，数据库允许使用特定的查询语言进行任意查询，但服务只公开了一个特定于应用程序的 API （只允许由服务业务逻辑预定义的输入和输出）。这种限制提 供了一定程度的封装：服务可以对客户可以做什么和不可以做什么施加细粒度的限制。 面向服务/微服务架构的一个关键设计目标是通过使服务独立部署和演化来使应用程序更易于更改和维护。例如，每个服务应该由一个团队拥有，并且该团队应该能够经常发布新版本的服务，而不必与其他团队协调。换句话说，我们应该期望服务器和客户端的旧版本和新版本同时运行，因此服务器和客户端使用的数据编码必须在不同版本的服务API之间兼容。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:2:0","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#服务中的数据流rest-与-rpc"},{"categories":["技术"],"content":" Web服务当服务使用HTTP作为底层通信协议时，可称之为Web服务。不仅在Web上使用，还可以用于不同的环境中。如： 客户端应用程序（移动设备上的app，使用Ajax的 JS 应用程序）通过 HTTP 像服务发出请求。通常通过公共互联网进行。 一种服务向同组织拥有的另一项服务提出请求，这些服务通常位于同一数据中心，作为面向服务/微型架构的一部分。（支持这种用例的软件有时被称为中间件）。 一种服务通过互连网向不同的组织所拥有的服务提出请求。用于不同组织后端系统之前的数据交换。如在线服务提供的公共 API，用户共享用户数据的OAuth 等。 由两种流行的服务方法： REST 和 SOAP。 REST 不是一个协议，是基于HTTP原则的设计哲学。强调简单的数据格式，使用 URL 来标记资源，使用 HTTP 功能来进行缓存控制，身份验证和内容类型协商。根据 REST 原则设计的 API 称为 RESTful 。 SOAP是一种基于XML的用于发起网络API请求的协议。尽管它最常用于HTTP上，但它的目标是独立于HTTP，并避免使用大多数HTTP特性。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:2:1","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#web服务"},{"categories":["技术"],"content":" 远程过程调用（RPC）的问题RPC模型试图向远程网络服务发出请求，看起来与在同一进程中调用编程语言中的函数或方法相同（这种抽象称为位置透明）。尽管RPC起初看起来很方便，但这种方法根本上是有缺陷的。网络请求与本地函数调用非常不同： 本地函数可预测，成功或失败取决于参数。网络请求不可预知：网络问题，请求和响应可能丢失，远程计算机可能很慢或不可用，这些问题不在您的控制范围且常见，所以你必须预测并应对它们，比如重试失败的请求。 本地函数要么返回结果，要么抛出异常，或永远不返回（进入无限循环或进程崩溃）。网络请求可能由于超时不返回结果，如果你没有收到来自远程服务的响应，可能的原因有服务器出错，网络超时（请求未到达，响应未到达）。 调用本地功能，通常需要的时间是大致相同的。网络请求比本地函数调用慢的多，且延迟非常可变：不到一毫秒可以完成的请求，在网络拥塞或远程服务超载时，可能需要几秒钟。 调用本地函数时，可以高效地将引用（指针）传递给本地内存中的对象。当你发出一个网络请求时，所有这些参数都需要被编码成可以通过网络发送的一系列字节。对于字符串和数字没什么问题，但是对于较大的对象可能就会成为问题。 客户端和服务可以用不同的编程语言实现，所以RPC框架必须将数据类型从一种语言翻译成另一种语言。这可能会捅出大篓子，因为不是所有的语言都具有相同的类型 。用单一语言编写的单个进程中不存在此问题。 所有这些因素意味着尝试使远程服务看起来像编程语言中的本地对象一样毫无意义，因为这是一个根本不同的事情。 REST的部分吸引力在于，它并不试图隐藏它是一个网络协议的事实（尽管这似乎并没有阻止人们在REST之上构建RPC库）。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:2:2","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#远程过程调用rpc的问题"},{"categories":["技术"],"content":" RPC 的当前方向尽管有这样那样的问题，RPC不会消失。有很多基于编码格式的RPC框架：Thrift和Avro带有RPC支持，gRPC是使用Protocol Buffers的RPC实现，Finagle也使用Thrift，Rest.li 使用 JSON over HTTP 。 这种新一代的RPC框架更加明确的是，远程请求与本地函数调用不同。Finagle和Rest.li 使用futures（promises）来封装可能失败的异步操作。 Futures 还可以简化需要并行发出多项服务的情况，并将其结果合并。gRPC支持流，其中一个调用不仅包括一个请求和一个响应，还包括一系列的请求和响应 其中一些框架还提供服务发现，即允许客户端找出在哪个IP地址和端口号上可以找到特定的服务。 使用二进制编码格式的自定义RPC协议可以实现比通用的JSON over REST更好的性能。但是，RESTful API还有其他一些显著的优点： 实验和调试更友好（只需使用Web浏览器或命令行工具curl，无需任何代码生成或软件安装即可向其请求） 它受到所有主流编程语言和平台的支持，并且拥有庞大的工具生态系统（服务器、缓存、负载均衡器、代理、防火墙、监控、调试工具、测试工具等）可供使用。 由于这些原因，REST 似乎是公共 API 的主要风格。 RPC框架的主要重点在于同一组织拥有的服务之间的请求，通常在同一数据中心内。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:2:3","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#rpc-的当前方向"},{"categories":["技术"],"content":" 消息传递中的数据流REST和RPC：其中一个进程通过网络向另一个进程发送请求并期望尽可能快的响应。 数据库：一个进程写入编码数据，另一个进程在将来再次读取。 异步消息传输 是介于 RPC 和 数据库之间的 进程间通信方式。类似 RPC 客户端的请求（通常称为消息）以低延迟传送到另一个进程，类似于数据库，不是通过直接的网络连接发送消息，而是通过称为消息代理（也称为消息队列或面向消息的中间件）的中介来临时存储消息。 VS RPC的优点： 接受端不可用或者过载时，充当缓冲区，可靠性++。 自动的重传消息到已经崩溃的进程，防止丢失。 避免发送方知道接收方的IP和端口（云部署环境中，虚拟机经常创建和销毁，比较有用） 允许一条消息发送给多个接受端。 发送端和接收端逻辑分离。 消息传递通信通常是单向的，RPC是双向的。发送者通常不期望收到消息的回复。但是回复也是可以实现的，通常是在另外一个单独的通道（channel）上，这种模式叫作 异步 。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:3:0","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#消息传递中的数据流"},{"categories":["技术"],"content":" 消息代理开源方案： RabbitMQ，ActiveMQ，HornetQ，NATS，Apache Kafka 消息代理的大致逻辑： 一个进程（称为生产者）将消息发送到一个被命名的队列或主题（topic），而消息代理（即经纪人）负责确保该消息被传递给订阅了该队列或主题的一个或多个消费者。 队列或主题（topic）是单向的。但是，消息的消费者可以将回复的消息发布到另一个主题或者原始消息发送者使用的回复队列（就类似与RPC了）。 消息代理通常不强制特定的数据模型——一个消息统称只是一个包含元数据的字节流。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:3:1","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#消息代理"},{"categories":["技术"],"content":" 分布式 actor 框架actor 模型是单进程的并发模型。对比直接操作线程（线程存在的问题竞争条件，锁，死锁），逻辑被封装在 actor 中，每个 actor 代表一个客户和实体，它有一些本地状态（不与其他 actor 共享），通过发送和接受异步消息来与其他 actor 通信。消息传递不具有保障性，可能丢失。 在分布式的 Actor框架中，此模型用于跨多个节点伸缩应用程序。不管发送方和接收方是相同还是不同的节点，都是用相同的消息传递机制。如果它们位于不同的节点，消息则被透明编码为字节序列，通过网络发送，然后在另一侧解码。 在使用Actor模型时，由于该模型已经假设了消息可能会丢失，因此位置透明性效果更好（相比 RPC ）。即使在单个进程内部，Actor模型也能处理消息丢失的情况。尽管网络延迟可能比同一进程内部更高，但在使用Actor模型时，本地和远程通信之间的基本不匹配较少，这意味着在设计应用程序时，可以更容易地处理本地和远程通信之间的差异。 分布式的 Actor 框架实质上是将消息代理和 actor 编程模型集成到一个框架中。 三个流行的分布式 actor 框架处理消息编码如下： 默认情况下，Akka 使用 Java 的内置序列化，不提供前向或后向兼容性。 但是，你可以用类似 Prototol Buffers 的东西替代它，从而获得滚动升级的能力。 Orleans 默认使用不支持滚动升级部署的自定义数据编码格式；要部署新版本的应用程序，你需要设置一个新的集群，将流量从旧集群迁移到新集群，然后关闭旧集群。 像 Akka 一样，可以使用自定义序列化插件。 在 Erlang OTP 中，对记录模式进行更改是非常困难的（尽管系统具有许多为高可用性设计的功能）。 滚动升级是可能的，但需要仔细计划。 一个新的实验性的 maps 数据类型（2014 年在 Erlang R17 中引入的类似于 JSON 的结构）可能使得这个数据类型在未来更容易。 ","date":"2023-09-07","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/:3:2","series":null,"tags":["数据密集","数据流","dataflow","不共享内存数据传递"],"title":"数据流的类型","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84%E7%B1%BB%E5%9E%8B/#分布式-actor-框架"},{"categories":["技术"],"content":"程序通常（至少）使用两种形式的数据： 内存中，数据存储在对象、结构体、列表、数组、散列表、树等中。这些数据结构针对CPU的高效访问和操作做了优化（通常使用指针）。 如果要将数据写入文件，或者通过网络发送，则必须将其 编码（encode） 为某种自包含的字节序列（如：json文档）。由于每个进程都有自己的独立的地址空间，一个进程中的指针对任何其他进程都没有意义，所以这个字节序列表示 与 通常在内存中使用的数据结构完全不同 （除了一些特殊情况外，如某些内存映射文件或直接压缩数据上的操作）。 所以，两者表示之间，需要进行某种类型的翻译。 从内存中表示 —\u003e 字节序列 ： 编码（Encoding）（也称 序列化（serialization） 、或 编组（marshalling）） 字节序列 —\u003e 内存： 解码（Decoding） （也称 解析（Parsing），反序列化（deserialization), 反编组（unmarshalling) ps1：编码（encode）和 加密（encryption） 无关。 ps2：Marshal 与 Serialization 的区别：Marshal 不仅传输对象的状态，而且会一起传输对象的方法（相关代码）。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:0:0","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#"},{"categories":["技术"],"content":" JSON、XML和二进制变体JSON，XML 和 CSV 属于文本格式，因此具有人类可读性（尽管它们的语法是一个热门争议话题）。除了表面的语法问题之外，它们也存在一些微妙的问题： 数字编码的模糊。XML 和 CSV 无法处理区分数字和字符串（除非使用外部模式）。JSON 虽然可以区分数字和字符串，但是不能区分整数和浮点数，且不能指定精度。处理大数时这是个问题。大于 2^53 的整数无法使用 IEEE754 双精度浮点数表示。 JSON 和 XML 对 Unicode 字符串有很好的支持，但是它们不支持二进制数据（即不带字符编码的字节序列）。 XML和JSON都有可选的模式支持。这些模式语言非常强大，因此学习和实现它们相对复杂。XML模式的使用相对广泛，但许多基于JSON的工具并不使用模式。由于数据的正确解释（例如数字和二进制字符串）取决于模式中的信息，不使用XML/JSON模式的应用程序需要在编码/解码逻辑中可能硬编码适当的处理方式。 CSV（逗号分隔值）没有任何模式，因此应用程序需要自行定义每行和每列的含义。如果应用程序更改导致添加新的行或列，您必须手动处理这些更改。CSV格式也比较模糊（如果值中包含逗号或换行符会发生什么？）。虽然它的转义规则已经正式规定，但并非所有解析器都正确地实现了这些规则。 尽管存在这些缺陷，但 JSON、XML 和 CSV 对很多需求来说已经足够好了。它们很可能会继续流行下去，特别是作为数据交换格式来说（即将数据从一个组织发送到另一个组织）。在这种情况下，只要人们对格式是什么意见一致，格式有多美观或者效率有多高效就无所谓了。让不同的组织就这些东西达成一致的难度超过了绝大多数问题。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:1:0","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#jsonxml和二进制变体"},{"categories":["技术"],"content":" 二进制编码对于仅在组织内部使用的数据，使用最通用的编码格式的压力较小。例如，您可以选择更紧凑或更快速解析的格式。对于小型数据集，这些改进可能微不足道，但一旦涉及到大量的数据（以TB为单位），数据格式的选择就会产生重大影响。 JSON相比XML较为简洁，但与二进制格式相比，它们仍然使用了大量的空间。这一观察结果导致了针对 JSON（例如MessagePack、BSON、BJSON、UBJSON、BISON和Smile）和XML（例如WBXML和Fast Infoset）的大量二进制编码的开发。这些格式在各个领域中被采用，但没有一个像JSON和XML的文本版本那样被广泛采纳。 由于它们没有规定模式，所以它们需要在编码数据中包含所有的对象字段名称。 eg: 示例 json 文档，以下会用多种格式对这段数据进行编码 json { \"userName\": \"Martin\", \"favoriteNumber\": 1337, \"interests\": [\"daydreaming\", \"hacking\"] } MessagePack 编码 第一个字节 0x83 表示接下来是 3 个字段（低四位 = 0x03）的 对象 object（高四位 = 0x80）。 （如果想知道如果一个对象有 15 个以上的字段会发生什么情况，字段的数量塞不进 4 个 bit 里，那么它会用另一个不同的类型标识符，字段的数量被编码两个或四个字节）。 第二个字节 0xa8 表示接下来是 8 字节长（低四位 = 0x08）的字符串（高四位 = 0x0a）。 接下来八个字节是 ASCII 字符串形式的字段名称 userName。由于之前已经指明长度，不需要任何标记来标识字符串的结束位置（或者任何转义）。 接下来的七个字节对前缀为 0xa6 的六个字母的字符串值 Martin 进行编码，依此类推。 二进制编码长度为 66 个字节，仅略小于文本 JSON 编码所取的 81 个字节（删除了空白）。所有的 JSON 的二进制编码在这方面是相似的。空间节省了一丁点（以及解析加速）是否能弥补可读性的损失，谁也说不准。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:1:1","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#二进制编码"},{"categories":["技术"],"content":" Thrift 与 Protocol BuffersFacebook 的 Thrift 和 Google 的 Protocol Buffers 都需要一个模式来编写数据： thrift struct Person { 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list\u003cstring\u003e interests } Protocol Buffers 的模式看起来是非常相似 protobuf message Person { required string user_name = 1; optional int64 favorte_number = 2; repeated string interests = 3; } 它们都带有一个代码生成工具，根据类似以上示例的模式定义，生成各种编程语言实现该模式的类。你的应用程序可以调用生成的代码对模式的记录进行编码和解码。编码后的数据如下： Thrift 提供两种不同的二进制编码格式（迷之操作），BinaryProtocol 和 CompactProtocol。（ps： 实际上，Thrift 有三种二进制协议：BinaryProtocol、CompactProtocol 和 DenseProtocol，尽管 DenseProtocol 只支持 C ++ 实现，所以不算作跨语言。 除此之外，它还有两种不同的基于 JSON 的编码格式。 真逗！） BinaryProtocol ： 不必传递字段全名，只需传递模式定义中的数字编号，所以更紧凑。 CompactProtocol ： 字段类型和标签号打包到单个字节中；可变长度的整数。数字 1337 不是使用全部八个字节，而是用两个字节编码，每个字节的最高位用来指示是否还有更多的字节。这意味着 - 64 到 63 之间的数字被编码为一个字节，-8192 和 8191 之间的数字以两个字节编码，等等。较大的数字使用更多的字节。 Protocol Buffers 编码： 需要注意的一个细节：在前面所示的模式中，每个字段被标记为必需或可选，但是这对字段如何编码没有任何影响（二进制数据中没有任何字段指示某字段是否必须）。区别在于，如果字段设置为 required，但未设置该字段，则所需的运行时检查将失败，这对于捕获错误非常有用。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:2:0","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#thrift-与-protocol-buffers"},{"categories":["技术"],"content":" 字段标签和模式演变如上例子，一个编码的记录就是一堆编码过的字段，每个字段使用 标签号（模式定义中的数字1，2，3） 唯一标识，并且标注一个数据类型（e.g. string 或 int）。如果一个字段没有设置值，编码是即可忽略它。也就是说你可以改变字段的名字，但是不能改变字段的标签号，否则将影响已经编码的数据。 新增字段，给新字段设置标签号即可。旧代码（不知道有新字段）尝试读取包含新字段的数据时，它不能识别新的标签号，直接忽略即可。数据类型注释会告诉解析器它需要跳过多少字节。这保证了 向前兼容：（新代码产生的编码后数据可以被旧代码读取）。 对于向后兼容，因为每个字段都有一个唯一的标签号且标签号代表这相同的意思，旧代码产生的编码后数据总是能被新代码读取。唯一需要注意的是，你不能设置字段是 required 的。如果一个新增字段被设置为 必须的 ，新代码读取时将检查该字段是否存在，而旧代码编码的数据压根不知道新字段的存在。因此，为了保持向后兼容性，在模式初始化部署之后，每个新增的字段必须是 optional 或者设置默认值。 删除一个字段与添加一个字段类似，只是向前兼容和向后兼容的关注点被颠倒了。这意味着你只能删除一个可选的字段（必需字段永远不能删除），而且你不能再次使用相同的标签号码（因为你可能仍然有数据写在包含旧标签号码的地方，新代码已经忽略了该字段）。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:2:1","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#字段标签和模式演变"},{"categories":["技术"],"content":" 数据类型和模式演变改变数据类型也是可行的，但是有一个风险，值将失去精度或者被截断。例如，你把 i32 转为 i64, 新代码可以很容易读取旧数据（前32位用零填充）即 向后兼容。然而新代码产生的 i64 数据，旧代码在读取时是按照 i32 读取的，会被截断，即没有 向前兼容。 Protobuf 没有 list 或 array 的数据类型，取而代之的是一个 repeated （和 required 和 optional 同一个等级 ）, 一个字段被定义为 repeated , 意味着,相同标签号的字段在记录出现了多次 。这种设计，对于将 optional （单值）字段变更为 repeated （多值）字段，非常有用。新代码在读取 旧数据 时，会看到一个出现了 1次或0次的元素（取决于旧数据字段值是否被设置）；旧代码读取新数据时，只能读取到列表的最后一个元素。 Thrift 有专门的列表数据类型，该数据类型使用列表元素的数据类型作为参数。这不允许像 Protocol Buffers 那样从单值到多值的演化，但它具有支持嵌套列表的优势。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:2:2","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#数据类型和模式演变"},{"categories":["技术"],"content":" AvroApache Avro 是作为 Hadoop 的子项目，在 2009 年开始。 有两种模式： Avro IDL 用于人工编辑； 基于json 更易于机器读取。 上述例子，使用 Avro IDL 编写，如下： avdl record Person { string userName; union { null, long } favoriteNumber = null; array\u003cstring\u003e interests; } 等价的 JSON 展示这个 模式如下： json { \"type\": \"record\", \"name\": \"Person\", \"fields\": [ {\"name\": \"userName\", \"type\": \"string\"}, {\"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null}, {\"name\": \"interests\", \"type\": {\"type\": \"array\", \"items\": \"string\"}} ] } 其编码字节如下所示 有几点有趣的如下： 该模式没有标签序号 字节序列既没有字段的唯一标识也没有字段的数据类型，只是一串连在一起的值 如上一个 string 类型的字段，只有一个长度前缀，后跟着UTF-8 字节，编码数据中没有任何告诉你这是一个字符串。它可以被当成整数，或者其他的数据类型 整数使用和 Thrift 的 CompactProtocol 相同的可变长度的方式编码 为了解析二进制数据，你应该按照模式中定义的顺序和数据类型遍历各个字段，这意味着，只有当读取数据和写入数据使用 完全相同的模式 时，二进制数据才能被正确解析。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:0","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#avro"},{"categories":["技术"],"content":" reader schema 和 writer schemawriter’s schema： 当一个应用想编码一些数据（写入到文件、数据库，或者通过网络发送等），应用会使用任一版本的模式（它知道的，例如被编译到应用中的）来编码数据。 reader’s schema：当一个应用想解码一些数据（从文件、数据库读取，或者从网络接收等），它期待数据遵循一些模式，这叫做读者模式。这个模式是应用程序代码所依赖的，也就是说，代码可能在应用程序的构建过程中根据这个模式生成。 Avro 的核心思想时，reader 模式和 writer 模式不必完全相同，只需要兼容。当数据解码（读取）时，Avro库通过并排查看 Writer 和 Reader模式，并将数据从 Writer 模式转换到Reader模式。以下是个示例： 顺序不同，没问题 writer 中有 ，reader 中没有，忽略它 writer 中没有，reader 中要获取，则使用 reader 模式中的默认值 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:1","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#reader-schema-和-writer-schema"},{"categories":["技术"],"content":" 模式演变规则对于 Avro， 向前兼容意味着你有一个新版本的模式被作为writer，而旧版本的模式作为 reader。相反，向后兼容意味着，你新版本的模式作为 reader，旧版本作为writer。 为了保证兼容性，你只能添加或删除有默认值的字段。这时，新字段会存在在新模式中而非旧模式中，当新模式的reader读取旧模式写入的记录时，缺少的字段填充默认值。 Avro 中如果允许一个字段为 null，则必须使用联合类型。如：union {null, long, string} 表示 field 可以是 null 、长整型和字符串，且默认值为 null（Avro 的限制默认值必须为联合的第一分支）。 Avro 没有 optional 和 required ，因为它有联合类型和默认值。 只要 Avro 可以支持相应的类型转换，就可以改变字段的数据类型。更改字段的名称也是可能的，但有点棘手：Reader 模式可以包含字段名称的别名，所以它可以匹配旧 Writer 的模式字段名称与别名。这意味着更改字段名称是向后兼容的，但不能向前兼容。同样，向联合类型添加分支也是向后兼容的，但不能向前兼容。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:2","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#模式演变规则"},{"categories":["技术"],"content":" writer模式到底是什么对于一段特定的编码数据，Reader 如何知道其 Writer 模式？我们不能只将整个模式包括在每个记录中，因为模式可能比编码的数据大得多，从而使二进制编码节省的所有空间都是徒劳的。 答案是 Avro 使用上下文： 很多记录的大文件 Avro 的一个常见用途，尤其在 Hadoop 环境中——用于存储数百万条记录的大文件，所有记录采用相同的模式进行编码。在这种情况下，该文件的作者可以在文件的开头只包含一次 Writer 模式。 Avro 指定了一个文件格式（对象容器文件）来做到这一点。 独立写入记录的数据库 在一个数据库中，不同的记录可能会在不同的时间点使用不同的 Writer 模式来写入 - 你不能假定所有的记录都有相同的模式。最简单的解决方案是在每个编码记录的开始处包含一个版本号，并在数据库中保留一个模式版本列表。Reader 可以获取记录，提取版本号，然后从数据库中获取该版本号的 Writer 模式。使用该 Writer 模式，它可以解码记录的其余部分（例如 Espresso 就是这样工作的）。 通过网络连接发送的记录 当两个进程通过双向网络连接进行通信时，他们可以在连接设置上协商模式版本，然后在连接的生命周期中使用该模式。 Avro RPC 协议就是这样工作的。 存储模式版本的数据库很有用，它可以让你有机会检查文档的模式兼容性。版本号可以递增，也可以hash。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:3","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#writer模式到底是什么"},{"categories":["技术"],"content":" 动态生成的模式Avro 不含任何标签号码，这使得 Avro 对动态生成模式更友善。 假如， 你想把一个关系数据库的内容转存到一个二进制文件中，使用 Avro 可以很容易生成一个模式，并用该模式对内容进行编码，然后将其存储到 Avro 对象容器文件中（数据库的列名，就映射为Avro模式中的字段名）。 现在，数据库模式发生变化（新增或删除了列），则可以从新数据库模式生成新的Avro模式，并使用新模式导出数据。更新的 Writer 模式仍然可以与旧的 Reader 模式匹配。 如果使用 Thrift 或 Protocol Buffers，则字段标签可能必须手动分配：每次数据库模式更改时，管理员都必须手动更新从数据库列名到字段标签的映射（这可能会自动化，但模式生成器必须非常小心，不要分配以前使用的字段标签）。这种动态生成的模式根本不是 Thrift 或 Protocol Buffers 的设计目标，而是 Avro 的。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:4","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#动态生成的模式"},{"categories":["技术"],"content":" 代码生成Thrift 和 Protobuf 依赖于代码生成。对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，因为它可以在编译时进行类型检查。对动态类型的语言，生成代码没有太多意义。 Avro 为静态类型编程语言提供了可选的代码生成功能，但是它也可以在不生成任何代码的情况下使用。如果你有一个对象容器文件（它嵌入了 Writer 模式），你可以简单地使用 Avro 库打开它，并以与查看 JSON 文件相同的方式查看数据。该文件是自描述的，因为它包含所有必要的元数据。（特别适合动态类型的数据处理语言如 Apache Pig ）。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:5","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#代码生成"},{"categories":["技术"],"content":" 模式的优点 它们可以比各种 “二进制 JSON” 变体更紧凑，因为它们可以省略编码数据中的字段名称。 模式是一种有价值的文档形式，因为模式是解码所必需的，所以可以确定它是最新的（而手动维护的文档可能很容易偏离现实）。 维护一个存储模式的数据库允许你在部署任何内容之前检查模式变更的向前和向后兼容性。 对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，因为它可以在编译时进行类型检查。 ","date":"2023-06-16","objectID":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/:3:6","series":null,"tags":["数据密集","编码","解码","模式"],"title":"编码数据的格式","uri":"/posts/ddia/%E7%BC%96%E7%A0%81%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%BC%E5%BC%8F/#模式的优点"},{"categories":["技术"],"content":"修改应用程序的功能也也意味着修改其存储的数据：使用新的字段或记录类型，或者以新的方式展示现有的数据。 不同的数据模型有不同的方法应对这种变化： 关系型数据库假定数据库中的所有数据都遵循一个模式：尽管可以更改这个模式（通过模式迁移，即 ALTER 语句），但是在任何时间点都有且仅有一个正确的模式。 读时模式（schema-on-read，或 无模式，即 schemaless）数据库不会强制一个模式，因此数据库可以包含在不同时间写入的新老数据格式的混合。 当数据 格式（format） 或 模式（schema） 发生变化时，通常需要对应用程序代码进行相应的更改（例如，为记录添加新字段，然后修改程序开始读写该字段）。但在大型应用程序中，代码变更通常不会立即完成： 对于 服务端（server-side） 应用程序，可能需要执行 滚动升级 （rolling upgrade） （也称为 阶段发布（staged rollout） ），一次将新版本部署到少数几个节点，检查新版本是否运行正常，然后逐渐部完所有的节点。这样无需中断服务即可部署新版本，为频繁发布提供了可行性，从而带来更好的可演化性。 对于 客户端（client-side） 应用程序，升不升级就要看用户的心情了。用户可能相当长一段时间里都不会去升级软件。 这意味着，新旧版本的代码，以及新旧数据格式可能会在系统中同时共处。系统想要继续顺利运行，就需要保持 双向兼容性： 向后兼容 (backward compatibility)， 又名 向下兼容（downward compatibility） 新的代码可以读取由旧的代码写入的数据。 向前兼容 (forward compatibility)，又名 向上兼容 （upward compatibility） 旧的代码可以读取由新的代码写入的数据。 向后兼容性通常并不难实现：新代码的作者当然知道由旧代码使用的数据格式，因此可以显示地处理它（最简单的办法是，保留旧代码即可读取旧数据）。 向前兼容性可能会更棘手，因为旧版的程序需要忽略新版数据格式中新增的部分。 ","date":"2023-06-01","objectID":"/posts/ddia/%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%85%BC%E5%AE%B9/:0:0","series":null,"tags":["数据密集","模式","兼容"],"title":"模式与兼容","uri":"/posts/ddia/%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%85%BC%E5%AE%B9/#"},{"categories":["技术"],"content":" 在线事务处理（OLTP）VS 在线分析处理（OLAP） 属性 事务处理系统 OLTP （OnLine Transaction Processing） 分析系统 OLAP （OnLine Analytice Processing） 主要读取模式 查询少量记录，按键读取 在大批量记录上聚合 主要写入模式 随机访问，写入要求低延时 批量导入（ETL）或者事件流 主要用户 终端用户，通过 Web 应用 内部数据分析师，用于决策支持 处理的数据 数据的最新状态（当前时间点） 随时间推移的历史事件 数据集尺寸 GB ~ TB TB ~ PB 起初，事务处理和分析查询使用了相同的数据库。 SQL 在这方面已证明是非常灵活的：对于 OLTP 类型的查询以及 OLAP 类型的查询来说效果都很好。尽管如此，在二十世纪八十年代末和九十年代初期，企业有停止使用 OLTP 系统进行分析的趋势，转而在单独的数据库上运行分析。这个单独的数据库被称为 数据仓库（data warehouse）。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:1:0","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#在线事务处理oltpvs-在线分析处理olap"},{"categories":["技术"],"content":" 数据仓库OLTP 往往对业务至关重要，要求 高可用 和 低延迟 。 数据仓库是一个独立的数据库，包含公司各种OLTP系统的所有只读副本。 从 OLTP 数据库中提取数据（使用定期的数据转存储或者连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中。这个过程称之为 ”抽取（Extract）-转换（Transform）-加载（Load）“ 简写为： ETL。 数据仓库可针对分析类的访问模式进行优化。 数据仓库的数据模型通常是关系型的，因为 SQL 通常适合分析查询。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:2:0","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#数据仓库"},{"categories":["技术"],"content":" 分析的模式：星型模式和雪花模式在分析型业务中，数据模型的多样性很少，许多数据仓库都以相对公式化的方式使用，即 星型模式（也称为维度建模）。 模式中心有一个 事实表 。事实表的每行代表在特定时间内发生的事件。例如：分析零售额，每行代表客户购买的产品；分卸网络流量，每行代表一个用户的页面浏览和点击。事实被视为单独的事件，因为这样可以在以后分析中获得最大的灵活性。但是，这意味着事实表可以变得非常大。 事实表中的一些列是属性，例如产品销售的价格和从供应商那里购买的成本（可以用来计算利润率）。事实表中的其他列是对其他表（称为维度表）的外键引用。由于事实表中的每一行都表示一个事件，因此这些维度代表事件发生的对象、内容、地点、时间、方式和原因。 甚至日期和时间也通常使用维度表来表示，因为这允许对日期的附加信息（诸如公共假期）进行编码，从而允许区分假期和非假期的销售查询。 “星型模式” 这个名字来源于这样一个事实，即当我们对表之间的关系进行可视化时，事实表在中间，被维度表包围；与这些表的连接就像星星的光芒。 这个模板的变体被称为 雪花模式，其中维度被进一步分解为子维度。 在典型的数据仓库中，表格通常非常宽：事实表通常有 100 列以上，有时甚至有数百列。维度表也可以是非常宽的，因为它们包括了所有可能与分析相关的元数据。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:2:1","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#分析的模式星型模式和雪花模式"},{"categories":["技术"],"content":" 列式存储如果事实表中有万亿行和数 PB 的数据，那么高效地存储和查询它们就成为一个具有挑战性的问题。 在大多数 OLTP 数据库中，存储都是以面向行的方式进行布局的：表格的一行中的所有值都相邻存储。文档数据库也是相似的：整个文档通常存储为一个连续的字节序列。 面向行的存储引擎仍然需要将所有这些行（每个包含超过 100 个属性）从硬盘加载到内存中，解析它们，并过滤掉那些不符合要求的属性。 列式存储背后的想法很简单：不要将所有来自一行的值存储在一起，而是将来自每一列的所有值存储在一起。如果每个列式存储在一个单独的文件中，查询只需要读取和解析查询中使用的那些列（分析时很少 select * …），这可以节省大量的工作。 列式存储布局依赖于每个列文件包含相同顺序的行。 因为，如果你需要重新组装完整的行，你可以从每个单独的列文件中获取第 23 项，并将它们放在一起形成表的第 23 行。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:0","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#列式存储"},{"categories":["技术"],"content":" 列压缩除了仅从硬盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对硬盘吞吐量的需求。幸运的是，列式存储通常很适合压缩。 位图编码技术通常情况下，一列中不同值的数量与行数相比要小得多（例如，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品）。现在我们可以拿一个有 n 个不同值的列，并把它转换成 n 个独立的位图：每个不同值对应一个位图，每行对应一个比特位。如果该行具有该值，则该位为 1，否则为 0。 如果 n 非常小（例如，国家 / 地区列可能有大约 200 个不同的值），则这些位图可以将每行存储成一个比特位。但是，如果 n 更大，大部分位图中将会有很多的零（我们说它们是稀疏的）。在这种情况下，位图可以另外再进行游程编码（run-length encoding，一种无损数据压缩技术）。这可以使列的编码非常紧凑。 位图索引非常适合数据仓库中常见的各种查询。按位与，按位或 即可满足 sql 中的 and 和 in 查询。 内存带宽和向量化处理对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从硬盘获取数据到内存的带宽。但是这并不是唯一的瓶颈，分析型数据库的开发人员还关注如何高效地利用从主内存到CPU缓存的带宽，避免分支预测错误和CPU指令处理流水线中的停顿，并利用现代CPU中的单指令多数据（SIMD single-instruction-multi-data）指令。 列式存储：可以减小从硬盘加载的数据量；也可以高效的利用CPU周期。如：查询引擎可以将一整块压缩好的列数据放进 CPU 的 L1 缓存中，然后在紧密的循环（即没有函数调用）中遍历。相比于每条记录的处理都需要大量函数调用和条件判断的代码，CPU 执行这样一个循环要快得多。列压缩允许列中的更多行被同时放进容量有限的 L1 缓存。前面描述的按位 “与” 和 “或” 运算符可以被设计为直接在这样的压缩列数据块上操作。这种技术被称为向量化处理（vectorized processing）。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:1","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#列压缩"},{"categories":["技术"],"content":" 列压缩除了仅从硬盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对硬盘吞吐量的需求。幸运的是，列式存储通常很适合压缩。 位图编码技术通常情况下，一列中不同值的数量与行数相比要小得多（例如，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品）。现在我们可以拿一个有 n 个不同值的列，并把它转换成 n 个独立的位图：每个不同值对应一个位图，每行对应一个比特位。如果该行具有该值，则该位为 1，否则为 0。 如果 n 非常小（例如，国家 / 地区列可能有大约 200 个不同的值），则这些位图可以将每行存储成一个比特位。但是，如果 n 更大，大部分位图中将会有很多的零（我们说它们是稀疏的）。在这种情况下，位图可以另外再进行游程编码（run-length encoding，一种无损数据压缩技术）。这可以使列的编码非常紧凑。 位图索引非常适合数据仓库中常见的各种查询。按位与，按位或 即可满足 sql 中的 and 和 in 查询。 内存带宽和向量化处理对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从硬盘获取数据到内存的带宽。但是这并不是唯一的瓶颈，分析型数据库的开发人员还关注如何高效地利用从主内存到CPU缓存的带宽，避免分支预测错误和CPU指令处理流水线中的停顿，并利用现代CPU中的单指令多数据（SIMD single-instruction-multi-data）指令。 列式存储：可以减小从硬盘加载的数据量；也可以高效的利用CPU周期。如：查询引擎可以将一整块压缩好的列数据放进 CPU 的 L1 缓存中，然后在紧密的循环（即没有函数调用）中遍历。相比于每条记录的处理都需要大量函数调用和条件判断的代码，CPU 执行这样一个循环要快得多。列压缩允许列中的更多行被同时放进容量有限的 L1 缓存。前面描述的按位 “与” 和 “或” 运算符可以被设计为直接在这样的压缩列数据块上操作。这种技术被称为向量化处理（vectorized processing）。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:1","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#位图编码技术"},{"categories":["技术"],"content":" 列压缩除了仅从硬盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对硬盘吞吐量的需求。幸运的是，列式存储通常很适合压缩。 位图编码技术通常情况下，一列中不同值的数量与行数相比要小得多（例如，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品）。现在我们可以拿一个有 n 个不同值的列，并把它转换成 n 个独立的位图：每个不同值对应一个位图，每行对应一个比特位。如果该行具有该值，则该位为 1，否则为 0。 如果 n 非常小（例如，国家 / 地区列可能有大约 200 个不同的值），则这些位图可以将每行存储成一个比特位。但是，如果 n 更大，大部分位图中将会有很多的零（我们说它们是稀疏的）。在这种情况下，位图可以另外再进行游程编码（run-length encoding，一种无损数据压缩技术）。这可以使列的编码非常紧凑。 位图索引非常适合数据仓库中常见的各种查询。按位与，按位或 即可满足 sql 中的 and 和 in 查询。 内存带宽和向量化处理对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从硬盘获取数据到内存的带宽。但是这并不是唯一的瓶颈，分析型数据库的开发人员还关注如何高效地利用从主内存到CPU缓存的带宽，避免分支预测错误和CPU指令处理流水线中的停顿，并利用现代CPU中的单指令多数据（SIMD single-instruction-multi-data）指令。 列式存储：可以减小从硬盘加载的数据量；也可以高效的利用CPU周期。如：查询引擎可以将一整块压缩好的列数据放进 CPU 的 L1 缓存中，然后在紧密的循环（即没有函数调用）中遍历。相比于每条记录的处理都需要大量函数调用和条件判断的代码，CPU 执行这样一个循环要快得多。列压缩允许列中的更多行被同时放进容量有限的 L1 缓存。前面描述的按位 “与” 和 “或” 运算符可以被设计为直接在这样的压缩列数据块上操作。这种技术被称为向量化处理（vectorized processing）。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:1","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#内存带宽和向量化处理"},{"categories":["技术"],"content":" 列式存储中的排序顺序列式存储中，存储列的顺序并不关键，按插入顺序存储它们最简单，因为插入一个新行只需要追加到每个列文件。但是，我们也可以选择按某种顺序来排列数据，就像我们之前对 SSTables 所做的那样，并将其用作索引机制。 注意，对每列分别执行排序也是无意义的，因为那样就没法知道不同列中的哪些项属于同一行。我们只能在明确一列中的第 k 项与另一列中的第 k 项属于同一行的情况下，才能重建出完整的行。 对数据的排序需要对整行进行统一操作，即使它们的存储方式是按列的。数据库管理员可以根据他们对常用查询的了解，来选择表格中用来排序的列。例如，如果查询通常以日期范围为目标，例如“上个月”，则可以将 date_key 作为第一个排序键。这样查询优化器就可以只扫描近1个月范围的行了，这比扫描所有行要快得多。 对于第一排序列中具有相同值的行，可以用第二排序列来进一步排序。例如，如果 date_key 是中的第一个排序关键字，那么 product_sk 可能是第二个排序关键字，以便同一天的同一产品的所有销售数据都被存储在相邻位置。这将有助于需要在特定日期范围内按产品对销售进行分组或过滤的查询。 按顺序排序的另一个好处是它可以帮助压缩列。如果主要排序列没有太多个不同的值，那么在排序之后，将会得到一个相同的值连续重复多次的序列。一个简单的游程编码可以将该列压缩到几 KB —— 即使表中有数十亿行。 第一个排序键的压缩效果最强。第二和第三个排序键会更混乱，因此不会有这么长的连续的重复值。排序优先级更低的列以几乎随机的顺序出现，所以可能不会被压缩。但对前几列做排序在整体上仍然是有好处的。 几个不同的排序顺序对这个想法，有一个巧妙的扩展被 C-Store 发现，并在商业数据仓库 Vertica 中被采用：既然不同的查询受益于不同的排序顺序，为什么不以几种不同的方式来存储相同的数据呢？反正数据都需要做备份，以防单点故障时丢失数据。因此你可以用不同排序方式来存储冗余数据，以便在处理查询时，调用最适合查询模式的版本。 在一个列式存储中有多个排序顺序有点类似于在一个面向行的存储中有多个次级索引。但最大的区别在于 面向行的存储将每一行保存在一个地方（在堆文件或聚集索引中），次级索引只包含指向匹配行的指针。 在列式存储中，通常在其他地方没有任何指向数据的指针，只有包含值的列。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:2","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#列式存储中的排序顺序"},{"categories":["技术"],"content":" 列式存储中的排序顺序列式存储中，存储列的顺序并不关键，按插入顺序存储它们最简单，因为插入一个新行只需要追加到每个列文件。但是，我们也可以选择按某种顺序来排列数据，就像我们之前对 SSTables 所做的那样，并将其用作索引机制。 注意，对每列分别执行排序也是无意义的，因为那样就没法知道不同列中的哪些项属于同一行。我们只能在明确一列中的第 k 项与另一列中的第 k 项属于同一行的情况下，才能重建出完整的行。 对数据的排序需要对整行进行统一操作，即使它们的存储方式是按列的。数据库管理员可以根据他们对常用查询的了解，来选择表格中用来排序的列。例如，如果查询通常以日期范围为目标，例如“上个月”，则可以将 date_key 作为第一个排序键。这样查询优化器就可以只扫描近1个月范围的行了，这比扫描所有行要快得多。 对于第一排序列中具有相同值的行，可以用第二排序列来进一步排序。例如，如果 date_key 是中的第一个排序关键字，那么 product_sk 可能是第二个排序关键字，以便同一天的同一产品的所有销售数据都被存储在相邻位置。这将有助于需要在特定日期范围内按产品对销售进行分组或过滤的查询。 按顺序排序的另一个好处是它可以帮助压缩列。如果主要排序列没有太多个不同的值，那么在排序之后，将会得到一个相同的值连续重复多次的序列。一个简单的游程编码可以将该列压缩到几 KB —— 即使表中有数十亿行。 第一个排序键的压缩效果最强。第二和第三个排序键会更混乱，因此不会有这么长的连续的重复值。排序优先级更低的列以几乎随机的顺序出现，所以可能不会被压缩。但对前几列做排序在整体上仍然是有好处的。 几个不同的排序顺序对这个想法，有一个巧妙的扩展被 C-Store 发现，并在商业数据仓库 Vertica 中被采用：既然不同的查询受益于不同的排序顺序，为什么不以几种不同的方式来存储相同的数据呢？反正数据都需要做备份，以防单点故障时丢失数据。因此你可以用不同排序方式来存储冗余数据，以便在处理查询时，调用最适合查询模式的版本。 在一个列式存储中有多个排序顺序有点类似于在一个面向行的存储中有多个次级索引。但最大的区别在于 面向行的存储将每一行保存在一个地方（在堆文件或聚集索引中），次级索引只包含指向匹配行的指针。 在列式存储中，通常在其他地方没有任何指向数据的指针，只有包含值的列。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:2","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#几个不同的排序顺序"},{"categories":["技术"],"content":" 写入列式存储这些优化在数据仓库中是有意义的，因为其负载主要由分析人员运行的大型只读查询组成。列式存储、压缩和排序都有助于更快地读取这些查询。然而，他们的缺点是写入更加困难。 使用 B 树的就地更新方法对于压缩的列是不可能的。如果你想在排序表的中间插入一行，你很可能不得不重写所有的列文件。由于行由列中的位置标识，因此插入必须对所有列进行一致地更新。 幸运的是，本章前面已经看到了一个很好的解决方案：LSM 树。所有的写操作首先进入一个内存中的存储，在这里它们被添加到一个已排序的结构中，并准备写入硬盘。内存中的存储是面向行还是列的并不重要。当已经积累了足够的写入数据时，它们将与硬盘上的列文件合并，并批量写入新文件。这基本上是 Vertica 所做的。 查询操作需要检查硬盘上的列数据和内存中的最近写入，并将两者的结果合并起来。但是，查询优化器对用户隐藏了这个细节。从分析师的角度来看，通过插入、更新或删除操作进行修改的数据会立即反映在后续的查询中。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:3","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#写入列式存储"},{"categories":["技术"],"content":" 聚合：数据立方体和物化视图数据仓库的另一个值得一提的方面是物化聚合（materialized aggregates）。如前所述，数据仓库查询通常涉及一个聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。如果相同的聚合被许多不同的查询使用，那么每次都通过原始数据来处理可能太浪费了。为什么不将一些查询使用最频繁的计数或总和缓存起来？ 创建这种缓存的一种方式是物化视图（Materialized View）。在关系数据模型中，它通常被定义为一个标准（虚拟）视图：一个类似于表的对象，其内容是一些查询的结果。不同的是，物化视图是查询结果的实际副本，会被写入硬盘，而虚拟视图只是编写查询的一个捷径。从虚拟视图读取时，SQL 引擎会将其展开到视图的底层查询中，然后再处理展开的查询。 当底层数据发生变化时，物化视图需要更新，因为它是数据的非规范化副本。数据库可以自动完成该操作，但是这样的更新使得写入成本更高，这就是在 OLTP 数据库中不经常使用物化视图的原因。在读取繁重的数据仓库中，它们可能更有意义（它们是否实际上改善了读取性能取决于使用场景）。 物化视图的常见特例称为数据立方体或 OLAP 立方。它是按不同维度分组的聚合网格。显示了一个例子。 想象一下，现在每个事实都只有两个维度表的外键 —— 在上图中分别是日期和产品。你现在可以绘制一个二维表格，一个轴线上是日期，另一个轴线上是产品。每个单元格包含具有该日期 - 产品组合的所有事实的属性（例如 net_price）的聚合（例如 SUM）。然后，你可以沿着每行或每列应用相同的汇总，并获得减少了一个维度的汇总（按产品的销售额，无论日期，或者按日期的销售额，无论产品）。 一般来说，事实往往有两个以上的维度。在中有五个维度：日期、产品、商店、促销和客户。要想象一个五维超立方体是什么样子是很困难的，但是原理是一样的：每个单元格都包含特定日期 - 产品 - 商店 - 促销 - 客户组合的销售额。这些值可以在每个维度上求和汇总。 物化数据立方体的优点是可以让某些查询变得非常快，因为它们已经被有效地预先计算了。例如，如果你想知道每个商店的总销售额，则只需查看合适维度的总计，而无需扫描数百万行的原始数据。 数据立方体的缺点是不具有查询原始数据的灵活性。例如，没有办法计算有多少比例的销售来自成本超过 100 美元的项目，因为价格不是其中的一个维度。因此，大多数数据仓库试图保留尽可能多的原始数据，并将聚合数据（如数据立方体）仅用作某些查询的性能提升手段 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:3:4","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#聚合数据立方体和物化视图"},{"categories":["技术"],"content":" 小结在高层次上，我们看到存储引擎分为两大类：针对 事务处理（OLTP） 优化的存储引擎和针对 在线分析（OLAP） 优化的存储引擎。这两类使用场景的访问模式之间有很大的区别： OLTP 系统通常面向最终用户，这意味着系统可能会收到大量的请求。为了处理负载，应用程序在每个查询中通常只访问少量的记录。应用程序使用某种键来请求记录，存储引擎使用索引来查找所请求的键的数据。硬盘查找时间往往是这里的瓶颈。 数据仓库和类似的分析系统会少见一些，因为它们主要由业务分析人员使用，而不是最终用户。它们的查询量要比 OLTP 系统少得多，但通常每个查询开销高昂，需要在短时间内扫描数百万条记录。硬盘带宽（而不是查找时间）往往是瓶颈，列式存储是针对这种工作负载的日益流行的解决方案。 在 OLTP 这一边，我们能看到两派主流的存储引擎： 日志结构学派：只允许追加到文件和删除过时的文件，但不会更新已经写入的文件。Bitcask、SSTables、LSM 树、LevelDB、Cassandra、HBase、Lucene 等都属于这个类别。 就地更新学派：将硬盘视为一组可以覆写的固定大小的页面。 B 树是这种理念的典范，用在所有主要的关系数据库和许多非关系型数据库中。 日志结构的存储引擎是相对较新的技术。他们的主要想法是，通过系统性地将随机访问写入转换为硬盘上的顺序写入，由于硬盘驱动器和固态硬盘的性能特点，可以实现更高的写入吞吐量。 关于 OLTP，我们最后还介绍了一些更复杂的索引结构，以及针对所有数据都放在内存里而优化的数据库。 然后，我们暂时放下了存储引擎的内部细节，查看了典型数据仓库的高级架构，并说明了为什么分析工作负载与 OLTP 差别很大：当你的查询需要在大量行中顺序扫描时，索引的重要性就会降低很多。相反，非常紧凑地编码数据变得非常重要，以最大限度地减少查询需要从硬盘读取的数据量。我们讨论了列式存储如何帮助实现这一目标。 ","date":"2023-05-20","objectID":"/posts/ddia/oltp-or-olap/:4:0","series":null,"tags":["数据密集","OLTP","OLAP","列式存储"],"title":"OLTP-or-OLAP","uri":"/posts/ddia/oltp-or-olap/#小结"},{"categories":["技术"],"content":" word 文档管理最近写word文档有点多，并且是多人编写，然后就造成文档的标题变成了： txt xxxx-1.docx xxxx(1).docx xxxx-0517.docx 总之乱七八糟。 所以就搜索了一下，word 的版本管理方案，这里简单介绍一下 Git+Pandoc 的版本管理方案（需要一定的 Git 基础）。 安装 Git 和 Pandoc 配置 git 的 diff 差异引擎 pandoc，其可以将 word 文件转换为 md 文件，且设置为Pandoc差异引擎的提示方式为不显示提示信息。 git # 使用于当前仓库 git config diff.pandoc.textconv \"pandoc --to=markdown\" git config diff.pandoc.prompt false # 全局配置 git config --global diff.pandoc.textconv \"pandoc --to=markdown\" git config --global diff.pandoc.prompt false 【可选】配置别名 git # 以单词为单位进行比较 git config alias.wdiff 'diff --word-diff=color --unified=1' 在需要管理的仓库下新建文件 .gitattributes，加入如下内容，用于指定 Git 在比较 .docx 文件时使用 Pandoc 差异引擎。 git *.docx diff=pandoc ","date":"2023-05-17","objectID":"/posts/git%E7%AE%A1%E7%90%86word%E6%96%87%E6%A1%A3/:1:0","series":null,"tags":["git"],"title":"Git管理word文档","uri":"/posts/git%E7%AE%A1%E7%90%86word%E6%96%87%E6%A1%A3/#word-文档管理"},{"categories":["技术"],"content":" 拓展其他 diff 引擎可以使用以下命令查看支持的 git diff/ git merge 插件 git git difftool --tool-help git mergetool --tool-help 所以可以指定 diff 插件为 Beyond Compare ，参考链接 Beyond Compare。 ","date":"2023-05-17","objectID":"/posts/git%E7%AE%A1%E7%90%86word%E6%96%87%E6%A1%A3/:2:0","series":null,"tags":["git"],"title":"Git管理word文档","uri":"/posts/git%E7%AE%A1%E7%90%86word%E6%96%87%E6%A1%A3/#拓展其他-diff-引擎"},{"categories":["技术"],"content":" 最简单的数据库 shell #!/bin/bash db_set () { echo \"$1,$2\" \u003e\u003e database } db_get () { grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1 } 使用方法 shell source db.sh db_set 123456 '{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}' db_get 123456 该数据库，写入性能很好，但是读取性能非常糟糕，查找开销 O(n) 。 为了高效的查找数据可特有的键，我需要一个数据结构：索引（index）。即额外保存一些元数据作为路标，帮助找到我们想要的数据。索引是主数据的附加数据，在解决提升查询性能这一问题的同时，引入了新的问题，即写入数据时，要额外维护索引，也就产生了额外开销。这是一个权衡问题。 当然，写入的性能很难超过简单的追加文件，因为只是最简单的写入操作。任何索引都会减慢写入速度，因为每次写入数据都需更新索引。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:1:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#最简单的数据库"},{"categories":["技术"],"content":" Hash 索引即在内存中保存一个哈希映射，hash map的key为上边数据库的键值，值为实际内容的字节偏移量。 当数据库中存储新的值时，除了将新的键值对追加到文件外，还需要更新内存中的hash映射（也适用于插入、更新现有键值对）；当查找一个值时，可以使用哈希映射来查找数据文件中的偏移量，寻找（seek）该位置并读取该值。 现实中，Bitcask（默认 Riak 引擎） 就是这么做的。Ritcask提供高性能读写的必要条件是所有的 key 都放置在可用RAM中，因为哈希映射完全在内存中。 value 可以使用比内存大的空间，因为它们可以通过磁盘seek加载。如果部分数据已经存在在文件系统缓存中，读取则不需要磁盘 I/O 了。 像 Bitcask 这样的存储引擎适合每个键经常更新的情况。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:2:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#hash-索引"},{"categories":["技术"],"content":" SSTables和LSM树Sorted String Table（SSTable）是一种用于存储键值对的数据结构，常用于分布式存储系统中。它将键值对按照键的大小有序存储在磁盘文件中，使得查找和迭代操作变得高效。SSTable通常被设计成不可变的，即一旦存储就不能被修改，这样可以避免数据不一致性问题。为了支持数据的更新和删除操作，SSTable通常会与内存中的数据结构（如哈希表）结合使用，形成LSM-Tree（Log-Structured Merge-Tree）等数据结构。SSTable在大规模数据存储和查询场景中具有很高的性能和可靠性，被广泛应用于各种分布式存储系统中，如Hadoop、Cassandra、LevelDB等。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:3:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#sstables和lsm树"},{"categories":["技术"],"content":" Sorted String Table 对比 Hash 的优点1、合并段简单高效。 2、查找特定的键，不需要内存中所以的索引。 3、因为读请求无论如何都有扫描一部分键值对，因此在写入磁盘前，可以将记录分组、压缩，并在内存中维护一个压缩块开始的key和偏移量的索引。既可以节约磁盘空间，也可以减少IO带宽的使用。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:3:1","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#sorted-string-table-对比-hash-的优点"},{"categories":["技术"],"content":" 构建和维护 SSTablesb树，在磁盘上维护有序结构。 红黑树、AVL树，在内存中维护有序结构。 写入时，将其添加到内存中的平衡数据结构（例如，红黑树）。这种内存中的树有时叫 内存表（memtable）。 当内存表大于某个阈值（通常几兆）时，将其作为 SSTable 文件写入磁盘。树已经维护了排序的键值对，所以写入可以高效完成。新的 SSTable 文件成为数据库最新的部分。当 SSTable 正在被写入磁盘时，第一步的写入操作可以继续到一个新的内存表实例中。 为了提供读取请求，首先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下一个较旧的段中找关键字。 不时，会在后台运行合并和压缩进程以组合文件段、丢弃覆盖和删除值。 此方案问题：如果数据库崩溃，最近写入的（在内存表中，还未写入磁盘）数据将丢失。 解决问题：在磁盘上保存一个单独的日志，每个写入操作都被立刻追加到该文件中。此操作日志的唯一目的是为了崩溃后恢复内存表。所以每当内存表中的数据被当作SSTable文件写入磁盘时，相应的日志文件都会被丢弃。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:3:2","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#构建和维护-sstables"},{"categories":["技术"],"content":" 用 SSTables 制作 LSM Tree该算法本质就是 LevelDB （google 开发） 和 RocksDB（facebook 开发）所使用的算法，它们都是被设计成可以嵌入到其他应用程序中的键值存储引擎库。另外LevelDB可以作为Bitcask的替代选项，在Riak中使用。类似的存储引擎也被用于 Cassandra 和 HBase 中。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:3:3","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#用-sstables-制作-lsm-tree"},{"categories":["技术"],"content":" 性能优化 查找数据库中不存在的键，LSM-tree 算法很慢：必须先检查内存表，然后检查磁盘文件端一直到最久的数据（可能需要读取每一个），才能确定键不存在。通常使用存储引擎通常使用额外的Bloom过滤器解决这种问题。 选择SSTables压缩和合并的顺序和时机会有不同的策略。最常见的策略有 大小分层（Size-tiered Compaction）和 平坦压缩 （Leveled compaction）[见参考连接]。LevelDB 和 RocksDB 使用了 平坦压缩 ，HBase 使用了 大小分层， Cassandra 同时都支持。 Size-Tiered Compaction 策略：memtable 逐步刷入到磁盘 sst，刚开始 sst 都是小文件，随着小文件越来越多，当数据量达到一定阈值时，STCS 策略会将这些小文件 compaction 成一个中等大小的新文件。同样的道理，当中等文件数量达到一定阈值，这些文件将被 compaction 成大文件，这种方式不断递归，会持续生成越来越大的文件。数据合并不是即时的（达到阈值合并），相同数据的文件，可能存在于不同大小等级的多个文件中，故存在 空间放大（实际数据量只有1G，可能存储过程中占用远超1G）。对于覆写频繁的场景并不适用。 Leveled compaction 策略： sst 的大小可控，默认每个 sst 的大小一致（Size-Tiered Compaction 策略最终会产生超大文件） LCS 在合并时，会保证除 Level 0（L0）之外的其他 Level 有序且无覆盖。 除 L0 外，每层文件的总大小呈指数增长，假如 L1 最多 10 个，则 L2 为 100 个，L3 为 1000 个… 关键范围被拆分成更小的SSTables，而较旧的数据被移动到单独的“水平”，这使得压缩能够更加递增地进行，并且使用更少的磁盘空间。 该策略会造成 写放大 。 LSM树的基本思想 —— 保存一系列在后台合并的SSTables —— 简单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存 储，因此可以高效地执行范围查询（扫描所有高于某些最小值和最高值的所有键），并且因为磁盘写入是连续的，所以LSM树可以支持非常高的写入吞吐量。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:3:4","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#性能优化"},{"categories":["技术"],"content":" B树像 SSTables 一样，B 树保持按键排序的键值对，这允许高效的键值查找和范围查询。但这也就是仅有的相似之处了：B 树有着非常不同的设计理念。 不同之处： 日志结构索引：将数据库分解为可变大小的 段（segments），通常几兆或更大，且都是顺序写入段。 B-tree ：将数据库分解为固定大小的 **块（block）**或 页（page）, 传统上大小为 4KB（有时会更大），且一次只能读取或写入一页。这样的设计更接近硬件底层（unix 上可以通过 dumpe2fs 或者 xfs_info 查看的块大小 ）。每个页面使用地址或位置来标记，允许一个页面引用另一个页面——类似指针（硬盘中非内存中）。 有一个 page 是 B-tree 的根（查找 key 时，从这里开始），这个 page 包含几个 key 和一些 child page 的引用。child page 负责一段连续的 key 范围 ，引用之间的 key 表明了范围的边界在那里。 叶子页面（ldaf page），要么直接包含了每个 key 的value，要么包含了 key 对应 value 存放位置的引用。 分支因子（branching factor），B-tree 中一个页面对子页面引用的数量。 更新操作某个key的值：先搜索该 key 所在的叶子页面，再更改该页中的值，并将该页面写回硬盘中。 新增一个 key ：先找到其范围包含新 key 的页面，并将其添加到该页面中，如果该页没有足够的空间容纳新 key，则将其分成两个 半页，并更新父页面以反映新的键范围分区。 删除一个 key：（同时保持树平衡）就会牵扯很多其他东西。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:4:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#b树"},{"categories":["技术"],"content":" 让 B-Trees 更可靠B-trees 底层写操作：用新数据覆写硬盘上的页面，并假定对该页面的引用保持完整。 日志结构的索引（LSM-trees）：只追加文件（最终删除过时的文件），从不修改文件中已有的内容。 覆写硬盘页面对应的实际硬件层面的操作：磁性硬盘驱动器，将磁头移动到正确的半径位置，等待旋转盘转到正确的起始角度，使用新数据覆写适当的扇区。固态硬盘上，更复杂，ssd必须一次擦除和重写相当大的存储芯片块。 情况一 ： 一些操作需要覆写几个不同的页面，如：因为插入导致页面过满而拆分页面，则需要写入新拆分的两个页面，并覆写其父页面以更新对两个子页面的引用。这是一个危险的操作，因为如果数据库在系列操作进行到一半时崩溃，那么最终将导致一个损坏的索引（例如，可能有一个孤儿页面没有被任何页面引用） 。 为了应对这种情况，B 树实现通常会带有一个额外的硬盘数据结构：预写式日志（WAL，即 write-ahead log，也称为 重做日志，即 redo log）。这是一个仅追加的文件，每个 B 树的修改在其能被应用到树本身的页面之前都必须先写入到该文件。当数据库在崩溃后恢复时，这个日志将被用来使 B 树恢复到一致的状态。 情况二：如果多个线程要同时访问 B 树，则需要仔细的并发控制 —— 否则线程可能会看到树处于不一致的状态。这通常是通过使用 锁存器（latches，轻量级锁）保护树的数据结构来完成。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:4:1","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#让-b-trees-更可靠"},{"categories":["技术"],"content":" B-Trees 的优化 使用写时复制方案（非 WAL ） 来处理崩溃：经过修改的页面被写入到不同的位置，并且还在树中创建了父页面的新版本，以指向新的位置。同时，此方法对并发控制也很有用。 我们可以通过不存储整个键，而是缩短其大小，来节省页面空间。特别是在树内部的页面上，键只需要提供足够的信息来充当键范围之间的边界。在页面中包含更多的键允许树具有更高的分支因子，因此也就允许更少的层级。 通常，页面可以放置在硬盘上的任何位置；没有什么要求相邻键范围的页面也放在硬盘上相邻的区域。如果某个查询需要按照排序顺序扫描大部分的键范围，那么这种按页面存储的布局可能会效率低下，因为每个页面的读取都需要执行一次硬盘查找。因此，许多 B 树的实现在布局树时会尽量使叶子页面按顺序出现在硬盘上。但是，随着树的增长，要维持这个顺序是很困难的。相比之下，由于 LSM 树在合并过程中一次性重写一大段存储，所以它们更容易使顺序键在硬盘上连续存储。 额外的指针被添加到树中。例如，每个叶子页面可以引用其左边和右边的兄弟页面，使得不用跳回父页面就能按顺序对键进行扫描。 B 树的变体如 分形树（fractal trees） 借用了一些日志结构的思想来减少硬盘查找（而且它们与分形无关）。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:4:2","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#b-trees-的优化"},{"categories":["技术"],"content":" 比较 B-Trees 和 LSM-Trees","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:5:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#比较-b-trees-和-lsm-trees"},{"categories":["技术"],"content":" LSM-Trees 的优点B 树索引每块数据必须至少写入两次：一次写入预写入日志（WAL），一次写入树本身（如果分页还需再写入一次）。即使几个字节的变化，也需要接收写入整个页面的开销。有些存储引擎甚至覆写同一个页面两次，以免电源故障情况下页面未完整更新。 由于反复压缩与合并SSTable，日志结构索引会多次重写数据。这种影响——在数据库的生命周期中每笔数据导致对硬盘的多次写入 —— 被称为 写入放大（write amplification）。使用固态硬盘的机器需要额外关注这点，固态硬盘的闪存寿命在覆写有限次数后就会耗尽。 写放大会导致直接的性能代价：存储引擎写入硬盘的次数越多，可用硬盘带宽内它能处理的每秒写入次数就越少。 LSM 树通常能够比 B 树支持更高的写入吞吐量： ​ 原因一：它们有时具有较低的写放大（尽管这取决于存储引擎的配置和工作负载） ​ 原因二：它们顺序地写入紧凑的 SSTable 文件而不是必须覆写树中的几个页面。这种差异在机械硬盘上尤其重要，其顺序写入比随机写入要快得多。 LSM 树可以被压缩得更好，因此通常能比 B 树在硬盘上产生更小的文件。B 树存储引擎会由于碎片化（fragmentation）而留下一些未使用的硬盘空间：当页面被拆分或某行不能放入现有页面时，页面中的某些空间仍未被使用。由于 LSM 树不是面向页面的，并且会通过定期重写 SSTables 以去除碎片，所以它们具有较低的存储开销，特别是当使用分层压缩（leveled compaction）时。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:5:1","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#lsm-trees-的优点"},{"categories":["技术"],"content":" LSM-Trees 的缺点日志结构存储的缺点压缩过程有时会干扰正在进行的读写操作。尽管存储引擎尝试增量地执行压缩以尽量不影响并发访问，但是硬盘资源有限，所以很容易发生某个请求需要等待硬盘先完成昂贵的压缩操作。对吞吐量和平均响应时间的影响通常很小，但是日志结构化存储引擎在更高百分位的响应时间有时会相当长，而 B 树的行为则相对更具有可预测性。 压缩的另一个问题出现在高写入吞吐量时：硬盘的有限写入带宽 需要在 初始写入（记录和刷新内存表到硬盘）和在后台运行的压缩线程之间共享。当向一个空数据库写入时，整个磁盘带宽可以用于初始写入，但是随着数据库变得越来越大，压缩需要更多的磁盘带宽。 如果在高写入吞吐量的情况下，如果压缩没有经过仔细配置，可能会发生压缩无法跟上写入速率。这时，磁盘上的未合并段（nmerged segments）会不断增加，直到磁盘空间用尽，读取速度也会变慢，因为要检查更多的段文件。通常SSTable存储引擎不会限制传入写入速率，即使压缩跟不上，所以你需要建立监控来检测这种情况。 B 树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的段中有相同键的多个副本。这个方面使得 B 树在想要提供强大的事务语义的数据库中很有吸引力：在许多关系数据库中，事务隔离是通过在键范围上使用锁来实现的，在 B 树索引中，这些锁可以直接附加到树上。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:5:2","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#lsm-trees-的缺点"},{"categories":["技术"],"content":" 其他索引主键（primary key） 索引。主键唯一标识关系表中的一行，或文档数据库中的一个文档或图形数据库中的一个顶点。数据库中的其他记录可以通过其主键（或 ID）引用该行 / 文档 / 顶点，索引就被用于解析这样的引用。 次级索引可以很容易地从键值索引构建。次级索引主要的不同是键不是唯一的，即可能有许多行（文档，顶点）具有相同的键。这可以通过两种方式来解决：将匹配行标识符的列表作为索引里的值（就像全文索引中的记录列表），或者通过向每个键添加行标识符来使键唯一。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:6:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#其他索引"},{"categories":["技术"],"content":" 将值存储到索引中索引中的值可以存储实际的行（文档、顶点），也可以存储一个引用（行实际存储的位置的引用）。通常是 堆文件（heap file） ，它存储的数据是无序的。它避免了存在多个次级索引时对数据的复制：索引只引用堆文件上的一个位置，实际数据保存在一个地方。 在不更改键值时更新值，堆文件是非常高效的：新值占用字节数据不大于旧值时，覆盖该记录即可；新值占用空间大于旧值时，就需要将新值移动到堆中足够空间的新位置，这是要么更新所有的索引，指向新的堆位置，要么在旧的堆位置留一个转发指针。 从索引到堆文件的额外跳跃对读取来说性能损失较大，希望索引的行直接存储在索引中。这被称为 聚簇索引（Clustered Index） ，如 Mysql 的 InnoDB 存储引擎中表主键总是一个 聚簇索引，次级索引则引用主键。 聚簇索引（在索引中存储所有的行数据） 和 非聚簇索引（仅在索引中存储对数据的引用） 之间的折衷称为 覆盖索引（covering index），其在索引内部存储了一部分列。这就允许了通过单独使用索引来处理一些查询（这种情况下，可以说索引 覆盖（cover） 了查询）。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:6:1","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#将值存储到索引中"},{"categories":["技术"],"content":" 多列索引连接索引（concatenated index） ，它通过将一列的值追加到另一列后面，简单地将多个字段组合成一个键（索引定义中指定了字段的连接顺序）。 多维索引（multi-dimensional index） 是一种查询多个列的更一般的方法，这对于地理空间数据尤为重要。 一种选择是使用 空间填充曲线（space-filling curve） 将二维位置转换为单个数字，然后使用常规 B 树索引。更普遍的是，使用特殊化的空间索引，例如 R 树。例如，PostGIS 使用 PostgreSQL 的通用 GiST 工具将地理空间索引实现为 R 树。 多维索引不仅可以用于地理位置。例如，在电子商务网站上可以使用建立在（红，绿，蓝）维度上的三维索引来搜索特定颜色范围内的产品，也可以在天气观测数据库中建立（日期，温度）的二维索引，以便有效地搜索 2013 年内的温度在 25 至 30°C 之间的所有观测资料。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:6:2","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#多列索引"},{"categories":["技术"],"content":" 全文索引和模糊索引全文搜索引擎通常允许搜索目标从一个单词扩展为包括该单词的同义词，忽略单词的语法变体，搜索在相同文档中的近义词，并且支持各种其他取决于文本的语言分析功能。为了处理文档或查询中的拼写错误，Lucene 能够在一定的编辑距离内搜索文本（编辑距离 1 意味着单词内发生了 1 个字母的添加、删除或替换）。 Lucene 为其词典使用了一个类似于 SSTable 的结构。这个结构需要一个小的内存索引，告诉查询需要在排序文件中哪个偏移量查找键。在 LevelDB 中，这个内存中的索引是一些键的稀疏集合，但在 Lucene 中，内存中的索引是键中字符的有限状态自动机，类似于 trie 。这个自动机可以转换成 Levenshtein 自动机，它支持在给定的编辑距离内有效地搜索单词。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:6:3","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#全文索引和模糊索引"},{"categories":["技术"],"content":" 在内存中存储一切某些内存中的键值存储（如 Memcached）仅用于缓存，在重新启动计算机时丢失的数据是可以接受的。但其他内存数据库的目标是持久性，可以通过特殊的硬件（例如电池供电的 RAM）来实现，也可以将更改日志写入硬盘，还可以将定时快照写入硬盘或者将内存中的状态复制到其他机器上。 内存数据库重新启动时，需要从硬盘或通过网络从副本重新加载其状态（除非使用特殊的硬件）。尽管写入硬盘，它仍然是一个内存数据库，因为硬盘仅出于持久性目的进行日志追加，读取请求完全由内存来处理。写入硬盘同时还有运维上的好处：硬盘上的文件可以很容易地由外部程序进行备份、检查和分析。 反直觉的是，内存数据库的性能优势并不是因为它们不需要从硬盘读取的事实。只要有足够的内存即使是基于硬盘的存储引擎也可能永远不需要从硬盘读取，因为操作系统在内存中缓存了最近使用的硬盘块。相反，它们更快的原因在于省去了将内存数据结构编码为硬盘数据结构的开销。 内存数据库的另一个有趣的地方是提供了难以用基于硬盘的索引实现的数据模型。例如，Redis 为各种数据结构（如优先级队列和集合）提供了类似数据库的接口。因为它将所有数据保存在内存中，所以它的实现相对简单。 内存数据库体系结构可以扩展到支持比可用内存更大的数据集，而不必重新采用以硬盘为中心的体系结构。所谓的 反缓存（anti-caching） 方法通过在内存不足的情况下将最近最少使用的数据从内存转移到硬盘，并在将来再次访问时将其重新加载到内存中。这与操作系统对虚拟内存和交换文件的操作类似，但数据库可以比操作系统更有效地管理内存，因为它可以按单个记录的粒度工作，而不是整个内存页面。尽管如此，这种方法仍然需要索引能完全放入内存中（就像本章开头的 Bitcask 例子）。 ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:6:4","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#在内存中存储一切"},{"categories":["技术"],"content":" 参考链接Compaction 策略 - Size-Tiered Compaction 策略 – Leveled ","date":"2023-04-03","objectID":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/:7:0","series":null,"tags":["数据密集","SSTable","LSM-Trees","B-Trees","索引"],"title":"数据的存储与检索","uri":"/posts/ddia/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/#参考链接"},{"categories":["技术"],"content":" BitMap原理使用一个 bit 位来存储某种状态（比如签到与否，是否存在等）。 如记录 0-7 这几个数字中哪些存在，只需要 8 个 bit 位。0 表示数字不存在，1 表示数字存在，1，2，3，5 这几个数存在， 存储如下： 代表数字 7 6 5 4 3 2 1 0 bit位表示 0 0 1 0 1 1 1 0 例如：记录一个英文句子（小写）中是否存在 a - z 所有字母： go // 方法1： 使用 hash 存储 var hash = map[rune]bool{ 'a':false, 'b':false, // ... 'z':false, } // 方法2：使用数组，使用下标表示字母 var arr [26]bool // 方法3：使用 int32 的低 26 位存储，每位 0 表示不存在，1 表示存在，从低到高依次表示 a-z var bit int32 bit |= 1\u003c\u003c0 // 存储 a 存在 bit |= 1\u003c\u003c1 // 存储 b 存在 bit == 0x3ffffff // 判断 a-z 全部存在, 0x3ffffff 转为 2 进制 低 26 位全部为 1 // 内存占用情况 fmt.Printf(\"%d\\n\", unsafe.Sizeof(hash)) // 8 字节 fmt.Printf(\"%d\\n\", unsafe.Sizeof(arr)) // 26 字节 fmt.Printf(\"%d\\n\", unsafe.Sizeof(bit)) // 4 字节 ","date":"2022-10-11","objectID":"/posts/algorithm/bitmap/:1:0","series":null,"tags":["算法","bitmap"],"title":"BitMap","uri":"/posts/algorithm/bitmap/#bitmap原理"},{"categories":["技术"],"content":" 示例对比题目 go // hashmap 方式 package summultiples func SumMultiples(limit int, divisors ...int) int { sum := 0 set := make(map[int]bool) for _, d := range divisors { if d == 0 { continue } for m := d; m \u003c limit; m += d { if _, ok := set[m]; !ok { sum += m set[m] = true } } } return sum } // 性能测试结果 // BenchmarkSumMultiples-12 1593 700847 ns/op 487055 B/op 451 allocs/op // bitmap 方式 func SumMultiples(limit int, divisors ...int) (sum int) { bits := make([]byte, (limit\u003e\u003e3)+1) for _, divisor := range divisors { if divisor == 0 { continue } for i := 1; ; i++ { num := divisor * i if num \u003e= limit { break } index := num \u003e\u003e 3 pos := num \u0026 0x07 if bits[index]\u0026(1\u003c\u003cpos) == 0 { sum += num bits[index] |= 1 \u003c\u003c pos } } } return } // 性能测试结果 // BenchmarkSumMultiples-12 66572 17749 ns/op 4040 B/op 16 allocs/op ","date":"2022-10-11","objectID":"/posts/algorithm/bitmap/:2:0","series":null,"tags":["算法","bitmap"],"title":"BitMap","uri":"/posts/algorithm/bitmap/#示例对比"},{"categories":["技术"],"content":" 缺点（1）数据碰撞。比如将字符串映射到 BitMap 的时候会有碰撞的问题，那就可以考虑用 Bloom Filter 来解决，Bloom Filter 使用多个 Hash 函数来减少冲突的概率。 （2）数据稀疏。又比如要存入(10,8887983,93452134)这三个数据，我们需要建立一个 99999999 长度的 BitMap ，但是实际上只存了3个数据，这时候就有很大的空间浪费，碰到这种问题的话，可以通过引入 Roaring BitMap 来解决。 ","date":"2022-10-11","objectID":"/posts/algorithm/bitmap/:3:0","series":null,"tags":["算法","bitmap"],"title":"BitMap","uri":"/posts/algorithm/bitmap/#缺点"},{"categories":["技术"],"content":" 关于 Roaring BitMap后续再补充，先放两个链接吧 一文读懂比BitMap有更好性能的Roaring Bitmap 不深入而浅出 Roaring Bitmaps 的基本原理 ","date":"2022-10-11","objectID":"/posts/algorithm/bitmap/:4:0","series":null,"tags":["算法","bitmap"],"title":"BitMap","uri":"/posts/algorithm/bitmap/#关于-roaring-bitmap"},{"categories":["技术"],"content":" 参考链接 go bitmap 实现 bitmap原理及应用 ","date":"2022-10-11","objectID":"/posts/algorithm/bitmap/:5:0","series":null,"tags":["算法","bitmap"],"title":"BitMap","uri":"/posts/algorithm/bitmap/#参考链接"},{"categories":["技术"],"content":" 语法一览图维基百科 截图地址 ","date":"2022-09-22","objectID":"/posts/regex/:1:0","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#语法一览图"},{"categories":["技术"],"content":" 补充说明","date":"2022-09-22","objectID":"/posts/regex/:2:0","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#补充说明"},{"categories":["技术"],"content":" 分组非捕获组 sh `What is -?\\d+(?: (?:plus|minus|divided by|multiplied by) -?\\d+)*\\?` 上面的正则表达式可以匹配 “What is 5?” “What is -10?” “What is 4 plus 5?” “What is 3 minus 2 divided by 1?” “What is -2 multiplied by -4 divided by 2 plus 6?” ","date":"2022-09-22","objectID":"/posts/regex/:2:1","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#分组"},{"categories":["技术"],"content":" 测试网站https://regex101.com/ ","date":"2022-09-22","objectID":"/posts/regex/:3:0","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#测试网站"},{"categories":["技术"],"content":" 练习网站https://alf.nu/RegexGolf ","date":"2022-09-22","objectID":"/posts/regex/:4:0","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#练习网站"},{"categories":["技术"],"content":" 正则可视化https://regex-vis.com/ https://jex.im/regulex/#!flags=\u0026re=%5E(a%7Cb)*%3F%24 https://regexper.com/ ","date":"2022-09-22","objectID":"/posts/regex/:5:0","series":null,"tags":["正则"],"title":"正则相关","uri":"/posts/regex/#正则可视化"},{"categories":["技术"],"content":" 字符串 shell # 单引号：原样， 双引号：会解析变量 a=1 # = 两边不加空格 str_1='a=$a' # a=$a str_2=\"a=$a\" # a=1 str_3=`pwd` # `(反引号)内的字符串被当作shell命令，执行完结果返回给 str_3 # 字符串长度 s='abcd' len=${#s} # 等价于 ${#s[0]} # 截取 从第1个字符开始，截取3个字符，索引从0开始 sub_s=${s:1:3} ","date":"2022-09-20","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/:1:0","series":null,"tags":["unix","shell"],"title":"Shell编程-1","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/#字符串"},{"categories":["技术"],"content":" 获取当前脚本的绝对路径 shell BASE_DIR=$(cd $(dirname $0) \u0026\u0026 pwd) ","date":"2022-09-20","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/:2:0","series":null,"tags":["unix","shell"],"title":"Shell编程-1","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/#获取当前脚本的绝对路径"},{"categories":["技术"],"content":" 获取配置文件内容 shell # config.ini 内容如下： # [mysql] # host=\"1.1.1.3\" # [redis] # host=\"1.1.1.4\" # db=\"7\" # 获取配置信息 function getconf(){ Section=$1; Item=$2 _readIni=`awk -F '=' '/\\['$Section'\\]/{a=1}a==1\u0026\u0026$1~/'$Item'/{print $2;exit}' $BASE_DIR/config.ini` echo ${_readIni} } mysql_ip=$(getconf mysql host) # mysql_ip的值为：1.1.1.3 ","date":"2022-09-20","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/:3:0","series":null,"tags":["unix","shell"],"title":"Shell编程-1","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/#获取配置文件内容"},{"categories":["技术"],"content":" 显示帮助文档 shell #!/bin/sh ### ### my-script — start or end data analysis server ### ### Usage: ### my-script \u003ccommand\u003e ### ### Command: ### up Start data analysis server. ### down End data analysis server. ### -h Show this message. function help() { sed -rn 's/^### ?//;T;p' \"$0\" } if [[ $# == 0 ]] || [[ \"$1\" == \"-h\" ]]; then help exit 1 fi ","date":"2022-09-20","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/:4:0","series":null,"tags":["unix","shell"],"title":"Shell编程-1","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/#显示帮助文档"},{"categories":["技术"],"content":" shell 参数 $# 参数个数 $0 执行文件名（含路径）$1.. $n 第几个参数 $$ 当前进程 id $* 和 $@ 引用所有参数，区别 在双引号中，假设在脚本运行时写了三个参数 1、2、3，则 $* 等价于 \"1 2 3\"（传递了一个参数），而 $@ 等价于 \"1\" \"2\" \"3\"（传递了三个参数） $? 显示最后命令的退出状态 0 表示没有错误，其余表示有错误 ","date":"2022-09-20","objectID":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/:5:0","series":null,"tags":["unix","shell"],"title":"Shell编程-1","uri":"/posts/unixs/shell%E7%BC%96%E7%A8%8B-1/#shell-参数"},{"categories":["技术"],"content":" 配置本机可以接收远端的syslog日志根据需要启用udp或tcp模块，并设置端口，使其能够接收消息。 shell # vim /etc/rsyslog.conf $ModLoad imudp $UDPServerRun 514 重启 rsyslog 服务 bash systemctl restart rsyslog 防火墙放行端口 bash iptables -A INPUT -p udp --dport 514 -j ACCEPT 监听放行的端口 bash # -l listen -p source-port -4 ipv4 -u udp nc -l -p 514 -4 -u 在远端的主机上发送日志测试 shell logger \"hello logs\" --server {{hostname}} --port 514 以上监听将会显示类似如下信息： shell \u003c5\u003eSep 15 15:36:00 root: hello logs ","date":"2022-09-15","objectID":"/posts/unixs/rsyslog-%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E8%BF%9C%E7%A8%8B%E6%97%A5%E5%BF%97/:1:0","series":null,"tags":["unix","linux","syslog"],"title":" rsyslog 发送和接收远程日志","uri":"/posts/unixs/rsyslog-%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E8%BF%9C%E7%A8%8B%E6%97%A5%E5%BF%97/#配置本机可以接收远端的syslog日志"},{"categories":["技术"],"content":" 配置本机将syslog日志发送远端修改配置文件 shell # 旧版本语法 # An on-disk queue is created for this action. If the remote host is # down, messages are spooled to disk and sent when it is up again. #$ActionQueueFileName fwdRule1 # unique name prefix for spool files #$ActionQueueMaxDiskSpace 1g # 1gb space limit (use as much as possible) #$ActionQueueSaveOnShutdown on # save messages to disk on shutdown #$ActionQueueType LinkedList # run asynchronously #$ActionResumeRetryCount -1 # infinite retries if host is down # Facility.Severity *.* 表示所有 @表示传输协议（@表示udp，@@表示tcp），后面是ip和端口。 *.* @@remote-host:514 # 新版本语法 *.* action(type=\"omfwd\" target=\"192.0.2.1\" port=\"10514\" protocol=\"tcp\") *.* action(type=\"omfwd\" #使用 omfwd udp 和 tcp 转发插件 queue.type=\"LinkedList\" # 启用 LinkedList 内存队列，queue_type 可以是 direct、linkedlist 或者 fixedarray（它们是内存队列）或者磁盘 action.resumeRetryCount=\"-1\" # 远程主机关闭时重试次数 -1 无限次 queue.size=\"10000\" # 内存中队列的数量，非字节大小，防止峰值 queue.saveonshutdown=\"on\" # 本地主机关闭时，是否保存内存中的队列到磁盘 target=\"10.43.138.1\" Port=\"10514\" Protocol=\"tcp\") 重启 rsyslog 服务 shell systemctl restart rsyslog 对于 rsyslog 你也可以定义一个配置文件，重新启动一个新的进程。 shell rsyslogd -i /var/run/rsyslog_reindexer.pid -f /home/me/rsyslog_reindexer.conf 参考： 如何使用 tcp 和 udp 端口将 syslog 日志发送到远程服务器上 syslog 详解和配置 使用 rsyslog 重排 es 数据 rsys文档 man 手册 ","date":"2022-09-15","objectID":"/posts/unixs/rsyslog-%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E8%BF%9C%E7%A8%8B%E6%97%A5%E5%BF%97/:2:0","series":null,"tags":["unix","linux","syslog"],"title":" rsyslog 发送和接收远程日志","uri":"/posts/unixs/rsyslog-%E5%8F%91%E9%80%81%E5%92%8C%E6%8E%A5%E6%94%B6%E8%BF%9C%E7%A8%8B%E6%97%A5%E5%BF%97/#配置本机将syslog日志发送远端"},{"categories":["技术"],"content":"Go 语言中方法分为 具名和匿名，当匿名函数引用了外部变量时候就成了闭包函数。 方法是绑定到一个具体类型的特殊函数，方法依托类型。必须在编译时静态绑定。 接口定义了方法的集合，这些方法依托于运行时的对象（实现了接口中的方法），因此接口对应的方法是运行时动态绑定的。 Go 语言程序的初始化和执行化总是从 main.main 开始。如果 main 导入了其他包，则按顺序将他们包含进来，一直递归；包含时，先创建和初始化包的常量、变量，然后调用包中的 init 函数。如下： 要注意的是，在 main.main 函数执行之前所有代码都运行在同一个 Goroutine 中，也是运行在程序的主系统线程中。如果某个 init 函数内部用 go 关键字启动了新的 Goroutine 的话，新的 Goroutine 和 main.main 函数是并发执行的。 ","date":"2022-09-09","objectID":"/posts/golang/function-method-interface-of-golang/:0:0","series":null,"tags":["golang"],"title":"Function-Method-Interface-of-Golang","uri":"/posts/golang/function-method-interface-of-golang/#"},{"categories":["技术"],"content":" 函数Go 语言中函数可以有多个返回值。 参数和返回值都是以值的形式和被调用者交换数据的。 函数支持可变数量的参数，可变数量的参数必须是最后一个。可变数量的参数其实是一个切片类型的参数。 go func Swap(a,b int) (int, int){ return b,a } func Sum(a int, more ...int) int{ for _,v :=range more{ a += v } return a } 函数的返回值也可以用名字： go func Find(m map[int]int, key int) (value int, ok bool) { value, ok = m[key] return } derfer 可以在 reurn 之后修改返回值： go func Inc() (v int) { defer func(){ v++ } () return 42 } Go 语言中，函数传参是传值，只是针对了数据结构中固定部分传值。如：字符串或切片传参时，只是复制了对应数据结构中的指针、字符串长度、切片容量等值，并不包含指针指向的内容。可以将字符串和切片类型的参数转换为 reflect.StringHeader reflect.SliceHeader的结构体，可以更好的理解传值的含义： go func twice(x []int){ for i := range x{ x[i] *= 2 } } type IntSliceHeader struct { Data []int Len int Cap int } func twice(x IntSliceHeader){ for i:= 0; i\u003cx.Len; i++{ x.Data[i] *= 2 } } 如上，切片中的底层数组部分是通过隐式指针传递(指针本身依然是传值的，但是指针指向的却是同一份的内存数据)，所以被调用函数是可以通过修改指针指向的数据（指针本身的指向并没有发生变化），从而达到修改掉调用参数切片中数据的效果。 Go 语言支持递归调用，且调用深度逻辑上没有限制，函数调用栈不会出现溢出错误，这是因为 Go 语言运行时会根据需要动态调整函数栈的大小。栈最大可达到GB级。 Go 1.4 之前，动态栈采用的时分段式的动态栈，通俗说就是采用一个链表实现动态栈。链表节点位置不变，缺点: 跨节点地址位置不是一定是连续的，CPU缓存命中率低。 Go 1.4 之后改用连续的动态栈实现，通俗来说是采用一个类似动态数组来实现栈。缺点：当连续栈需要动态增长时候，需要重新分配内存空间并复制数据到新的空间，这导致了栈中之前变量的地址发生变化（Go 的运行时会自动更新这些变化）。但是这意味着 ，Go 语言中的指针不是固定不变的，不能随意将指针保存在数值变量中，不能将 Go 语言的地址随意保存到非 GC 的环境中（如：使用 CGO 时，C 语言不能长期持有 Go 与语言的地址）。 Go 语言隐藏了堆栈的细节，编译器和运行时帮我们做了还多工作，以下代码在 C/C++ 中不可行的，但在 Go 语言中并没问题： go // 如果参数变量在栈上的话，函数返回之后栈变量就失效了，返回的地址自然也应该失效了 func f(x int) *int { return \u0026x } // 内部虽然调用 new 函数创建了 *int 类型的指针对象，但是依然不知道它具体保存在哪里 func g() int { x := new(int) return *x } 需要注意的是：不要假设变量在内存中的位置是固定不变的，指针随时可能会变化，特别是在你不期望它变化的时候。 ","date":"2022-09-09","objectID":"/posts/golang/function-method-interface-of-golang/:1:0","series":null,"tags":["golang"],"title":"Function-Method-Interface-of-Golang","uri":"/posts/golang/function-method-interface-of-golang/#函数"},{"categories":["技术"],"content":" 方法Go 语言方法关联到结构体上，在编译阶段完成静态绑定。 我们可以给任意自定义的结构体添加一个或多个方法。方法和结构体定义必须在同一个包里。 其实，方法是由函数演化而来，只是将函数的第一个对象参数移动到函数名前面而已。因此我们可以使用方法表达式的特性将方法还原为普通类型函数： go // func ReadFile(f *File, offset int64, data []byte) int var ReadFile = (*File).Read // func CloseFile(f *File) error var CloseFile = (*File).Close f, _:= OpenFile(\"foo.txt\") ReadFile(f,0,data) CloseFile(f) 对于有些场景，我们并不关心具体操作对象的类型，只要满足通用的 行为 就可以了。在Go语言中，我们可以通过闭包做参数绑定，从而消除上边函数参数类型的的限制： go f, _:=OpenFile(\"foo.txt\") // 绑定了 f 对象 // func Close() error var Close = func() err(){ return (*File).Close(f) } // 绑定到了 f 对象 // func Read(offset int64, data []byte) int var Read = func(offset int64, data []byte) int { return (*File).Read(f, offset, data) } // 文件处理 Read(0, data) Close() // 使用方法值简化以上问题： // 方法值: 绑定到了 f 对象 // func Close() error var Close = f.Close // 方法值: 绑定到了 f 对象 // func Read(offset int64, data []byte) int var Read = f.Read // 文件处理 Read(0, data) Close() Go语言使用组合的方式支持继承。使用结构体中的匿名成员来实现继承。因此继承来的方法的接收者参数依然是匿名成员自身，而不是当前变量。 传统继承，子类的方法是运行时动态绑定到对象的，this 可能不是集类类型对应的对象，不确定性。 Go语言继承，子类方法是编译时静态绑定的，this 就是实现该方法的类型对象，确定性。 ","date":"2022-09-09","objectID":"/posts/golang/function-method-interface-of-golang/:2:0","series":null,"tags":["golang"],"title":"Function-Method-Interface-of-Golang","uri":"/posts/golang/function-method-interface-of-golang/#方法"},{"categories":["技术"],"content":" 接口Go 语言的接口满足隐式的鸭子类型。即：走路看着像鸭子，叫起来也像鸭子的，就可以当作鸭子。go语言中，一个对象只要看起来像是某个接口的实现，那么就可以把他作为该接口使用。 这种设计可以让你创建一个新的接口类型满足已经存在的具体类型却不用去破坏这些类型原有的定义。 Go语言的接口类型是延迟绑定的，可以实现虚函数的多态功能。 go // fmt.Fprintf 签名 func Fprintf(w io.Writer, format string, args ...interface{}) (int, error) // io.Writer 是用于输出的接口 type io.Writer interface { Write(p []byte) (n int, err error) } 我们可以定制自己的输出对象： go type UpperWrite struct{ io.Writer } func (p *UppperWriter) Write(data []byte) (n int, err error){ return p.Writer.Write(bytes.ToUpper(data)) } 如果满足 fmt.Stringer 接口，则默认使用对象的 String 方法的返回结构打印： go type UpperString string func (s UpperString) String() string { return strings.ToUpper(string(s)) } 对于基础类型（非接口），go 不支持隐式的类型转换，例如我们无法将一个 int 类型直接赋值给 int64类型的变量。 对于接口类型，可以隐式转换，对象和接口，接口和接口都可以转换： go var ( a io.ReadCloser = (*os.File)(f) // 隐式转换, *os.File 满足 io.ReadCloser 接口 b io.Reader = a // 隐式转换, io.ReadCloser 满足 io.Reader 接口 c io.Closer = a // 隐式转换, io.ReadCloser 满足 io.Closer 接口 d io.Reader = c.(io.Reader) // 显式转换, io.Closer 不满足 io.Reader 接口 ) 防止这种对象和接口太灵活的方式： 包含特殊方法，来区分接口。(君子协定) go type runtime.Error interface { error // RuntimeError 方法 用于避免其它类型无意中适配了该接口 RuntimeError() } type proto.Message interface { Reset() String() string // 用于避免其它类型无意中适配了该接口 ProtoMessage() } 这是可以伪造的。 更严格一点，可以定义一个私有方法。 go type testing.TB interface { Error(args ...interface{}) Errorf(format string, args ...interface{}) ... // A private method to prevent users implementing the // interface and so future additions to it will not // violate Go 1 compatibility. private() } 这是有代价的： 这个接口只能内部使用。 这种防护也不是绝对的。（可以在结构体中嵌入匿名类型的成员来绕过）。 我们在自己的 TB 结构体类型中重新实现了 Fatal 方法，然后通过将对象隐式转换为 testing.TB 接口类型（因为内嵌了匿名的 testing.TB 对象，因此是满足 testing.TB 接口的），然后通过 testing.TB 接口来调用我们自己的 Fatal 方法。 go package main import ( \"fmt\" \"testing\" ) type TB struct { testing.TB } func (p *TB) Fatal(args ...interface{}) { fmt.Println(\"TB.Fatal disabled!\") } func main() { var tb testing.TB = new(TB) tb.Fatal(\"Hello, playground\") } 这种通过嵌入匿名接口或嵌入匿名指针对象来实现继承的做法其实是一种纯虚继承，我们继承的只是接口指定的规范，真正的实现在运行的时候才被注入。比如，我们可以模拟实现一个gRPC的插件： go type grpcPlugin struct { *generator.Generator } func (p *grpcPlugin) Name() string { return \"grpc\" } func (p *grpcPlugin) Init(g *generator.Generator) { p.Generator = g } func (p *grpcPlugin) GenerateImports(file *generator.FileDescriptor) { if len(file.Service) == 0 { return } p.P(`import \"google.golang.org/grpc\"`) // ... } 构建的对象必须满足 https://github.com/golang/protobuf/blob/master/protoc-gen-go/generator/generator.go 中 Plugin 接口。 也就是说 grpcPlugin 类型的 GenerateImports 方法中使用的 p.P(...) 函数却是通过 Init 函数注入的 generator.Generator 对象实现（因为grpcPlugin 类型中比没有 P 方法，且只要一个成员）。 https://chai2010.cn/advanced-go-programming-book/ch1-basic/ch1-04-func-method-interface.html ","date":"2022-09-09","objectID":"/posts/golang/function-method-interface-of-golang/:3:0","series":null,"tags":["golang"],"title":"Function-Method-Interface-of-Golang","uri":"/posts/golang/function-method-interface-of-golang/#接口"},{"categories":["技术"],"content":" 数组","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:1:0","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#数组"},{"categories":["技术"],"content":" 定义方式 go var a[3] int var b = [...]int{1, 2, 3} var c = [...]int{2:3, 1:2} // 长度为3 {0,2,3} // 混合以上方式 var d = [...]int{1,2,4:5,6} // {1, 2, 0, 0, 5, 6} Go 语言中数组是值语义。一个数组变量即表示整个数组，它并不是隐式的指向第一个元素的指针（比如 C 语言的数组），而是一个完整的值。当一个数组变量被赋值或者被传递的时候，实际上会复制整个数组。如果数组较大的话，数组的赋值也会有较大的开销。为了避免复制数组带来的开销，可以传递一个指向数组的指针，但是数组指针并不是数组。 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:1:1","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#定义方式"},{"categories":["技术"],"content":" 循环 go // 数组元素大小为 0 ， 所以没有内存占用 var times [5][0]int for range times { fmt.Println(\"aaa\") } ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:1:2","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#循环"},{"categories":["技术"],"content":" 多种数组 go // 函数数组 图像解码器 var decoder1 [2]func(io.Reader) (image.Image, error) var decoder2 = [...]func (io.Reader) (image.Image,error) { png.Decode, jpeg.Decode, } // 接口数组 var unknown1 [2]interface {} var unknown2 = [...]interface {}{ 123 , \"你好\" } // 管道数组 var chanlist = [2]chan int{} ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:1:3","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#多种数组"},{"categories":["技术"],"content":" 空数组长度为 0 的数组部占用空间。可用于强调某种特有的类型。如管道的同步操作 go c1 := make(chan [0]int) go func() { fmt.Println(\"c1\") c1 \u003c- [0]int{} }() \u003c-c1 我们并不关心管道中传输数据的真实类型，其中管道接收和发送操作只是用于消息的同步。对于这种场景，我们用空数组来作为管道类型可以减少管道元素赋值时的开销。当然一般更倾向于用无类型的匿名结构体代替： go c2 := make(chan struct{}) go func(){ fmt.Println(\"c2\") c2 \u003c- struct{}{} // struct{} 部分是类型， {} 表示对应结构体的值 }() \u003c-c2 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:1:4","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#空数组"},{"categories":["技术"],"content":" 字符串字符串是一个不可改变的字节序列。go 源码要求 UTF8 编码。所以源码中的字符串字面量通常被解释为 UTF8编码的 unicode rune 序列。 底层结构: go type StringHeader struct { Data uintptr Len int } ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:2:0","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#字符串"},{"categories":["技术"],"content":" 字符串拼接 go s1:=\"hello\" s2:=\"world\" s3 = s1+s2 因为 字符串是一个不可改变的字节序列，所以以上代码性能较差，推荐使用 bytes.Buffer 或 strings.Builder。 go var sb strings.Builder sb.Write([]byte(\"hello world\")) sb.WriteByte('!') sb.WriteRune(rune(\"~\")) sb.WriteString(\"la la la ...\") sb.Strring() // 获取最终的字符串 ： \"hello world!~la la la ...\" strings.Builder 内部使用 slice 实现来保存和管理内容。通过一个指针来指向实际保存的内容（见下节切片内部数据结构）。因此，复制时，就复制了 slice 的指针，出现两个指针指向同一个位置，所以 strings.Builder 不允许复制。 go var b1 strings.Builder b1.WriteString(\"ABC\") b2 := b1 b2.WriteString(\"DEF\") // illegal use of non-zero Builder copied by value ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:2:1","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#字符串拼接"},{"categories":["技术"],"content":" 切片切片是一种简化版的动态数组。 go x := []int{2, 3, 5, 7, 11} y := x[1:3] // output: [2 3 5 7 11] len(x): 5 cap(x): 5 fmt.Println(x, \" len(x):\", len(x), \" cap(x):\", cap(x)) // output: [3 5] len(y): 2 cap(y): 4 fmt.Println(y, \" len(y):\", len(y), \" cap(y):\", cap(y)) ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:0","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#切片"},{"categories":["技术"],"content":" 定义方式 go var ( a []int // nil 切片, 和 nil 相等, 一般用来表示一个不存在的切片 b = []int{} // 空切片, 和 nil 不相等, 一般用来表示一个空的集合 c = []int{1, 2, 3} // 有 3 个元素的切片, len 和 cap 都为 3 d = c[:2] // 有 2 个元素的切片, len 为 2, cap 为 3 , [0,2) e = c[0:2:cap(c)] // 有 2 个元素的切片, len 为 2, cap 为 3 , [0,2) 且指定 cap 为 c 的 cap f = c[:0] // 有 0 个元素的切片, len 为 0, cap 为 3 g = make([]int, 3) // 有 3 个元素的切片, len 和 cap 都为 3 h = make([]int, 2, 3) // 有 2 个元素的切片, len 为 2, cap 为 3 i = make([]int, 0, 3) // 有 0 个元素的切片, len 为 0, cap 为 3 ) 切片的内部结构 go type SliceHeader struct { Data uintptr //引用数组指针地址 Len int // 切片的目前使用长度 Cap int // 切片的容量 } ps ： nil 切片和空切片的本质区别在于，nil 切片的 data 指向地址为 0 地址（即不分配地址空间）； 空切片共享相同的 data pointer，指向同一个大小为 0 内存地址。stackoverflow ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:1","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#定义方式-1"},{"categories":["技术"],"content":" 添加切片元素 go var a []int a = append(a, 1) // 追加 1 个元素 a = append(a, 1, 2, 3) // 追加多个元素, 手写解包方式 a = append(a, []int{1,2,3}...) // 追加 1 个切片, 切片需要解包 当容量不足时，append 会重新分配内存，导致巨大的内存分配和复制数据开销。 头部追加元素 go var a = []int{1,2,3} a = append([]int{0}, a...) a = append([]int{-3,-2,-1}, a...) 中间插入元素 go var a []int a = append(a[:i], append([]int{x}, a[i:]...)...) // 在第 i 个位置插入 x a = append(a[:i], append([]int{1,2,3}, a[i:]...)...) // 在第 i 个位置插入切片 // 以上会创建中间的临时切片，使用 copy 和 append 会避免 a = append(a, 0) // 切片扩展 1 个空间 copy(a[i+1:], a[i:]) // a[i:] 向后移动 1 个位置 a[i] = x // 设置新添加的元素 a = append(a, x...) // 为 x 切片扩展足够的空间 copy(a[i+len(x):], a[i:]) // a[i:] 向后移动 len(x) 个位置 copy(a[i:], x) // 复制新添加的切片 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:2","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#添加切片元素"},{"categories":["技术"],"content":" 删除切片元素尾部删除 go a = []int{1,2,3} a = a[:len(a)-1] // 删除尾部一个元素 a = a[:len(a)-n] // 删除尾部n个元素 删除开头 go a = []int{1, 2, 3} a = a[1:] // 删除开头 1 个元素 a = a[N:] // 删除开头 N 个元素 a = []int{1, 2, 3} a = append(a[:0], a[1:]...) // 删除开头 1 个元素 a = append(a[:0], a[N:]...) // 删除开头 N 个元素 a = []int{1, 2, 3} a = a[:copy(a, a[1:])] // 删除开头 1 个元素 a = a[:copy(a, a[N:])] // 删除开头 N 个元素 删除中间 go a = []int{1, 2, 3, ...} a = append(a[:i], a[i+1:]...) // 删除中间 1 个元素 a = append(a[:i], a[i+N:]...) // 删除中间 N 个元素 a = a[:i+copy(a[i:], a[i+1:])] // 删除中间 1 个元素 a = a[:i+copy(a[i:], a[i+N:])] // 删除中间 N 个元素 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:3","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#删除切片元素"},{"categories":["技术"],"content":" 切片内存技巧len 为 0 且 cap 不为 0 的切片非常有用，可以降低内存分配的次数。 go func TrimSpace(s []byte) []byte { b := s[:0] for _, x := range s { if x != ' ' { b = append(b, x) } } return b } func Filter(s []byte, fn func(x byte) bool) []byte { b := s[:0] for _, x := range s { if !fn(x) { b = append(b, x) } } return b } ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:4","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#切片内存技巧"},{"categories":["技术"],"content":" 避免内存泄漏切片操作并不会复制底层的数据。底层的数组会被保存在内存中，直到它不再被引用。但是有时候可能会因为一个小的内存引用而导致底层整个数组处于被使用的状态，这会延迟自动内存回收器对底层数组的回收。 例如，FindPhoneNumber 函数加载整个文件到内存，然后搜索第一个出现的电话号码，最后结果以切片方式返回。 go func FindPhoneNumber(filename string) []byte { b, _ := ioutil.ReadFile(filename) return regexp.MustCompile(\"[0-9]+\").Find(b) } 这段代码返回的 []byte 指向保存整个文件的数组。因为切片引用了整个原始数组，导致自动垃圾回收器不能及时释放底层数组的空间。一个小的需求可能导致需要长时间保存整个文件数据。这虽然这并不是传统意义上的内存泄漏，但是可能会拖慢系统的整体性能。 要修复这个问题，可以将感兴趣的数据复制到一个新的切片中（数据的传值是 Go 语言编程的一个哲学，虽然传值有一定的代价，但是换取的好处是切断了对原始数据的依赖）： go func FindPhoneNumber(filename string) []byte { b, _ := ioutil.ReadFile(filename) b = regexp.MustCompile(\"[0-9]+\").Find(b) return append([]byte{}, b...) } 指向指针的切片也可能遇到类似的问题： go var a []*int{ ... } a = a[:len(a)-1] // 被删除的最后一个元素依然被引用, 可能导致 GC 操作被阻碍 var a []*int{ ... } a[len(a)-1] = nil // GC 回收最后一个元素内存 a = a[:len(a)-1] // 从切片删除最后一个元素 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:3:5","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#避免内存泄漏"},{"categories":["技术"],"content":" 参考数组，字符串，切片 strings.Builder 的 7个要点 ","date":"2022-09-09","objectID":"/posts/golang/array-string-slice-of-golang/:4:0","series":null,"tags":["golang"],"title":"Golang 中的数组字符串和切片","uri":"/posts/golang/array-string-slice-of-golang/#参考"},{"categories":["生活"],"content":" 好奇心是什么好奇心是 【我不知道】：我不知道接下来会经历什么，但我愿意尝试。 好奇心是【我允许】：尝试的过程中，一些意外状况是正常现象，我允许自己做不好。 好奇心是【我不着急】：我不着急立刻得出结论，而是多观察一阵子再说。 真正的好奇心，意味着 【对新经验的开放】。 暂停实验室 ","date":"2022-06-13","objectID":"/posts/mess/%E5%A5%BD%E5%A5%87%E5%BF%83%E6%98%AF%E4%BB%80%E4%B9%88/:1:0","series":null,"tags":["日常","人间清醒"],"title":"好奇心是什么","uri":"/posts/mess/%E5%A5%BD%E5%A5%87%E5%BF%83%E6%98%AF%E4%BB%80%E4%B9%88/#好奇心是什么"},{"categories":["技术"],"content":" scp 多个主机并填写密码 shell #!/usr/bin/expect set filename [lindex $argv 0] set dir_path [lindex $argv 1] set IP_NOS_1 \"192.168.122.11\" set IP_NOS_2 \"192.168.122.12\" set IP_NOS_3 \"192.168.122.13\" set IP_NOS_4 \"192.168.122.14\" set IP_NOS_5 \"192.168.122.15\" set username \"root\" set passwd \"root\" set timeout 200 if { $argc!=2 } { puts \"WARNING ! srcipt needs two parameter !\" puts \" Usage: (parameter 1: filename) filename you want to transfer\" puts \" Usage: (parameter 2: path) path you want to save to where.\" puts \"this scripts will cp file into 5 kvm!\" exit 0 } for {set i 1} {$i \u003c 6} {incr i} { switch $i { 1 {set ip $IP_NOS_1} 3 {set ip $IP_NOS_3} 4 {set ip $IP_NOS_4} 5 {set ip $IP_NOS_5} } puts \"#---------------------copy file to kvm$i------------------#\" spawn scp ./$filename $username@$ip:$dir_path expect { \"(yes/no)?\" { send \"yes\"; exp_continue } \"assword:\" { send \"$passwd\\r\" } } expect \"100%\" sleep 1 } ","date":"2022-06-01","objectID":"/posts/unixs/expect%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC/:1:0","series":null,"tags":["linux","unix","脚本"],"title":"expect-自动交互脚本","uri":"/posts/unixs/expect%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC/#scp-多个主机并填写密码"},{"categories":["技术"],"content":" 参考网站expect范例 expect安装 ","date":"2022-06-01","objectID":"/posts/unixs/expect%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC/:2:0","series":null,"tags":["linux","unix","脚本"],"title":"expect-自动交互脚本","uri":"/posts/unixs/expect%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC/#参考网站"},{"categories":["技术"],"content":"Unix中，每个对象或者由文件表示，或者由进程表示。简单来讲，文件就是一个输出源或者输出目标。进程是一个正在运行的程序。文件提供对数据的访问，而进程使事件发生。 进程来源何处？系统如何管理自己的进程？如何控制自己的进程？ ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:0:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#"},{"categories":["技术"],"content":" 内核管理进程的方式精确讲，进程是一个加载到内存中准备运行的程序，再加上程序所需的数据，以及跟踪管理程序状态所需要的各种信息。 所有进程由内核管理。 当进程创建时，内核赋予其一个唯一标识号，叫做进程ID或 PID 。为了跟踪管理系统中的所有进程，内核维护一个进程表（process table）。按照 PID 索引，每个进程在进程表中有一个条目。除了 PID ，每个条目还包含有描述和管理进程所需的信息。 进程共享系统的资源：处理器、内存、I/O设备、网络连接等。为了管理这样一个复杂的工作负荷，内核提供了一个复杂的调度服务，即 调度器（scheduler）。 调度器一直维护这一个所有正在等待执行的进程的列表。通过复杂的算法，调度器每次选择一个进程，给予这个进程在一个短暂的事件间隔（称为时间片）中运行的机会（多处理器系统中，调度器每次可以选择多个进程）。 时间片又称 CPU时间。通常是10毫秒（千分之十秒）的 CPU 时间。一旦时间片用尽，该进程就返回调度列表，由内核启动另一个进程。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:1:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#内核管理进程的方式"},{"categories":["技术"],"content":" 进程分叉到死亡进程是如何创建的呢？ 内核为进程提供基本服务： 内存管理（虚拟内存管理，包括分页） 进程管理（进程创建、终止、调度） 进程间通信（本地、网络） 输入、输出（通过设备驱动程序，即与物理设备实际通信的程序） 文件管理 安全和访问控制 网络访问（如 TCP、IP） 当进程需要内核执行服务时，它就使用系统调用发送请求。最重要的系统调用就是那些用户进程控制和 I/O 的系统调用。 系统调用 目的 fork 创建当前进程的一个副本（原始进程称为 父进程（parent process），新进程和父进程一模一样，称为 子进程（child process）） wait 等待另一个进程结束执行 exec 在当前进程中执行一个新的程序 exit 终止当前进程 kill 向另一个进程发送一个信号 open 打开一个用户读取或写入的文件 read 从文件中读取数据 write 向文件中写入数据 close 关闭文件 显示当前 shell 的 PID：echo $$ 命令有两种类型：内部命令和外部命令。内部命令直接由shell解释不创建新进程。外部命令需要shell运行一个单独的程序。 shell 使用 fork 系统调用创建一个全新的进程。 子进程使用 exec 系统调用将它自身从运行 shell 的进程变成运行外部程序的进程； 父进程使用 wait 系统调用暂停，直到子进程执行结束。 外部程序结束，子进程使用 exit 系统调用停止自身。称为进程 死亡（die） 或 终止(terminate)。故意停止一个进程，称为 杀死（kill）。 进程死亡时，进程所使用的资源（内存、文件等）都被释放，从而可以被其他进程使用。 子进程已死但仍然没有被回收称为 僵进程（zombie），尽管僵进程已经死了，进程表里仍然保存这自己的条目，这是因为该条目包含着，最近死亡的子进程的数据，而父进程可能对这些数据感兴趣。 在类UNIX系统中，僵尸进程是指完成执行（通过exit系统调用，或运行时发生致命错误或收到终止信号所致），但在操作系统的进程表中仍然存在其进程控制块，处于\"终止状态“的进程。这发生于子进程需要保留表项以允许其父进程读取子进程的退出状态：一旦退出态通过wait系统调用读取，僵尸进程条目就从进程表中删除，称之为\"回收”（reaped）。正常情况下，进程直接被其父进程wait并由系统回收。进程长时间保持僵尸状态一般是错误的并导致资源泄漏。 父进程一直在等待子进程的死亡，当子进程成为僵进程之后，立即被内核唤醒。 现在父进程有机会查看进程表中的僵进程条目，看看发生了什么结果。 然后内核将进程表中的僵进程条目移除。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:1:1","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#进程分叉到死亡"},{"categories":["技术"],"content":" 孤儿进程和废弃进程 当父进程分叉后，意外死亡 ，只剩下子进程，会发生什么？ ​ 子进程继续执行，称为 孤儿 进程。孤儿进程完成工作死亡时，没有父进程被唤醒，以僵进程的形式存在。 ​ 现代操作系统，孤儿进程将自动被 #1 进程 （init进程）收养，孤儿进程死亡时，init 进程充当父进程，快速清理僵进程。 父进程创建子进程，但是没有等待进程死亡？（仅当程序有 bug ，允许程序创建子进程而不等待子进程死亡） 情况1. 父进程死亡，形成孤儿进程如上。 情况2. 子进程死亡时，子进程就称为了僵进程（没有父进程读取子进程的退出状态，导致进程表的条目无法被回收）。 ​ 如果程序以偶然的方式创建了一个僵进程，那么将没有办法清除这个进程（kill 对僵进程无效），毕竟无法杀死已经死掉的东西。 ​ 为了清除成为僵进程的废弃子进程，可以使用 kill 程序终止父进程，父进程死亡，僵进程就成为孤儿进程，从而自动被init进程收养。适当的时候，init 进程将履行继父的职责，清除僵尸进程的参与信息。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:1:2","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#孤儿进程和废弃进程"},{"categories":["技术"],"content":" 区分父进程和子进程fork 两个相同的进程，父进程和子进程。如果两个进程相同，那么父进程怎么直到它是父进程，子进程如何知道他是子进程呢？ 当 fork 系统调用结束它的工作时，它向父进程和子进程各传递一个数值，这个数值成为返回值。子进程返回值是 0，父进程的返回值是新创建的进程的的进程 ID。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:1:3","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#区分父进程和子进程"},{"categories":["技术"],"content":" 第一个进程：init如果进程是使用 fork 创建，那么每个子进程必须有一个父进程。那么在某个地方必然存在第一个进程。 在引导过程的末尾，内核 “手动” 创建一个特殊的进程，不是通过fork。这个进程的 PID 是 0。称为 空闲进程（idle process）。 在执行了一些重要的功能（例如 初始化内核所需要的数据结构）之后，空闲进程进行分叉，创建#1号进程。然后空闲进程执行一个非常简单的程序，本质是一个无穷的循环，不做任何事情（因此这个进程被命名为空闲进程）。这里的思想是，每当没有进程等待执行时，调度器就运行空闲进程。 进程#1执行设置内核以及结束引导过程所需的剩余步骤。因此称它为 初始化进程（init process），具体而言，初始化进程，打开系统控制台，挂载根文件系统，运行包含在文件/etc/inittab 中的脚本。在这一过程中，init 多次fork，创建运行系统所需的基本进程（如，运行级别设置），并允许用户登录，在这一过程中，init 成为系统中所有其他进程的祖先。 与 空闲进程（#0） 进程不同，初始化进程 （#1） 进程永远不会停止。且是进程表中的第一个进程，一直存在进程表中，直到系统关闭。 Linux系统在引导时加载Linux内核后，便由Linux内核加载init程序，由init程序完成余下的引导过程，比如加载执行级别，加载服务，启动Shell/图形化界面等等。– 维基百科 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:1:4","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#第一个进程init"},{"categories":["技术"],"content":" 前台进程与后台进程在命令的末尾键入一个 \u0026 字符，可以将前台进程转换成后台进程 (也叫 异步进程（asynchronous process）)。 前台进程: shell 在提示用户输入新命令之前等待当前程序结束，这样的进程。 后台进程：shell 启动一个程序，但又让该程序自己运行，这样的进程。 大多数unix程序从标准输入（stdin）读取输入，将输出写到标准输出（stdout），错误消息则写入标准错误（stderr）。stdin 相连 键盘，stdout 和 stderr 相连显示器。我们可以重定向 stdin、 stdout 和 stderr。如果用户想要在进程自己结束之前终止进程，可以按 ^C 发送 intr 信号，或者 ^\\ 发送 quit 信号，区别是 quit 信号会生成一个供调试使用的磁芯转存。 异步进程 默认情况下标准输入与空文件 dev/null 相连，也不响应 intr 和 quit 信号。 当后台程序试图从标准 I/O读取并写到标准 I/O时，会发生什么？ 后台运行的程序试图从 stdin 读取数据，但是stdin 什么都没有，进程将无限期的暂停，等待输入。（可通过fg 命令，将其移动至前台） 后台运行的程序向 stdout 和 stderr 写入数据时，将显示在显示器上。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#前台进程与后台进程"},{"categories":["技术"],"content":" 作业控制作业控制使多个进程同时运行成为可能：一个进程在前台运行，其他进程在后台运行。 作业的本质是将每条输入的命令视为一个作业，该作业由一个唯一的作业号（job number）标识。 作业控制命令 jobs 显示作业列表 ps 显示进程列表 fg 将作业移动至前台 bg 将作业移动至后台 supend 挂起当前 shell ^Z 挂起当前前台作业 kill 向作业发送信号，默认情况下，终止作业 变量 echo $$ 显示当前 shell 的 PID echo $! 显示上一条移至后台的命令的 PID 终端设置 stty tostop 挂起试图向终端写数据的后台作业 stty -tostop 关闭 tostop shell 选项：Bash 、Korn shell set -o monitor 允许作业控制 set +o nomonitor 关闭 monitor set -o notify 当后台作业结束时，立即通报 set +o nonotify 关闭 notify 作业和进程的区别 进程是正在执行或者准备执行的程序，作业指解释整个命令行所需的全部进程。 进程内核控制的，作业 shell 控制的。 内核使用进程表记录进程，shell 使用 **作业表（job table）**记录作业。 shell # 1个进程；1个作业 date # 4个进程；1个作业 who | cut -c 1-8 | sort | uniq -c date; who; uptime; cal 12 2008 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:1","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#作业控制"},{"categories":["技术"],"content":" 前后台运行示例 作业结束通知显示时机 shell # 后台运行作业 ls \u003e temp \u0026 # 此时会输出 [作业ID] 进程ID [1] 4003 # 如果一个作业由多个程序构成的话 显示最后一个程序的进程ID who | cut -c 1-8 | sort |uniq -c \u0026 # 会输出 4356 是 uniq 的进程ID [2] 4356 # 当后台作业结束时，shell 不会立即通知您，以防止干扰您正在做的事情。 # shell 会一直等待，直到要显示下一个 shell 提示时显示，如下 [1] Done ls \u003e temp # 强制作业结束时，立即通知您 set -o notify # 恢复默认设置 set +o notify # C-Shell 家族 分别对应 set notify unset notify 挂起作业任何时候作业有三种状态：前台运行；后台运行；暂停，等待信号恢复。 暂停前台作业，可以按 ^Z 键(Ctrl-Z)，即发送 susp 信号。我们称将进程**挂起（suspend）**或进程 停止（stop）（实际是临时中止，可以重新启动）。永久停止必须 ^C 键或 kill 命令。 恢复挂起的程序，使用 fg 命令。 shell # eg： 挂起 vim 然后查看 cal 说明书，然后再返回 vim vi a.txt ^Z # 挂起vim man cal fg # 返回vim 挂起作业时，进程会无限期停止。如果试图注销系统，shell 会得到一条警告，你可以 fg 将挂起的作业移动到前台，并正确的退出程序，或者第二次注销系统（可能会丢失数据）。 shell # eg： 挂起shell # 切换 root 用户 su - # 做一些工作 ... # 挂起root用户的 shell,有密码需 -f 强制挂起 suspend -f # 做一些工作... # 回到root的shell fg ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:2","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#前后台运行示例"},{"categories":["技术"],"content":" 前后台运行示例 作业结束通知显示时机 shell # 后台运行作业 ls \u003e temp \u0026 # 此时会输出 [作业ID] 进程ID [1] 4003 # 如果一个作业由多个程序构成的话 显示最后一个程序的进程ID who | cut -c 1-8 | sort |uniq -c \u0026 # 会输出 4356 是 uniq 的进程ID [2] 4356 # 当后台作业结束时，shell 不会立即通知您，以防止干扰您正在做的事情。 # shell 会一直等待，直到要显示下一个 shell 提示时显示，如下 [1] Done ls \u003e temp # 强制作业结束时，立即通知您 set -o notify # 恢复默认设置 set +o notify # C-Shell 家族 分别对应 set notify unset notify 挂起作业任何时候作业有三种状态：前台运行；后台运行；暂停，等待信号恢复。 暂停前台作业，可以按 ^Z 键(Ctrl-Z)，即发送 susp 信号。我们称将进程**挂起（suspend）**或进程 停止（stop）（实际是临时中止，可以重新启动）。永久停止必须 ^C 键或 kill 命令。 恢复挂起的程序，使用 fg 命令。 shell # eg： 挂起 vim 然后查看 cal 说明书，然后再返回 vim vi a.txt ^Z # 挂起vim man cal fg # 返回vim 挂起作业时，进程会无限期停止。如果试图注销系统，shell 会得到一条警告，你可以 fg 将挂起的作业移动到前台，并正确的退出程序，或者第二次注销系统（可能会丢失数据）。 shell # eg： 挂起shell # 切换 root 用户 su - # 做一些工作 ... # 挂起root用户的 shell,有密码需 -f 强制挂起 suspend -f # 做一些工作... # 回到root的shell fg ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:2","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#作业结束通知显示时机"},{"categories":["技术"],"content":" 前后台运行示例 作业结束通知显示时机 shell # 后台运行作业 ls \u003e temp \u0026 # 此时会输出 [作业ID] 进程ID [1] 4003 # 如果一个作业由多个程序构成的话 显示最后一个程序的进程ID who | cut -c 1-8 | sort |uniq -c \u0026 # 会输出 4356 是 uniq 的进程ID [2] 4356 # 当后台作业结束时，shell 不会立即通知您，以防止干扰您正在做的事情。 # shell 会一直等待，直到要显示下一个 shell 提示时显示，如下 [1] Done ls \u003e temp # 强制作业结束时，立即通知您 set -o notify # 恢复默认设置 set +o notify # C-Shell 家族 分别对应 set notify unset notify 挂起作业任何时候作业有三种状态：前台运行；后台运行；暂停，等待信号恢复。 暂停前台作业，可以按 ^Z 键(Ctrl-Z)，即发送 susp 信号。我们称将进程**挂起（suspend）**或进程 停止（stop）（实际是临时中止，可以重新启动）。永久停止必须 ^C 键或 kill 命令。 恢复挂起的程序，使用 fg 命令。 shell # eg： 挂起 vim 然后查看 cal 说明书，然后再返回 vim vi a.txt ^Z # 挂起vim man cal fg # 返回vim 挂起作业时，进程会无限期停止。如果试图注销系统，shell 会得到一条警告，你可以 fg 将挂起的作业移动到前台，并正确的退出程序，或者第二次注销系统（可能会丢失数据）。 shell # eg： 挂起shell # 切换 root 用户 su - # 做一些工作 ... # 挂起root用户的 shell,有密码需 -f 强制挂起 suspend -f # 做一些工作... # 回到root的shell fg ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:2","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#挂起作业"},{"categories":["技术"],"content":" 显示作业列表 shell # -l 会显示进程号 jobs [-l] # 将作业移动至前台 fg %[job] %[job] # 将作业移动至后台 bg [%job...] 作业号 含义 %% 当前作业 %+ 当前作业 %- 前一个作业 %n 作业#n %name 含有指定命令名的作业 %?name 命令中任意位置含有name的作业 ps: 当准备后台运行程序，但是输入命令时忘记键入 \u0026 字符，只需按下 ^Z 挂起作业，然后使用 bg 命令将作业移动至后台。 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:2:3","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#显示作业列表"},{"categories":["技术"],"content":" ps 程序的使用ps 难用的原因是20世纪80年代，unix 分支：官方 Unix（AT\u0026T公司）、非官方（加利福尼亚大学伯克利分校）。UNIX 选项 和 BSD 选项。 unix 选项以连字符 - 开头，BSD 选项没有连字符。 shell # unix 选项 ps [-aefFly] [-p pid] [-u userid] # BSD ps [ajluvx] [p pid] [U userid] UNIX 选项 显示哪些进程？ ps 与您用户标识和终端相关的进程 ps -a 与任何用户标识和终端相关的进程 ps -e 所有进程（包含守护进程） ps -p pid 与指定进程 ID pid 相关的进程 ps -u userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TTY TIME CMD ps -f UID PID PPID C TTY TIME CMD ps -F UID PID PPID C SZ RSS STIME TTY TIME CMD ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD ps -ly S UID PID PPID C PRI NI RSS SZ WCHAN TTY TIME CMD 有用的特殊组合 ps 显示自己的进程 ps -ef 显示所有用户进程，完整给输出 ps -a 显示所有非守护进程的 进程 ps -t - （仅显示所有守护进程） 数据列说明： UNIX 标题 含义 ADDR 进程表中的 虚拟地址 C 处理器利用率（废弃率） CMD 正在执行的命令名 F 与进程相关的标识 NI nice 值，用于设置优先级 PID 进程 ID PPID 父进程 ID PRI 优先级（较大的数字：优先级低） RSS 内存驻留空间 S 状态代码（D、R、S、T、Z） STIME 累计系统时间 SZ 物理页的大小（内存管理） TIME 累计 CPU 时间 TTY 控制终端的完整名称 UID 用户标识 WCHAN 等待通道 BSD 选项 显示哪些进程？ ps 与你的用户标识和终端相关的进程 ps a 与任何用户标识和终端相关的进程 ps e 所有进程（包含守护进程） ps p pid 与指定进程 ID pid 相关的进程 ps U userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TT STAT TIME COMMAND ps j USER PID PPID PGID SESS JOBC STAT TT TIME COMMAND ps l UID PID PPID CPU PRI NI VSZ RSS WCHAN STAT TT TIME COMMAND ps u USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND ps v PID STAT TIME SL RE PAGEN VSZ RSS LIM TSIZ %CPU %MEM COMMAND 有用的特殊组合 ps 显示自己的进程 ps ax 显示所有进程 ps aux 显示所有进程，完整输出 数据列说明： BSD 标题 含义 %CPU CPU 使用百分比 %MEM 真实内存使用百分比 CMD 正被执行的命令的名称 COMMAND 正被执行的命令的完整名称 CPU 短期 CPU 使用（调度） JOBC 作业控制统计 LIM 内存使用限额 NI nice 值，用户设置优先级 PAGEIN 总的缺页错误（内存错误 ） PGID 进程组号 PID 进程号ID PPID 父进程的进程ID PRI 调度有限级 RE 内存驻留时间（单位秒） RSS 内存驻留空间大小（内存管理） SESS 会话指针 SL 睡眠时间（单位秒） STARTED 定时启动 STAT 状态代码（O、R、S、T、Z） TIME 积累的CPU时间 TSIZ 文本大小（单位KB） TT 控制终端的缩写名称 TTY 控制终端的完整名称 UID 用户标识 USER 用户名 VSZ 虚拟大小（单位KB） WCHAN 等待通道 状态码说明： Linux、FreeBSD D 不可中断睡眠：等待时间结束（通常是I/O，D=“磁盘”） I 空闲：超过20秒的睡眠（仅仅适用于 FreeBSD） R 正在运行或可运行（可运行=正在运行队列中等待） S 可中断睡眠：等待事件结束 T 挂起：由作业控制信号挂起或者因为追踪而被挂起 Z 僵进程：终止后，父进程没有等待 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:3:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#ps-程序的使用"},{"categories":["技术"],"content":" ps 程序的使用ps 难用的原因是20世纪80年代，unix 分支：官方 Unix（AT\u0026T公司）、非官方（加利福尼亚大学伯克利分校）。UNIX 选项 和 BSD 选项。 unix 选项以连字符 - 开头，BSD 选项没有连字符。 shell # unix 选项 ps [-aefFly] [-p pid] [-u userid] # BSD ps [ajluvx] [p pid] [U userid] UNIX 选项 显示哪些进程？ ps 与您用户标识和终端相关的进程 ps -a 与任何用户标识和终端相关的进程 ps -e 所有进程（包含守护进程） ps -p pid 与指定进程 ID pid 相关的进程 ps -u userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TTY TIME CMD ps -f UID PID PPID C TTY TIME CMD ps -F UID PID PPID C SZ RSS STIME TTY TIME CMD ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD ps -ly S UID PID PPID C PRI NI RSS SZ WCHAN TTY TIME CMD 有用的特殊组合 ps 显示自己的进程 ps -ef 显示所有用户进程，完整给输出 ps -a 显示所有非守护进程的 进程 ps -t - （仅显示所有守护进程） 数据列说明： UNIX 标题 含义 ADDR 进程表中的 虚拟地址 C 处理器利用率（废弃率） CMD 正在执行的命令名 F 与进程相关的标识 NI nice 值，用于设置优先级 PID 进程 ID PPID 父进程 ID PRI 优先级（较大的数字：优先级低） RSS 内存驻留空间 S 状态代码（D、R、S、T、Z） STIME 累计系统时间 SZ 物理页的大小（内存管理） TIME 累计 CPU 时间 TTY 控制终端的完整名称 UID 用户标识 WCHAN 等待通道 BSD 选项 显示哪些进程？ ps 与你的用户标识和终端相关的进程 ps a 与任何用户标识和终端相关的进程 ps e 所有进程（包含守护进程） ps p pid 与指定进程 ID pid 相关的进程 ps U userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TT STAT TIME COMMAND ps j USER PID PPID PGID SESS JOBC STAT TT TIME COMMAND ps l UID PID PPID CPU PRI NI VSZ RSS WCHAN STAT TT TIME COMMAND ps u USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND ps v PID STAT TIME SL RE PAGEN VSZ RSS LIM TSIZ %CPU %MEM COMMAND 有用的特殊组合 ps 显示自己的进程 ps ax 显示所有进程 ps aux 显示所有进程，完整输出 数据列说明： BSD 标题 含义 %CPU CPU 使用百分比 %MEM 真实内存使用百分比 CMD 正被执行的命令的名称 COMMAND 正被执行的命令的完整名称 CPU 短期 CPU 使用（调度） JOBC 作业控制统计 LIM 内存使用限额 NI nice 值，用户设置优先级 PAGEIN 总的缺页错误（内存错误 ） PGID 进程组号 PID 进程号ID PPID 父进程的进程ID PRI 调度有限级 RE 内存驻留时间（单位秒） RSS 内存驻留空间大小（内存管理） SESS 会话指针 SL 睡眠时间（单位秒） STARTED 定时启动 STAT 状态代码（O、R、S、T、Z） TIME 积累的CPU时间 TSIZ 文本大小（单位KB） TT 控制终端的缩写名称 TTY 控制终端的完整名称 UID 用户标识 USER 用户名 VSZ 虚拟大小（单位KB） WCHAN 等待通道 状态码说明： Linux、FreeBSD D 不可中断睡眠：等待时间结束（通常是I/O，D=“磁盘”） I 空闲：超过20秒的睡眠（仅仅适用于 FreeBSD） R 正在运行或可运行（可运行=正在运行队列中等待） S 可中断睡眠：等待事件结束 T 挂起：由作业控制信号挂起或者因为追踪而被挂起 Z 僵进程：终止后，父进程没有等待 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:3:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#unix-选项"},{"categories":["技术"],"content":" ps 程序的使用ps 难用的原因是20世纪80年代，unix 分支：官方 Unix（AT\u0026T公司）、非官方（加利福尼亚大学伯克利分校）。UNIX 选项 和 BSD 选项。 unix 选项以连字符 - 开头，BSD 选项没有连字符。 shell # unix 选项 ps [-aefFly] [-p pid] [-u userid] # BSD ps [ajluvx] [p pid] [U userid] UNIX 选项 显示哪些进程？ ps 与您用户标识和终端相关的进程 ps -a 与任何用户标识和终端相关的进程 ps -e 所有进程（包含守护进程） ps -p pid 与指定进程 ID pid 相关的进程 ps -u userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TTY TIME CMD ps -f UID PID PPID C TTY TIME CMD ps -F UID PID PPID C SZ RSS STIME TTY TIME CMD ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD ps -ly S UID PID PPID C PRI NI RSS SZ WCHAN TTY TIME CMD 有用的特殊组合 ps 显示自己的进程 ps -ef 显示所有用户进程，完整给输出 ps -a 显示所有非守护进程的 进程 ps -t - （仅显示所有守护进程） 数据列说明： UNIX 标题 含义 ADDR 进程表中的 虚拟地址 C 处理器利用率（废弃率） CMD 正在执行的命令名 F 与进程相关的标识 NI nice 值，用于设置优先级 PID 进程 ID PPID 父进程 ID PRI 优先级（较大的数字：优先级低） RSS 内存驻留空间 S 状态代码（D、R、S、T、Z） STIME 累计系统时间 SZ 物理页的大小（内存管理） TIME 累计 CPU 时间 TTY 控制终端的完整名称 UID 用户标识 WCHAN 等待通道 BSD 选项 显示哪些进程？ ps 与你的用户标识和终端相关的进程 ps a 与任何用户标识和终端相关的进程 ps e 所有进程（包含守护进程） ps p pid 与指定进程 ID pid 相关的进程 ps U userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TT STAT TIME COMMAND ps j USER PID PPID PGID SESS JOBC STAT TT TIME COMMAND ps l UID PID PPID CPU PRI NI VSZ RSS WCHAN STAT TT TIME COMMAND ps u USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND ps v PID STAT TIME SL RE PAGEN VSZ RSS LIM TSIZ %CPU %MEM COMMAND 有用的特殊组合 ps 显示自己的进程 ps ax 显示所有进程 ps aux 显示所有进程，完整输出 数据列说明： BSD 标题 含义 %CPU CPU 使用百分比 %MEM 真实内存使用百分比 CMD 正被执行的命令的名称 COMMAND 正被执行的命令的完整名称 CPU 短期 CPU 使用（调度） JOBC 作业控制统计 LIM 内存使用限额 NI nice 值，用户设置优先级 PAGEIN 总的缺页错误（内存错误 ） PGID 进程组号 PID 进程号ID PPID 父进程的进程ID PRI 调度有限级 RE 内存驻留时间（单位秒） RSS 内存驻留空间大小（内存管理） SESS 会话指针 SL 睡眠时间（单位秒） STARTED 定时启动 STAT 状态代码（O、R、S、T、Z） TIME 积累的CPU时间 TSIZ 文本大小（单位KB） TT 控制终端的缩写名称 TTY 控制终端的完整名称 UID 用户标识 USER 用户名 VSZ 虚拟大小（单位KB） WCHAN 等待通道 状态码说明： Linux、FreeBSD D 不可中断睡眠：等待时间结束（通常是I/O，D=“磁盘”） I 空闲：超过20秒的睡眠（仅仅适用于 FreeBSD） R 正在运行或可运行（可运行=正在运行队列中等待） S 可中断睡眠：等待事件结束 T 挂起：由作业控制信号挂起或者因为追踪而被挂起 Z 僵进程：终止后，父进程没有等待 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:3:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#bsd-选项"},{"categories":["技术"],"content":" ps 程序的使用ps 难用的原因是20世纪80年代，unix 分支：官方 Unix（AT\u0026T公司）、非官方（加利福尼亚大学伯克利分校）。UNIX 选项 和 BSD 选项。 unix 选项以连字符 - 开头，BSD 选项没有连字符。 shell # unix 选项 ps [-aefFly] [-p pid] [-u userid] # BSD ps [ajluvx] [p pid] [U userid] UNIX 选项 显示哪些进程？ ps 与您用户标识和终端相关的进程 ps -a 与任何用户标识和终端相关的进程 ps -e 所有进程（包含守护进程） ps -p pid 与指定进程 ID pid 相关的进程 ps -u userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TTY TIME CMD ps -f UID PID PPID C TTY TIME CMD ps -F UID PID PPID C SZ RSS STIME TTY TIME CMD ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD ps -ly S UID PID PPID C PRI NI RSS SZ WCHAN TTY TIME CMD 有用的特殊组合 ps 显示自己的进程 ps -ef 显示所有用户进程，完整给输出 ps -a 显示所有非守护进程的 进程 ps -t - （仅显示所有守护进程） 数据列说明： UNIX 标题 含义 ADDR 进程表中的 虚拟地址 C 处理器利用率（废弃率） CMD 正在执行的命令名 F 与进程相关的标识 NI nice 值，用于设置优先级 PID 进程 ID PPID 父进程 ID PRI 优先级（较大的数字：优先级低） RSS 内存驻留空间 S 状态代码（D、R、S、T、Z） STIME 累计系统时间 SZ 物理页的大小（内存管理） TIME 累计 CPU 时间 TTY 控制终端的完整名称 UID 用户标识 WCHAN 等待通道 BSD 选项 显示哪些进程？ ps 与你的用户标识和终端相关的进程 ps a 与任何用户标识和终端相关的进程 ps e 所有进程（包含守护进程） ps p pid 与指定进程 ID pid 相关的进程 ps U userid 与指定用户标识 userid 相关的进程 显示哪些数据列？ ps PID TT STAT TIME COMMAND ps j USER PID PPID PGID SESS JOBC STAT TT TIME COMMAND ps l UID PID PPID CPU PRI NI VSZ RSS WCHAN STAT TT TIME COMMAND ps u USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND ps v PID STAT TIME SL RE PAGEN VSZ RSS LIM TSIZ %CPU %MEM COMMAND 有用的特殊组合 ps 显示自己的进程 ps ax 显示所有进程 ps aux 显示所有进程，完整输出 数据列说明： BSD 标题 含义 %CPU CPU 使用百分比 %MEM 真实内存使用百分比 CMD 正被执行的命令的名称 COMMAND 正被执行的命令的完整名称 CPU 短期 CPU 使用（调度） JOBC 作业控制统计 LIM 内存使用限额 NI nice 值，用户设置优先级 PAGEIN 总的缺页错误（内存错误 ） PGID 进程组号 PID 进程号ID PPID 父进程的进程ID PRI 调度有限级 RE 内存驻留时间（单位秒） RSS 内存驻留空间大小（内存管理） SESS 会话指针 SL 睡眠时间（单位秒） STARTED 定时启动 STAT 状态代码（O、R、S、T、Z） TIME 积累的CPU时间 TSIZ 文本大小（单位KB） TT 控制终端的缩写名称 TTY 控制终端的完整名称 UID 用户标识 USER 用户名 VSZ 虚拟大小（单位KB） WCHAN 等待通道 状态码说明： Linux、FreeBSD D 不可中断睡眠：等待时间结束（通常是I/O，D=“磁盘”） I 空闲：超过20秒的睡眠（仅仅适用于 FreeBSD） R 正在运行或可运行（可运行=正在运行队列中等待） S 可中断睡眠：等待事件结束 T 挂起：由作业控制信号挂起或者因为追踪而被挂起 Z 僵进程：终止后，父进程没有等待 ","date":"2022-05-07","objectID":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/:3:0","series":null,"tags":["折腾","进程","unix","linux"],"title":"进程和作业控制","uri":"/posts/unixs/%E8%BF%9B%E7%A8%8B%E5%92%8C%E4%BD%9C%E4%B8%9A%E6%8E%A7%E5%88%B6/#状态码说明"},{"categories":["技术"],"content":" 排序、区域设置unix 的默认排序方式取决于你使用的区域设置，如：现在有A、a、B、b、C、c 几个文件，执行 ls 列举文件： 如果使用 C 区域设置，你会得到：A B C a b c (基于 ASCII 码)； 如果使用 en_US, 你将会得到： a A b B c C export LC_COLLATE=C ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:1:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#排序区域设置"},{"categories":["技术"],"content":" 目录栈相关pushd 、popd、dirs 三个命令将维护一个目录栈，我们可以通过调整栈的顺序实现快速的切换工作目录。 命令 动作 dirs 显示名称：home 显示为 ~ dirs -l 显示完整名称 dirs -v 显示名称：每行一个，并且有数字标识 pushd directory 改变工作目录：将 directory 压入到栈中 pushd +n 改变工作目录：将 #n 移动到栈顶 popd 改变工作目录：弹出栈顶 popd +n 从栈中移除目录 #n dirs -c 除当前工作目录外，移除栈中的全部目录 shell # 定义别名快速操作 alias d='dirs -v' alias p=pushd ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:2:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#目录栈相关"},{"categories":["技术"],"content":" 检查文件类型和颜色 shell alias ls='ls -F --color=auto' alias la='ls -a' alias ldot='ls -d .??*' ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:3:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#检查文件类型和颜色"},{"categories":["技术"],"content":" 防止误删文件 shell # 先 ls 检查删除的文件，再通过fc命令替换为rm alias del='fc -s ls=rm' ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:4:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#防止误删文件"},{"categories":["技术"],"content":" 利用历史记录 shell alias a=alias alias info='date; who' alias h=\"fc -l\" alias r=\"fc -s\" # 使用 # h 显示记录 # r #number # r #name old=new ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:5:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#利用历史记录"},{"categories":["技术"],"content":" 掌握磁盘空间情况 shell # -s(size，大小 单位 KB) ls -s # -h(human-readable, 适合人类阅读) ls -sh # -a(all,全部) # -c(count,统计) 末位显示总量 # -s(sum, 总和) # du(disk usage,磁盘使用) du [-achs] [name...] # df(disk free-space, 磁盘可用空间) df [-h] ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:6:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#掌握磁盘空间情况"},{"categories":["技术"],"content":" 块和分配单元：dumpe2fs文件系统中，空间以固定大小的组块进行分配，即 块（block），为文件所分配的最小磁盘空间数量。所以在一个块大小为1KB（1024字节）的文件系统中，仅仅包含1字节的文件也要占用一个完整的块，1025字节的数据文件需要两个块。 出于效率的考虑，当文件写入磁盘或其他存储介质上的时候，磁盘存储空间也是以固定大小的组块分配，即 分配单元（allocationuniut）或 簇（cluster）。分配单元的大小取决于文件系统和存储设备。 例如，我的 linux 系统上，块大小为 1 KB，但是磁盘分配单元 8KB。因此，一个只有一个字节的文件实际上要占用 8KB 的磁盘空间。 查看自己系统块大小和分配单元： shell # 创建一个很小的文件 echo a \u003e temp # 查看文件包含的数据的数量 文件大小在日期前面, 单位字节 2 个字节 ls -l temp # 查看文件占用的存储空间 4K 所以分配单元是 4K du -h temp # 查看块大小 单位：字节 sudo dumpe2fs /dev/vda1 | grep \"Block size\" dumpe2fs 命令是用于显示 ext2、ext3 和 ext4 文件系统的详细信息的命令，对于 xfs 文件系统，应该使用 xfs_info 命令来查看其详细信息，包括块大小等。可以执行以下命令来查看 /dev/sda1 分区上的 xfs 文件系统的块大小。 shell # 查看所有块设备 lsblk #查经分区或硬盘文件类型 blkid /dev/sda1 # /dev/sda1 分区的文件系统信息的输出结果 sudo xfs_info /dev/sda1 # 结果如下： bsize 每个块的字节数 ；isize=512: 每个inode所占用的字节数 等 meta-data=/dev/sda1 isize=512 agcount=4, agsize=65536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:6:1","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#块和分配单元dumpe2fs"},{"categories":["技术"],"content":" 通配符 符号 含义 * 任何0个或多个字符 ？ 任何单个字符 [list] list 中的任何字符 [^list] 不在 list 中的任何字符 {string1|string2} 其中一个指定字符串 ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:7:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#通配符"},{"categories":["技术"],"content":" 预定义的字符 类 含义 类似于 [[:lower:]] 小写字母 [a-z] [[:upper:]] 大写字母 [A-Z] [[:digit:]] 数字 [0-9] [[:alnum:]] 大写小写字母与数字 [A-Za-z0-9] [[:alpha:]] 大写和小写字母 [A-Za-z] ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:7:1","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#预定义的字符"},{"categories":["技术"],"content":" Unix 为新文件指定权限的方式：umaskUnix 创建文件时，将根据文件类型为文件指定以下几种模式： 666: 不可执行的普通文件 777：可执行的普通文件 777： 目录 在这以初始模式上，Unix再减去用户掩码（user mask）。设置掩码的命令为：umask [mode] shell # 限制其他人写的权限 umask 022 # 显示当前的掩码 umask ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:8:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#unix-为新文件指定权限的方式umask"},{"categories":["技术"],"content":" 清空文件内容当删除文件时，文件所使用的实际磁盘空间还没被清除。文件系统只是将这部分磁盘空间标识为可重用。最终旧数据会被新数据覆盖。但是时间不确定，且有一些特殊的恢复工具可以恢复数据。 shred 程序的目的就是多次覆盖硬盘上已有的数据。 shell # -f(force,强制) # -u 覆盖完删除文件 # -v(verbose, 详细) # -z 填充零，默认是随机数据 shred -fuvz datafile ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:8:1","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#清空文件内容"},{"categories":["技术"],"content":" 链接的概念","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:9:0","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#链接的概念"},{"categories":["技术"],"content":" stat、ls -i i 节点（i-node）当 unix 创建文件时，unix 做了两件事。 Unix 在存储设备上保留一块空间用来存储数据。 Unix 创建一个**索引节点（index node）**或 **i 节点（i-node）**的结构。用来存放文件的基本信息。 查看文件的 i 节点内容： stat filename 文件系统将所有的 i 节点存放在一个大表中，称为 i 节点表（inode table），每个 i 节点由所谓的索引号或 i 节点号表示。 查看文件的节点号：ls -i filename 处理目录时候，就好像目录包含文件一样，其实，目录并不包含文件，目录只包含文件的名称及文件的 i 节点号。因此目录的内容相当小：只有一列名称，每个名称对应一个 i 节点号。 文件名和 i 节点之间的连接称为 链接。 ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:9:1","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#statls--i-i-节点i-node"},{"categories":["技术"],"content":" 多重链接Unix 文件系统允许多重链接。一个文件可以有不止一个名称。文件的唯一标识是 i 节点号，不是名称，所有多个名称可以引用到同一个 i 节点号。 链接的基本思想是同一个文件可能拥有不同的含义（取决于文件使用的环境）。并且 Unix 平等的对待所有的链接。 ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:9:2","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#多重链接"},{"categories":["技术"],"content":" 创建新链接: ln shell # file 是已有的普通文件的名称，newname是希望赋予链接的名称 ln file newname # 为一个或多个普通文件创建链接，并放到指定的目录中 ln file... directory rm 和 rmdir 其实是移除链接，只移除了文件名和 i 节点号之间的连接，如果文件没有链接了，Unix 会删除该文件。 ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:9:3","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#创建新链接-ln"},{"categories":["技术"],"content":" 符号链接：ln -s以上链接有两个限制：1. 不能为目录创建链接，2. 不能为不同的文件系统创建链接。 如果要实现以上需求，需要使用 符号链接（symbol link 或 symlink） 。 符号链接包含的不是文件的 i 节点号，而是文件的路径名。当访问符号链接时候，Unix 借助路径名查找文件。类似于 Windows 的快捷方式。 shell ls -l /bin/sh # 输出 /bin/sh 是一个指向 /bin/bash文件的符号链接 lrwxrwxrwx 1 root root 4 Aug 7 2020 /bin/sh -\u003e bash 为了区分两种链接： 常规的链接 : 硬链接（hard link） ls -l 可以查看硬链接数量，rm 可以移除硬链接 符号链接 : 软链接（soft link） 如果一个文件存在符号链接，删除文件，符号链接不会被删除，使用时则会报错。 当一个目录使用符号链接时，cd 和 pwd 既可以将符号链接视为一个实体，也可以将链接作为真实目录的一个跳板。 -L （logical， 逻辑）、-P（physical，物理）选项指定，默认 -L ","date":"2022-05-05","objectID":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/:9:4","series":null,"tags":["折腾","unix","linux"],"title":"Unix文件,别名等相关","uri":"/posts/unixs/unix%E6%96%87%E4%BB%B6%E5%88%AB%E5%90%8D%E7%9B%B8%E5%85%B3/#符号链接ln--s"},{"categories":["诗歌"],"content":"Tiger got to hunt, bird got to fly; Man got to sit and wonder ‘why, why, why?’ Tiger got to sleep, bird got to land; Man got to tell himself he understand. — Kurt Vonnegut 翻译： 虎猎鸟飞人疑惑：为啥为啥为啥捏？ 虎息鸟憩人自语：好呗算我懂了呗。 ","date":"2021-12-24","objectID":"/posts/mess/tiger-got-to-hunt/:0:0","series":null,"tags":["诗歌"],"title":"Tiger Got to Hunt","uri":"/posts/mess/tiger-got-to-hunt/#"},{"categories":["技术"],"content":" 目标wsl 可以使用 win10 的代理。 ","date":"2021-12-11","objectID":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/:1:0","series":null,"tags":["代理","折腾","wsl"],"title":"Wsl设置代理","uri":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/#目标"},{"categories":["技术"],"content":" 环境说明win10 + 小飞机 + wsl2 ","date":"2021-12-11","objectID":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/:2:0","series":null,"tags":["代理","折腾","wsl"],"title":"Wsl设置代理","uri":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/#环境说明"},{"categories":["技术"],"content":" 步骤 WSL 中获取宿主机 IP WSL 每次启动的时候都会有不同的 IP 地址，所以并不能直接用静态的方式来设置代理。WSL2 会把 IP 写在 /etc/resolv.conf 中，因此可以用 cat /etc/resolv.conf | grep nameserver | awk '{ print $2 }' 这条指令获得宿主机 IP 。 WSL2 自己的 IP 可以用 hostname -I | awk '{print $1}' 得到。 设置代理 有了宿主机 IP 之后，就可以通过设置环境变量的方式设置代理了。这里端口需要自己填写，而且别忘了代理软件中设置允许来自局域网的连接。 shell export http_proxy='http://\u003cWindows IP\u003e:\u003cPort\u003e' export https_proxy='http://\u003cWindows IP\u003e:\u003cPort\u003e' 这种设置方式每次重启终端都得重新设置一遍，而且 IP 还得自己手打，还是挺麻烦的，这种时候就得靠脚本了！ 脚本实现 shell #!/bin/sh hostip=$(cat /etc/resolv.conf | grep nameserver | awk '{ print $2 }') wslip=$(hostname -I | awk '{print $1}') port=\u003cPORT\u003e PROXY_HTTP=\"http://${hostip}:${port}\" set_proxy(){ export http_proxy=\"${PROXY_HTTP}\" export HTTP_PROXY=\"${PROXY_HTTP}\" export https_proxy=\"${PROXY_HTTP}\" export HTTPS_proxy=\"${PROXY_HTTP}\" } unset_proxy(){ unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY } test_setting(){ echo \"Host ip:\" ${hostip} echo \"WSL ip:\" ${wslip} echo \"Current proxy:\" $https_proxy } if [ \"$1\" = \"set\" ] then set_proxy elif [ \"$1\" = \"unset\" ] then unset_proxy elif [ \"$1\" = \"test\" ] then test_setting else echo \"Unsupported arguments.\" fi 第 4 行 记得换成自己宿主机代理的端口！！！！！！ git 实现代理 如果希望 git 也能通过代理，可以分别在 set_proxy 和 unset_proxy 函数中加上如下命令 shell // 添加代理 git config --global http.proxy \"${PROXY_HTTP}\" git config --global https.proxy \"${PROXY_HTTP}\" // 移除代理 git config --global --unset http.proxy git config --global --unset https.proxy 分别加到上边脚本对应位置即可。 之后运行 . ./proxy.sh set 就可以自动设置代理了。unset 可以取消代理，test 可以查看代理状态，能够用来检查环境变量是否被正确修改。 运行的时候不要忘记之前的 .，或者使用 source ./proxy.sh set，只有这样才能够修改环境变量。 直接运行 ./proxy.sh set 或者 sh proxy.sh set，这样会是运行在一个子 shell 中，对当前 shell 没有效果。 自动执行 另外可以在 ~/.bashrc 中选择性的加上下面两句话，记得将里面的路径修改成你放这个脚本的路径。 shell alias proxy=\"source /path/to/proxy.sh\" . /path/to/proxy.sh set 第一句话可以为这个脚本设置别名 proxy，这样在任何路径下都可以通过 proxy 命令使用这个脚本了，之后在任何路径下，都可以随时都可以通过输入 proxy unset 来暂时取消代理。 第二句话就是在每次 shell 启动的时候运行该脚本实现自动设置代理，这样以后不用额外操作就默认设置好代理啦~ 防火墙设置 如果前面完成后已经可以正常使用了，那么下面就不用管了。如果你代理已经设置正确了，尤其是已经允许来自局域网的访问，但是依旧无法正常访问，代理的软件的确也没收到请求，那么很可能是被 Windows 的防火墙给拦截了。 可以先尝试 ping 宿主机 ip 和 telnet 代理的端口，检查是否连通。如果无法连通，则多半是防火墙的问题。 可以尝试在控制面板的防火墙面板左侧“允许应用或功能通过防火墙”，即上述界面中，打上勾允许代理软件通过防火墙。 或者可以尝试在高级设置中，入站规则中新建一个相关规则，如果你不是很了解，可以允许任何程序的任何协议，远程 IP 为 172.16.0.0/12 及 192.168.0.0/16 的入站请求。 使用 PowerShell 命令行创建一条给 WSL 网卡的防火墙策略： powershell New-NetFirewallRule -DisplayName \"WSL\" -Direction Inbound -InterfaceAlias \"vEthernet (WSL)\" -Action Allow ","date":"2021-12-11","objectID":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/:3:0","series":null,"tags":["代理","折腾","wsl"],"title":"Wsl设置代理","uri":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/#步骤"},{"categories":["技术"],"content":" 参考文章主要部分 防火墙部分 防火墙手动设置 ","date":"2021-12-11","objectID":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/:4:0","series":null,"tags":["代理","折腾","wsl"],"title":"Wsl设置代理","uri":"/posts/wsl%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/#参考文章"},{"categories":["生活","技术"],"content":" 实验归纳 模型推演 仿真模拟 数据密集型科学发现 拓展阅读 ","date":"2021-11-24","objectID":"/posts/mess/%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%9A%844%E7%A7%8D%E8%8C%83%E5%BC%8F/:0:0","series":null,"tags":["日常","人间清醒"],"title":"科学研究的4种范式","uri":"/posts/mess/%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%9A%844%E7%A7%8D%E8%8C%83%E5%BC%8F/#"},{"categories":["生活"],"content":" 多巴胺模式： 吃、逛、买、看 恐惧驱动模式： 没有办法放手做事，瞻前顾后。 创造力模式： 关注点不在内在的状态，也不是去寻找快乐，而是想到要做成某事，关注点集中于当下正在做的事情。透过做一件事，将自己的注意力，从内心的小毛病小纠结转移到事情本身。 ","date":"2021-11-19","objectID":"/posts/mess/%E4%BA%BA%E7%94%9F3%E5%A4%A7%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F/:0:0","series":null,"tags":["日常","人间清醒","得意忘形"],"title":"人生3大行为模式","uri":"/posts/mess/%E4%BA%BA%E7%94%9F3%E5%A4%A7%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F/#"},{"categories":["生活"],"content":" 以各种各样的方式表现与别人不一样，来营造一种优越感（营造优越感的诱惑） 面对陌生事物的本能的恐惧和排挤 由于自身匮乏感，本能性的嫉妒 ","date":"2021-11-19","objectID":"/posts/mess/%E4%BA%BA%E7%94%9F3%E5%A4%A7%E5%8E%9F%E7%BD%AA/:0:0","series":null,"tags":["日常","人间清醒","得意忘形"],"title":"人生3大原罪","uri":"/posts/mess/%E4%BA%BA%E7%94%9F3%E5%A4%A7%E5%8E%9F%E7%BD%AA/#"},{"categories":["技术"],"content":" 开启与关闭 nginx关闭 nginx nginx -s stop 重新加载 nginx -s relaod ","date":"2021-11-19","objectID":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/:0:1","series":null,"tags":["nginx"],"title":"nginx相关","uri":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/#开启与关闭-nginx"},{"categories":["技术"],"content":" 配置文件 全局配置 影响 nginx 服务器整体运行的配置指令 eg：worker_processes 1; // 越大 并发越大 event 块 nginx 服务器与用户的网络连接 eg: worker_connections 1024; // 支持的最大连接数 http 块 http 全局块 server 块 ","date":"2021-11-19","objectID":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/:0:2","series":null,"tags":["nginx"],"title":"nginx相关","uri":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/#配置文件"},{"categories":["技术"],"content":" 配置实例 反向代理 访问 http://yii2.test:80 实际访问的是 http://vdong.test:8080 conf server { listen 80; server_name yii2.test *.yii2.test; root \"D:/laragon/www/yii2/web/\"; location / { proxy_pass http://vdong.test:8080; # ... } } 根据目录 跳转 不同 服务器 conf location ~ /yii/ { proxy_pass http://yii2-base.test; } location ~ /laravel/ { proxy_pass http://laravel.test:8080; } 负载均衡 conf http{ .... upstream php_upstream { # ip_hash; # fair; server 127.0.0.1:9001 weight=1 max_fails=1 fail_timeout=1; server 127.0.0.1:9002 weight=1 max_fails=1 fail_timeout=1; } .... } server{ .... location / { proxy_pass http://php_upstream; try_files $uri $uri/ /index.php$is_args$args; autoindex on; } .... } 负载策略： 轮询（默认） 时间顺序 逐一分配，down 了 剔除 weight 权重 根据权重 分配 ip_hash 解决 session 共享问题 fair ( 第三方 ) 根据响应时间分配，越短越多。 动静分离 动态请求和静态请求分离 location 根据后缀名 实现转发 ， expires 缓存时间 3d 3 天 conf location /www/{ root /data/; index index.html index.htm; } location /image/ { root /data/; autoindex on; } 高可用集群 两台 nginx； keepalived；虚拟 ip keepalived 其实 对于两台 主备服务器的， 路由作用 shell yum install keepalived -y 配置在 etc/keepalived/keepalived.conf 。 修改配置 在 /usr/local/src 添加检测脚本 ","date":"2021-11-19","objectID":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/:0:3","series":null,"tags":["nginx"],"title":"nginx相关","uri":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/#配置实例"},{"categories":["技术"],"content":" nginx 原理一个 master 多个 worker master 管理 监控， worker 争抢 client 热部署 独立进程，一个出问题，不影响其他的 io 多路复用机制， 每个 worker 都是 独立的进程，每个进程只有一个线程，通过异步非阻塞的方式来处理请求。 worker 和 cpu 核心数量相等，cpu 性能充分发挥。 worker_connection 连接数 发送一个请求， 占用了 2 / 4 个连接数。静态 不经过 php-fpm 的 。 nginx 一个 master， 4 个 worker， 每个 worker 支持最大连接数 1024，目前最大并发数是多少？ 4*1024/(2/4) 静态访问 ： worker_connection * worker_processes / 2 动态访问： worker_connection * worker_processes / 4 ","date":"2021-11-19","objectID":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/:0:4","series":null,"tags":["nginx"],"title":"nginx相关","uri":"/posts/php-web/nginx%E7%9B%B8%E5%85%B3/#nginx-原理"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag \u003ctagname\u003e用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a \u003ctagname\u003e -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin \u003ctagname\u003e可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d \u003ctagname\u003e可以删除一个本地标签； 命令git push origin :refs/tags/\u003ctagname\u003e可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#常用命令"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#clone-特定分支"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#日志查看"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#添加远程库"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#第一次推送"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#比较差异"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#配置别名"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#标签"},{"categories":["技术"],"content":" 常用命令 clone 特定分支 shell git clone -b v5.12 git@gitee.com:xxx/xxx.git 日志查看 shell git log --graph --pretty=oneline --abbrev-commit 添加远程库 shell git remote add origin git@github.com:michaelliao/learngit.git 第一次推送 shell git push -u origin master 比较差异 shell git diff readme.txt 配置别名 shell git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u003c%an\u003e%Creset' --abbrev-commit\" git config --global alias.lg \"log --graph --pretty=oneline --abbrev-commit\" git config --global alias.st \"status\" git config --global alias.ck \"checkout\" git config --global alias.br \"branch\" 标签 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a -m \"blablabla...\"可以指定标签信息； 命令git tag可以查看所有标签。 命令git push origin 可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d 可以删除一个本地标签； 命令git push origin :refs/tags/可以删除一个远程标签。 获取今天的日志 shell git log --since=\"6am\" ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:1","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#获取今天的日志"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#错误记录"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#pull-或-merge-时错误"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#git如何忽略已经提交的文件-gitignore文件无效"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#文件名大小写问题"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#换行符问题"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#修改最后一次commit的message"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#文件名中文-xxx-八进制中文乱码"},{"categories":["技术"],"content":" 错误记录 pull 或 merge 时错误fatal: refusing to merge unrelated histories 解决方案： 在你操作命令后面加–allow-unrelated-histories eg: git merge master –allow-unrelated-histories git pull origin master –allow-unrelated-histories git如何忽略已经提交的文件 (.gitignore文件无效)解决方案： git rm -r --cached 要忽略的文件 (如: git rm -r --cached build/*, 如修改列表中的内容全部是不需要的, 那么你可以使用最最简单的命令搞定git rm -r --cached .) git add . git commit -m \" commet for commit .....\" git push 文件名大小写问题默认对大小写不敏感 文件名相同 造成 Changes not staged for commit 错误 解决方案： shell git config --global core.ignorecase false 换行符问题win上会提示CRLF will be replaced by LF 解决方案： https://stackmirror.com/questions/5834014 shell git config --global core.autocrlf input 修改最后一次commit的message shell git commit --amend 文件名中文 \\xxx 八进制中文乱码 shell git config core.quotepath false --global 删除远程分支, 后还显示 shell git push origin -d git remote prune origin ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:2","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#删除远程分支-后还显示"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD \u003cfileName\u003e 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log \u003cfilename\u003e git checkout \u003ccommitID\u003e \u003cfileName\u003e 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#撤销"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景1-本地修改了一些文件未-add-但是想放弃修改"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景11-本地新增了一些文件未add想放弃新增"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景2执行了-add--但是想放弃修改"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景3commit-了想修改且不想产生新的提交"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景4commit-了想撤销到某次-commit"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景5-已经-push-了撤销制定文件到指定版本"},{"categories":["技术"],"content":" 撤销 场景1 ：本地修改了一些文件，未 add ，但是想放弃修改 shell git checkout fileName // or git checkout . 场景1.1： 本地新增了一些文件，未add，想放弃新增 shell # 删除文件 rm filename # 删除新增的文件，如果文件已经已经 git add 到暂存区，并不会删除！ git clean -xdf # 同上，但是还会处理文件夹 git clean -xdff 场景2：执行了 add , 但是想放弃修改 shell git reset HEAD 场景3：commit 了，想修改，且不想产生新的提交 shell git add fileName git commit -amend -m '说明' 场景4：commit 了，想撤销到某次 commit shell git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 场景5： 已经 push 了，撤销制定文件到指定版本 shell # 查看文件版本 git log git checkout 场景6： 撤销最后一次提交 shell git revert HEAD // 是反做了目标版本，产生一个新的commits git reset --hard HEAD^ // 会删除目标版本后的版本 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:3","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#场景6-撤销最后一次提交"},{"categories":["技术"],"content":" 学习网站learngitbranching 图解git githowto git飞行规则 ","date":"2021-11-04","objectID":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:4","series":null,"tags":["git"],"title":"git 常用命令","uri":"/posts/git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#学习网站"},{"categories":["技术"],"content":" 版本管理有一个仓库（可以自建，也可以用线上的），将开发者本地的代码每次修改管理起来，可以查看修改记录，回滚等，常用的管理工具有svn 、git等，主流 git。 简要流程： 修改代码 提交修改到仓库 当然反过来也可以： 从仓库下载代码到本地 svn： 叫检出（ svn checkout path）； git 叫 拉取（git pull） ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:1","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#版本管理"},{"categories":["技术"],"content":" 服务器同步本地修改的代码要上传服务器，才能生效 简要流程： 修改代码 上传服务器（ftp，sftp： 直接本地上传；或者版本仓库里 拉取） ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:2","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#服务器同步"},{"categories":["技术"],"content":" 自动部署可以发现第一步是一样的，我们可以把服务器当作一个新的本地环境，我们可以使用计算机，帮我们简化操作流程。 简化后的流程： 修改代码 提交修改到代码仓库 从仓库拉取代码到服务器，即完成了上传服务器 以git为例，我们看一些实际部署（手动）： 本地计算机执行： git commit -am 修改了一些文件 本地计算机执行： git push origin/master 提交远程版本仓库 服务器执行： git pull 服务器拉取版本仓库文件 webhook 帮助我们自动实现这个拉取过程。 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:3","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#自动部署"},{"categories":["技术"],"content":" webhook自动部署的原理 webhooks：其实就是类似于触发事件 A事件发生 —-触发—-\u003e B事件 push事件 —-触发—-\u003e post : www.a.com/pull.php pull.php 完成工作： 切到网站根目录，git pull 拉取仓库代码 核心代码： shell_exec(“cd /home/www/www.a.com \u0026\u0026 git pull 2\u003e\u00261”); ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:4","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#webhook自动部署的原理"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#具体实现"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#生成并部署ssh-key"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#服务器初始化clone仓库"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#准备php代码"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#绑定webhooks"},{"categories":["技术"],"content":" 具体实现 生成并部署SSH key 服务器上生成ssh key shell ssh-keygen -t rsa -C \"xxxxx@xxxxx.com\" # Generating public/private rsa key pair... # 三次回车即可生成 ssh key 查看公钥并添加的码云 shell cat ~/.ssh/id_rsa.pub # ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6eNtGpNGwstc.... 复制以上公钥，添加到仓库 “部署公钥管理”。 测试出现如下表示添加成功 shell ssh -T git@gitee.com Hi Anonymous! You've successfully authenticated, but GITEE.COM does not provide shell access. Note: Perhaps the current use is DeployKey. Note: DeployKey only supports pull/fetch operations 服务器初始化clone仓库 shell $ cd /home/wwwroot/ # 代码存放目录 $ git clone git@gitee.com:vdong/t.whaoot.com.git 准备php代码 php \u003c?php // 本地仓库路径 $local = '/home/wwwroot/t.whaoot.com'; // 安全验证字符串，为空则不验证 $token = '111111'; // 如果启用验证，并且验证失败，返回错误 $httpToken = isset($_SERVER['HTTP_X_GITEE_TOKEN']) ? $_SERVER['HTTP_X_GITEE_TOKEN'] : ''; if ($token \u0026\u0026 $httpToken != $token) { header('HTTP/1.1 403 Permission Denied'); die('Permission denied.'); } // 如果仓库目录不存在，返回错误 if (!is_dir($local)) { header('HTTP/1.1 500 Internal Server Error'); die('Local directory is missing'); } //如果请求体内容为空，返回错误 $payload = file_get_contents('php://input'); if (!$payload) { header('HTTP/1.1 400 Bad Request'); die('HTTP HEADER or POST is missing.'); } /* * 这里有几点需要注意： * * 1.确保PHP正常执行系统命令。写一个PHP文件，内容： * `\u003c?php shell_exec('ls -la')` * 在通过浏览器访问这个文件，能够输出目录结构说明PHP可以运行系统命令。 * 否则 修改web服务器上php.ini的 disable_functions 列表，去掉 shell_exec; 重启php-fpm服务 * * 2、PHP一般使用www-data或者nginx用户运行，PHP通过脚本执行系统命令也是用这个用户， * 所以必须确保在该用户家目录（一般是/home/www-data或/home/nginx）下有.ssh目录和 * 一些授权文件，以及git配置文件，如下： * ``` * /root/.ssh * + .ssh * - authorized_keys * - config * - id_rsa * - id_rsa.pub * - known_hosts * - .gitconfig * ``` * * 3.在执行的命令后面加上2\u003e\u00261可以输出详细信息，确定错误位置 * * 4.git目录权限问题。比如： * `fatal: Unable to create '/data/www/html/awaimai/.git/index.lock': Permission denied` * 那就是PHP用户没有写权限，需要给目录授予权限: * `` * sudo chown -R www:www /data/www/html/awaimai` * sudo chmod -R g+w /data/www/html/awaimai * ``` * * 5.SSH认证问题。如果是通过SSH认证，有可能提示错误： * `Could not create directory '/.ssh'.` * 或者 * `Host key verification failed.` * .ssh 要放到对应的用户目录下 * 例如： php 是以 www 用户运行的， /home/www/.ssh * */ #以可写权限打开git_new.log文件，用于记录git日志 $fs = fopen('/home/www/git.t.whaoot.com.log', 'a'); fwrite($fs, '================ Update Start ==============='.PHP_EOL.PHP_EOL); #获取请求端的IP $client_ip = $_SERVER['REMOTE_ADDR']; #将时间与请求端IP写入日志文件 fwrite($fs, 'Request on ['.date(\"Y-m-d H:i:s\").'] from ['.$client_ip.']'.PHP_EOL); #执行shell命令cd到网站根目录执行git pull操作并把返回信息赋值给变量output $output=shell_exec(\"cd {$local} \u0026\u0026 git pull 2\u003e\u00261\"); #将日志信息写入日志文件中 fwrite($fs, 'Info:'. $output.PHP_EOL); fwrite($fs,PHP_EOL. '================ Update End ==============='.PHP_EOL.PHP_EOL); #关闭日志文件 echo $output die(\"done \" . date('Y-m-d H:i:s', time())); $fs and fclose($fs); 绑定webhooks去码云设置 WebHooks，点击测试 参考链接：https://www.awaimai.com/2203.html https://blog.csdn.net/gbenson/article/details/84346696 ","date":"2021-11-04","objectID":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/:0:5","series":null,"tags":["php","webhook"],"title":"PHP webhook 部署码云","uri":"/posts/php-web/php-webhooks-%E9%83%A8%E7%BD%B2%E7%A0%81%E4%BA%91/#参考链接"},{"categories":["技术"],"content":"composer require “mk-j/php_xlsxwriter” –no-update ","date":"2021-11-04","objectID":"/posts/php-web/composer/:0:0","series":null,"tags":["composer"],"title":"composer","uri":"/posts/php-web/composer/#"},{"categories":["技术"],"content":" 版本说明“require”: { “vendor/package”: “1.3.2”, // exactly 1.3.2 // \u003e, \u003c, \u003e=, \u003c= | specify upper / lower bounds \"vendor/package\": \"\u003e=1.3.2\", // anything above or equal to 1.3.2 \"vendor/package\": \"\u003c1.3.2\", // anything below 1.3.2 // * | wildcard \"vendor/package\": \"1.3.*\", // \u003e=1.3.0 \u003c1.4.0 // ~ | allows last digit specified to go up \"vendor/package\": \"~1.3.2\", // \u003e=1.3.2 \u003c1.4.0 \"vendor/package\": \"~1.3\", // \u003e=1.3.0 \u003c2.0.0 // ^ | doesn't allow breaking changes (major version fixed - following semver) \"vendor/package\": \"^1.3.2\", // \u003e=1.3.2 \u003c2.0.0 \"vendor/package\": \"^0.3.2\", // \u003e=0.3.2 \u003c0.4.0 // except if major version is 0 } ","date":"2021-11-04","objectID":"/posts/php-web/composer/:1:0","series":null,"tags":["composer"],"title":"composer","uri":"/posts/php-web/composer/#版本说明"},{"categories":["技术"],"content":"Linux 直接发送UDP包 如果往本地UDP端口發送數據，那麼可以使用以下命令： bash echo \"hello\" \u003e /dev/udp/192.168.1.81/5060 意思是往本地192.168.1.81的5060端口發送數據包hello。 如果往遠程UDP端口發送數據，那麼可以使用以下命令： bash echo \"hello\" | socat - udp4-datagram:192.168.1.80:5060 意思是往遠程192.168.1.80的5060端口發送數據包 hello 。 远程可以监听对应端口查看： bash # -l listen -p source-port -4 ipv4 -u udp nc -l -p 5060 -4 -u ps: 先安装 socat ，centos: yum install -y socat​ ","date":"2021-11-04","objectID":"/posts/unixs/linux%E5%8F%91udp%E5%8C%85/:0:0","series":null,"tags":["unix","linux"],"title":"linux发udp包","uri":"/posts/unixs/linux%E5%8F%91udp%E5%8C%85/#"},{"categories":["技术"],"content":" 三大范式 非标准化形式： 主键不能重复；不出现重复记录 字段原子性，不能再分 反例： 联系方式 =》（电话，邮箱） 不能存在部分依赖。 随着主键值的变化，其它列名也必须随之而变化，如果主键是几个列名的组合，其它列名（副键）必须同时被这几个列名影响，少一个的话，不行。 反例： 组件id，价格，供应商id，供应商名称，供应商地址。 同一个组件有可能由不同的供应商提供，故 组件 id 和 供应商 id 组成主键， 价格 和 主键 完全依赖； 供应商名称和地址 只依赖于 供应商 id， 部分依赖 不能存在传递依赖。 副键与副键之间，不能存在依赖关系。 反例： 学号，姓名， 性别，班级，班主任。班主任 受到 班级 的影响，就需要另外 建一张表。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:1:0","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#三大范式"},{"categories":["技术"],"content":" 事务数据库事务（简称:事务）是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。事务的使用是数据库管理系统区别文件系统的重要特征之一。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:2:0","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#事务"},{"categories":["技术"],"content":" 4个特性 原子性 要么做完，要么不做，没有中间停滞环节。 一致性 从一种状态变成另一种状态。开始和结束后，完整性约束没有变化。 隔离性 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务的运行效果。即，每个事务有各自完整的数据空间。 mysql 通过锁机制来保证事务的隔离性。 持久性 事务一旦提交，其结果就是永久性的。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:2:1","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#4个特性"},{"categories":["技术"],"content":" 事务的隔离等级 READ UNCOMMITTED（读未提交）脏读 事务2 读到了 事务1 未提交的数据。 READ COMMITTED （读提交） 不可重复读 事务2 只能读到 事务1 已经提交的数据。是 Oracle 和 SQL Server 的默认隔离级别。 REPEATABLE READ （可重复读） 幻读 事务2 不会读到 事务1 对已有数据的修改，即使事务1的修改 已提交，也就是说，事务2 开始时读到的数据是什么，在事务2 提交前的任意时刻，这些数据的值都是一样的。 mysql 的默认隔离级别。 如果 根据读取数据插入或修改时，与数据库真实数据发生冲突（完整性和一致性），mysql 称为幻读。（如，主键冲突等）。 SERIALIZABLE（序列化） 事务是串行顺序执行的，MySql 的 InnoDB 会给读操作隐式的加一把读共享锁，从而避免了脏读、不可重复读、幻读的问题。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:2:2","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#事务的隔离等级"},{"categories":["技术"],"content":" 锁锁也是数据库管理系统区别文件系统的重要特征之一。锁机制使得在对数据库进行并发访问时，可以保障数据的完整性和一致性。 Mysql 的锁其实可以按很多种形式分类: 按加锁机制分,可分为乐观锁与悲观锁. 按兼容性来分,可分为X锁与S锁. 按锁粒度分,可分为表锁,行锁,页锁. 按锁模式分,可分为记录锁, gap锁, next-key锁, 意向锁, 插入意向锁. InnoDB 实现了两种类型的行级锁： 共享锁 （S 锁）： 允许事务读取一行数据。 select * from TableName where … lock in share mode; 独占锁 （X 锁）： 允许事务删除或更新一行数据。 select * from TableName where … for update; S 锁与 S锁 是兼容的，X 锁和其它锁都不兼容。 为了实现多粒度的锁机制，InnoDB 还有两种内部使用的 意向锁 ，由 InnoDB 自动添加，且都是表级别的锁。 意向共享锁 （IS）：事务即将给表中的各个行设置共享锁，事务给数据行加 S 锁前必须获得该表的 IS 锁。 意向排他锁 （IX）：事务即将给表中的各个行设置排他锁，事务给数据行加 X 锁前必须获得该表 IX 锁。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:3:0","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#锁"},{"categories":["技术"],"content":" 死锁死锁 是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:3:1","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#死锁"},{"categories":["技术"],"content":" 锁优化建议 合理设计索引，让 InnoDB 在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他 Query 的执行。 尽可能减少基于范围的数据检索过滤条件，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录。 尽量控制事务的大小，减少锁定的资源量和锁定时间长度。 在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少 MySQL 因为实现事务隔离级别所带来的附加成本。 ","date":"2021-11-04","objectID":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/:3:2","series":null,"tags":["mysql"],"title":"Mysql 范式 事务 锁","uri":"/posts/php-web/mysql-%E8%8C%83%E5%BC%8F-%E4%BA%8B%E5%8A%A1-%E9%94%81/#锁优化建议"},{"categories":["技术"],"content":" 一个请求的生命周期","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:0","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#一个请求的生命周期"},{"categories":["技术"],"content":" 0. 启动服务 准备工作启动 php-fpm，通信模式， TCP socket， Unix scoket。 PHP-FPM 启动两种进程， master：监控端口、分配任务，管理 worker 进程， worker： 就是 php 的cgi 程序，解释和编译执行 php 脚本。 启动 nginx。载入 ngx_http_fastcgi_module 模块，初始化 FastCGI 执行环境，实现 FastCGI 协议 请求代理。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:1","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#0-启动服务-准备工作"},{"categories":["技术"],"content":" 1. request =\u003e nginxnginx 接受请求，基于 location 配置，选择 handle，即 代理 PHP 的 handle。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:2","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#1-request--nginx"},{"categories":["技术"],"content":" 2. nginx =\u003e PHP-FPMnginx 将请求翻译成 fastcgi 请求。 通过 tcp scoket /unix scoket 发送给 PHP-FPM 的 master 进程 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:3","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#2-nginx--php-fpm"},{"categories":["技术"],"content":" 3. PHP-FMP 的 master =\u003e workermaster 分配 worker 执行 php 脚本，没空闲 worker 返回 502。 worker （php-cgi）进程执行 php 超时 返回 504 处理解释返回结果 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:4","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#3-php-fmp-的-master--worker"},{"categories":["技术"],"content":" 4. worker =\u003e master =\u003e nginxworker 返回处理结果给 master，关闭连接，等待下一个请求 master 进程返回结果给 nginx nginx 根据 handle 顺序，将响应一步一步返回给客户端 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:1:5","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#4-worker--master--nginx"},{"categories":["技术"],"content":" php解释执行的机制 初始化 ，启动 zend 引擎，加载注册的拓展模块 （php-fpm 启动时） 读取脚本文件，词法分析和语法分析 生成语法树 zend 引擎编译语法树， 生成 opcode 执行 opcode，返回结果 步骤 2-4 每个请求都要执行一次，极大的浪费。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:2:0","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#php解释执行的机制"},{"categories":["技术"],"content":" opcache将编译好的操作码放入共享内存，提供给其他进程访问。 主要缓存 opcode interned string ， 如注释，变量名 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:3:0","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#opcache"},{"categories":["技术"],"content":" php-fpm 三种运行模式 ondemand 内存优先 php-fpm 启动时候不会创建 worker 进程，链接进来时按需创建 static 静态池 启动时 创建固定数量的 worker 进程，也有1 秒的定时器，统计进程状态 dynamic 服务优先 启动时候创建一部分进程，运行过程中动态调整worker 数量 ps： https://segmentfault.com/a/1190000022549643 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/:4:0","series":null,"tags":["php"],"title":"PHP 和 Nginx 通信","uri":"/posts/php-web/php%E5%92%8Cnginx%E9%80%9A%E4%BF%A1/#php-fpm-三种运行模式"},{"categories":["技术"],"content":" php 假值 与 empty 和 is_null 当 $a 是以下 值时候 (boolean) $a 为假 false： 未设置的变量 var $a; 未初始化 $a 的值 $a = null; false 0 '' ‘0’ [] 以上值 empty() 是均为 true 。 isset() 检测 一个变量设置了，且不为 null ，所以仅仅前 3 条 为 false 。 is_null() 正好与 isset() 相反， 仅仅 前 3 条 为 true 。 gettype() 前3条 ，均返回 null，即 类型 未知。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9F%BA%E7%A1%80/:1:0","series":null,"tags":["php"],"title":"PHP 基础","uri":"/posts/php-web/php%E5%9F%BA%E7%A1%80/#php-假值-与-empty-和-is_null"},{"categories":["技术"],"content":" php array_map 和 array_walk 的区别相同点 都是利用回调函数对数组中每个元素进行操作。 array_map(callable $callback, $$arr, …$$arr):array array_walk( array \u0026$arr, callable $$callback [,$$userData]) 描述 对数组的每个元素应用回调函数 使用用户子定义函数对每个元素做回调处理 返回值 返回数组，如果回调函数没有返回值 返回 [] 返回 bool ， 成功 true，否则 false 参数顺序 先回调，再数组，再额外数组 先数组，再回调，再用户数据 回调函数参数 只有数组的 value， 且 个数 与 传入的数组一致，即可以转入多个数组 默认 value, key [可选用户数据], 如果要修改原数组的值以引用的方式 (\u0026) 传第一个值 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9F%BA%E7%A1%80/:2:0","series":null,"tags":["php"],"title":"PHP 基础","uri":"/posts/php-web/php%E5%9F%BA%E7%A1%80/#php-array_map-和-array_walk-的区别"},{"categories":["技术"],"content":" php 的魔术方法__construct()， __destruct()， __call()， __callStatic()， __get()， __set()， __isset()， __unset()， __sleep()， __wakeup()， __serialize(), __unserialize(), // 序列化和反序列化 __toString()， __invoke()， // 以函数方式调用一个对象时 $a= new A; $a(1); class A 中的 __invoke($val) 方法将被调用 __set_state()， __clone() // 深度复制时 调用 __debugInfo() ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9F%BA%E7%A1%80/:3:0","series":null,"tags":["php"],"title":"PHP 基础","uri":"/posts/php-web/php%E5%9F%BA%E7%A1%80/#php-的魔术方法"},{"categories":["技术"],"content":" 变量的存储结构众所周知，php 是用 c 语言写的，所以其变量存储也依赖于 c 语言。php 的变量的内部是使用一种 zval 的数据结构来保存的。 c // php源码 Zend/zend.h 文件中 typedef struct _zval_struct zval; // ... struct _zval_struct { zvalue_value value; /* 变量的值 */ zend_uint refcount__gc; /* 指向该变量容器的变量(也称符号即symbol)个数 */ zend_uchar type; /* 变量的类型 */ zend_uchar is_ref__gc; /* 是否是引用 默认 false */ }; refcount__gc 为 0 表示没有变量指向这个内存容器， 就可以被释放。 php // refcount__gc 举例 $a = 1; // 内存中存在 一个 zval 结构 { value:1; type:integer; refcount__gc:1; is_ref__gc:false } ps: zval 结构的值是 zvalue_value（数据结构） 的，这里说 1, 不准确 $b = $a; // zval 中的 refcount__gc 就加 1， 为 { value:1; type:integer; refcount__gc:2; is_ref__gc:false } unset($b) // zval 中的 refcount__gc 就减 1，为 { value:1; type:integer; refcount__gc:1; is_ref__gc:false } ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/:1:0","series":null,"tags":["php"],"title":"PHP 垃圾回收机制","uri":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/#变量的存储结构"},{"categories":["技术"],"content":" 顽固垃圾的产生简单变量 refcount__gc = 0 时，直接释放 zval 内存，没什么问题。 php \u003c?php $a = array( 'one' ); $a[] =\u0026 $a; xdebug_debug_zval( 'a' ); 以上将输出： text a: (refcount=2, is_ref=1)=array ( 0 =\u003e (refcount=1, is_ref=0)='one', 1 =\u003e (refcount=2, is_ref=1)=... ) 此时 unset($a) ， 引用情况将变成： text (refcount=1, is_ref=1)=array ( 0 =\u003e (refcount=1, is_ref=0)='one', 1 =\u003e (refcount=1, is_ref=1)=... ) 尽管 $a 已经被 unset， 但是内存中的数据还在，refcount=1， 不能被清理。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/:2:0","series":null,"tags":["php"],"title":"PHP 垃圾回收机制","uri":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/#顽固垃圾的产生"},{"categories":["技术"],"content":" 清理顽固垃圾php 5.3 引入了新的垃圾清理算法来清理以上垃圾。 具体参考 https://www.php.net/manual/zh/features.gc.collecting-cycles.php 可以这么理解： 对于一个包含环形引用的数组，对数组中包含的每个元素的 zval 进行减1操作，之后如果发现数组自身的 zval 的 refcount 变成了0，那么可以判断这个数组是一个垃圾，对象也类似。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/:3:0","series":null,"tags":["php"],"title":"PHP 垃圾回收机制","uri":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/#清理顽固垃圾"},{"categories":["技术"],"content":" 配置php 中 GC 默认开启，配置项是 ini 文件 zend.enable_gc 项 。也可以通过 gc_enable() 和 gc_disable()函数来开启和关闭GC。 垃圾分析算法将在节点缓冲区(roots buffer)满了之后启动。缓冲区默认可以放10,000个节点，当然你也可以通过修改Zend/zend_gc.c中的_GC_ROOT_BUFFER_MAX_ENTRIES_ 来改变这个数值，需要重新编译链接PHP。你也可以通过调用gc_collect_cycles()在节点缓冲区未满的情况下强制执行垃圾分析算法。 ","date":"2021-11-04","objectID":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/:4:0","series":null,"tags":["php"],"title":"PHP 垃圾回收机制","uri":"/posts/php-web/php%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/#配置"},{"categories":["技术"],"content":"过滤器可以用于管道，通过组合，解决实际问题，优雅而强大。 常用过滤器 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:0:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#"},{"categories":["技术"],"content":" cat shell cat name.txt mobile.txt \u003e data # 合并文件 # -n 显示行号 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:1:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#cat"},{"categories":["技术"],"content":" split ：拆分文件 shell split [-d] [-a num] [-l lines] [file [prefix]] split data # 默认每个文件1000行 命名 xaa xab ... # -l 100 : 指定每个文件行数 # -d ：改命名为 00 01 02 # -a 2 ： 命名改为 3 位 aaa aab / 001 002 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:2:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#split-拆分文件"},{"categories":["技术"],"content":" tac：反转文本行的顺序和 cat 类似， 写入标准输出前将文本行反转，适用于日志文件等。 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:3:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#tac反转文本行的顺序"},{"categories":["技术"],"content":" rev：反转字符 shell rev data # data 内容： # 1234 # abcd # 结果如下： # dcba # 4321 rev data | tac # 倒背如流 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:4:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#rev反转字符"},{"categories":["技术"],"content":" head / tail ：从数据开头和末尾选择数据行 shell head [-n lines] tail [-n lines] # 默认 10 行数据 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:5:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#head--tail-从数据开头和末尾选择数据行"},{"categories":["技术"],"content":" 列操作","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:6:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#列操作"},{"categories":["技术"],"content":" colrm ：删除数据列 shell colrm [startcol [endcol]] # 列编号 从 1 开始 # 把 students_old 文件删除 14 列到 20 列后 保存到 student_new 文件中 colrm 14 20 \u003c students_old \u003e students_new ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:6:1","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#colrm-删除数据列"},{"categories":["技术"],"content":" cut ： 抽取数据列 （与 colrm 相反） shell cut -c list [file ...] # 抽取data中的 14-30 和 42-49 列 cut -c 14-30,42-49 data # 结合其他命令 who | cut -c 1-8 | sort | uniq -c # 定界符分割，如下形式的数据也可以抽取 张三:男:23岁 王麻子:女:105岁 # 抽取按照 ':' 分割后的， 第 1 和第 3 列数据 cut -f 1,3 -d ':' data # 可以删除选项的空格 如 cut -f1,3 -d':' data # ps 如果一行没有定界符，默认返回整行，如果想抛弃这样的行，可以使用 -s ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:6:2","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#cut--抽取数据列-与-colrm-相反"},{"categories":["技术"],"content":" paste 组合数据列 shell paste [-d char...] [file ...] # 合并三个文件 paste file1 file2 file3 # 使用定界符 | paste -d '|' file1 file2 # 定界符轮换使用 paste -d '|\u0026' file1 file2 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:6:3","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#paste-组合数据列"},{"categories":["技术"],"content":" 比较文件","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:7:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#比较文件"},{"categories":["技术"],"content":" cmp ：逐字节比较 shell cmp file1 file2 # 相同的话就 不做任何处理 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:7:1","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#cmp-逐字节比较"},{"categories":["技术"],"content":" comm ： 比较有序文本文件 shell comm [-123] file1 file2 # 第一列 仅输出第一个文件有的行 # 第二列 仅输出第二个文件有的行 # 第三列 输出两个文件都有的行 # -1 -2 -3 表示抑制某一列的输入 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:7:2","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#comm--比较有序文本文件"},{"categories":["技术"],"content":" diff / sdiff： 比较无序文件 shell diff file1 file2 # 默认 有指示符 （c, d, a）和行号构成 # c 代表改变 d 代表删除 a 代表增加 # 中间 ------- 分割 # 文件2 新增两行 0a1,2 \u003e 沁园春·雪 \u003e # 第5 第7行不一致 5c7 \u003c 须晴日5，看红装素裹，分外妖娆6。 --- \u003e 须晴日，看红装素裹，分外妖娆。 # 文件2 删除了一行 10d11 \u003c 俱往矣，数风流人物，还看今朝。 # 选项 # -c 和 -u 可以显示 2 行上下文信息 # -C3 -U3 可以显示 3 行上下文信息 # 并列比较 等价的 diff -y file1 file2 sdiff file1 file2 # 选项 -l 相同的行只显示左侧的 -s 不显示相同的行 -w 50 指定列宽度 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:7:3","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#diff--sdiff-比较无序文件"},{"categories":["技术"],"content":" 统计和格式化","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#统计和格式化"},{"categories":["技术"],"content":" nl ：创建行号 shell nl [-v start] [-i increment] [-b a] [-n ln|rn|rz] [file ...] # start 起始号 # increment 增量 # 默认 nl 不对空行编号， -b a （all lines 所有行） # ln=左对齐，没有前导 0 # rn=右对齐，没有前导 0 # rz=右对齐，有前导 0 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:1","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#nl-创建行号"},{"categories":["技术"],"content":" wc : 单词统计 shell wc [-clLw] [file ...] # wc 默认输出三个数字：行数（line），单词数（word），字符数（char），LWC （look at Women Carefully） # -clw 显示指定列 # -L 显示最长的行 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:2","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#wc--单词统计"},{"categories":["技术"],"content":" expand: 将制表符转换成空格 shell expand [-i] [-t size | -t list] [file...] # -i 只转换开头的制表符 # -t size 默认size等于8 ，每8个位置一个制表符 # -t list 指定制表符位于特定的位置，0指第一个位置，如 expand -t 7,15,21 data ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:3","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#expand-将制表符转换成空格"},{"categories":["技术"],"content":" unexpand : 将空格转换成制表符 shell unexpand [-a] [-t size | -t list] [file ...] # 默认只替换开头的空格 # -a 标识替换所有空格 # -t 的参数和 expand 一致 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:4","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#unexpand--将空格转换成制表符"},{"categories":["技术"],"content":" fold: 长行分成短行 shell fold [-s] [-w width] [file ...] # 默认 80 字符每行， # -w width 可以重新指定宽度 # -s 不从中间分割单词 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:5","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#fold-长行分成短行"},{"categories":["技术"],"content":" fmt：将段落中的各行连接起来 shell fmt [-su] [-w width] [file...] # -w width 行宽 # -u 统一间距，单词间有多个空格，减少，只保留一个 # -s 仅拆分长行，而不连接短行 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:8:6","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#fmt将段落中的各行连接起来"},{"categories":["技术"],"content":" 选取、排列、组合与变换","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:0","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#选取排列组合与变换"},{"categories":["技术"],"content":" grep：选取包含特定模式的行 shell grep [-cilLnrsvwx] pattern [file...] # -c 统计数量, ls -F 会在子目录会添加/字符，以下可以统计反斜线的数量 即子目录的数量 ls -F /etc | grep -c \"/\" # 统计总数 ls /etc | wc -l # -i 忽略大小写 # -n 显示行号 grep -in pizza food-list.md # 忽略大小写查找出含 pizza 的行并显示行号 # -l 不显示包含的行，而显示包含的文件名 查找哪个班有小明同学 grep -il xiaoming class1_names class2_names class3_names # output class1_names # -L 与 -l相反，显示不包含的文件 grep -iL xiaoming class1_names class2_names class3_names # output class2_names class3_names # -w 单词完整匹配， grep now data # 会匹配 now，也会匹配 know grep -w now # 只匹配 now # -v 取反，选取不包含的模式的所有行 grep -v done todo_list # 列出所有未完成的项目 grep -cv done todo_list # 统计为完成的项目数目 # -x 查找占用整行的 grep -x \"hello world\" data # 返回 由 'hello world' 构成的行 # -r 递归搜索目录树 grep -r initialize admin # 搜索admin目录及子目录文件中是否有单词 “inttialize” # -s 抑制错误显示 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:1","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#grep选取包含特定模式的行"},{"categories":["技术"],"content":" look：查找以特定模式开头的所有单词 shell look [-df] pattern file # look 搜索以字母顺序排列的数据，并以特定的模式开头的行 # 因为 look 使用二分法，搜索数据，要求同时获取所有数据，故只能再管道的开头使用数据，不能再管道中间使用 # -d 只考虑字母和数字 # -f 忽略大小写 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:2","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#look查找以特定模式开头的所有单词"},{"categories":["技术"],"content":" sort : 排序数据 shell sort [-dfnru] [-o outfile] [infile...] # 排序数据和查看数据是否排序 # 排序后保存到原文件时，这是错误的 重定向输出时， shell 运行命令前会清空输出的文件，（除非设置了变量 noclobber ） sort names \u003e names # wrong sort -o names names # right # -d 只查看字母、数字和空白符，如果数据中有妨碍排序的标点时，使用此选项 # -f 忽略大小写 # -n 识别开头的数字，按数字大小排序 # -r 倒序 # -u 唯一，相同的行，只留下一行 sort -c[u] [file] # -c 检查数据是否有序，无序输出，开始无序的行，有序无消息（没有消息就好的消息） # -u 配合-c使用，可以确保数据是有序， 唯一 # 排序规则：取决于你系统的字符组织方式 # 早期使用 ASCII 顺序， （SNUL） 空格 数字 大写字母 小写字母 # 现在 使用环境变量 LC_COLLATE 指定使用哪一种排序 c 是 ASCII locale # 可以查看所有区域设置的当前值 locale -a # 查看自己的系统支持哪些区域设置 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:3","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#sort--排序数据"},{"categories":["技术"],"content":" uniq: 查找重复行uniq 可以做四个不同的任务 消除重复行 uniq 选取重复行 uniq -d 选取唯一行 uniq -u 统计重复行数量 uniq -c shell uniq [-cdu] [infile] [outfile] # 输入必须是有序的，即重复行是连续的 # -d 只查看重复行 # -u 只查看唯一行 # -c 统计唯一行的数量 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:4","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#uniq-查找重复行"},{"categories":["技术"],"content":" jion: 基于特定字段的值将两个有序文件组合在一起 shell join [-i] [-a1|-v1] [-a2|v2] [-1 field1] [-2 field2] file1 file2 # eg: cat file1 111 张三 222 李四 333 王五 999 孙悟空 cat file2 111 10086 222 10010 333 110 444 120 555 119 # join file1 file2 111 张三 10086 222 李四 10010 333 王五 110 # join -a1 file1 file2 111 张三 10086 222 李四 10010 333 王五 110 999 孙悟空 # join -v1 file1 file2 999 孙悟空 # 如上， join 关联类似于关系数据库的的join， # 没-a选项时 内关联 inner join ，只输出关联字段匹配的行 # -a1 -a2 时还输出关联字段不匹配的行 # -v1 -v2 只查看不匹配的行 # -i 忽略大小写 # join 默认使用的是 文件的第一个字段 可以使用 -1 -2 指定不同的字段 # 特别注意： join 使用的有序数据 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:5","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#jion-基于特定字段的值将两个有序文件组合在一起"},{"categories":["技术"],"content":" tsort: 以偏序创建全序 shell tsort [file] # 偏序：只指定了一部分活动的顺序 # file 中的每一行必须包含一堆空白符分割的字符串 每个字符串代表一个偏序 # 最终结果生成一个完整序列 ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:6","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#tsort-以偏序创建全序"},{"categories":["技术"],"content":" strings: 二进制文件中搜索","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:7","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#strings-二进制文件中搜索"},{"categories":["技术"],"content":" tr: 转换字符 转换字符 ：小写转大写，制表符转空格 挤压字符：连续数字 替换为 “X” ,多个空格转换为一个 删除指定字符：删除所有指标符 shell tr [-cds] [set1 [set2]] # 只能从标准输入接收数据，不能从文件读取，如果要从文件读取请重定向 tr a A \u003c old \u003e new # old 文件中的 a 替换成 A 了, 并写入 new 文件 tr abc ABC \u003c old \u003e new # abc 替换 ABC # 以下情况，第二组最后一个字符是重复的，等价 tr abcd Ax tr abcd Axxxx # 特殊字符需要引用或转义 tr ':;?' \\. # :;? 均替换成了 . # 支持范围 tr A-Z a-z tr 0-9 A-J # 缩写 (预定义字符) tr [:upper:] [:lower:] # 大写转小写 tr [:diggit:] A-J # 不可显示的字符 (转义 or ASCII码的值) tr '\\r' '\\n' \u003c macfile \u003e unixfile # 回车符转换为换行符 tr '\\015' '\\012' \u003c macfile \u003e unixfile tr '\\t' ' ' # 制表符转换为空格 # ********* 挤压 ********* tr -s [:digit:] X # 数字转换成 x tr -s ' ' ' ' # 挤压空格 # **** 删除 **** tr -d '()' # **** 补集 ***** tr -c ' \\n' x \u003c olddata \u003e newdata # hello world ==\u003e xxxxx xxxxx eg: 统计两个文件的单词数 cat greek roman | tr -cs [:alpha:]\\' \"\\n\" | sort -fu | wc -l ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:8","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#tr-转换字符"},{"categories":["技术"],"content":" sed: 非交互式文本编辑(提前设计命令，命令发给程序，自动执行)(流编辑器) shell sed [-i] command | -e command ... [f] -i # 直接修改原文件 # 命令 # a 行后插入， 多行的话 末尾\"\\\" # i 行前插入， 多行 \"\\\" # c 替换 # d 删除 # p 打印 # s 替换 可用正则表达式 # 查看某一行 sed -n '1003,1p' data.txt # 修改某一行 sed -i '1003c new line content' data.txt # 将 11 修改为 12，g 代表一行的所有 sed -i 's/11/12/g' xxx.log # 每行 开头/末尾 追加 aa sed -i 's/^/aa/g' xxx.log sed -i 's/$/aa/g' xxx.log # 修改每行第2个 匹配的11 sed -i 's/11/12/2' xxx.log # 修改每行弟2个以及以后的 匹配的11 sed -i 's/11/12/2g' xxx.log # \u0026 符号代表的是你前面的匹配的模式 将每行的第一个 hello 替换为 (hello) sed 's/hello/(\u0026)/' xxx.log ","date":"2021-11-04","objectID":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/:9:9","series":null,"tags":["unix","linux"],"title":"Unix 过滤器","uri":"/posts/unixs/unix-%E8%BF%87%E6%BB%A4%E5%99%A8/#sed-非交互式文本编辑提前设计命令命令发给程序自动执行流编辑器"},{"categories":["技术"],"content":" 移动光标 - 光标移动到上一行开头 + 或 \u003creturn\u003e 光标移动到下一行开头 0 当前行开头 $ 当前行末尾 ^ 当前行第一个非空字符 w 下一个单词的词首 W 忽略标点 e 下一个单词的词尾 E … b 上一个单词的词首 B … ( 移动到上一个句子 ) 下一个句子 { 上一个段落 } 下一个段落 H 移动到屏幕顶部 M 屏幕中间 L 最后一行 数字组合 10w 向后移动10个单词 20j 向下移动20行 3{ 向后移动3段 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:1:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#移动光标"},{"categories":["技术"],"content":" 在编辑缓冲区移动^F 向下移动一屏 ^B 向上移动一屏 ^D 向下移动半屏 ^U 向上移动半屏 6^F 向下移动6屏 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:2:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#在编辑缓冲区移动"},{"categories":["技术"],"content":" 跳转到前一位置`` 或 '' 返回到前一个位置 ma 标识当前行名称为 “a” `a 或 'a 即可跳转到标记开头 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:3:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#跳转到前一位置"},{"categories":["技术"],"content":" 搜索模式/ 或者 ? 加要搜索的内容 / 向前搜索 ? 向后搜索 n 下一个 N 上一个 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:4:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#搜索模式"},{"categories":["技术"],"content":" 行号:set number 显示行号 :set nonumber 隐藏行号 100G 跳转到100行 gg 第一行 G 末尾一行 :100 跳转到100行 :$ 最后一行 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:5:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#行号"},{"categories":["技术"],"content":" 插入文本i 光标前插入 a 光标后插入 I 当前行开头插入 A 当前行结尾插入 o 当前行下面插入一行 O 当前行上面插入一行 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:6:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#插入文本"},{"categories":["技术"],"content":" 修改文本ra 把光标上的单个字符修改为a Rxxx\u003cEsc\u003e R会切换到输入模式，键入的字符将被替换当前行的一个字符， 返回命令模式 s (substitute, 替换) 使用多个字符替换光标上的字符 C 替换当前光标位置到本行结尾所有字符 S 或 cc 替换整行 cmove 从光标处替换到 move 所给出的位置处 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:7:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#修改文本"},{"categories":["技术"],"content":" 替换文本:s/pattern/replace/gc 当前行上替换 默认替换第一个匹配项，g(global 全局)，替换本行所有匹配项； c(confirm, 确认) 替换前需要你同意 :45s/pattern/replace/ 替换57行的第一个匹配项 :10,50s/pattern/replace/ 10到50行 :.,$s/s/pattern/replace/g 当前行都结尾最后一行 . 代表当前行, $ 代表结尾最后一行 :%s/pattern/replace/g 替换所有行 % 代表所有行 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:8:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#替换文本"},{"categories":["技术"],"content":" 删除文本x 删除当前光标处字符 X 删除光标左边的一个字符 D 删除当前光标到本行结尾的字符 dmove 删除当前光标都 move 所在位置的字符 dd 删除当前行 :12d 删除12行 等价于 12dd :12,34 删除12到34行 eg: dw 删除一个单词 d10w 删除10个单词 d10W 删除10个单词（忽略标点） db 向后删除1个单词 d2) 删除2个句子 d3} 删除5个段落 dG 删除到结尾 dgg 删除到开头 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:9:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#删除文本"},{"categories":["技术"],"content":" 撤销或重复改变u 撤销上一条命令 U 恢复当前行 . 重复上一命令对编辑缓冲区的修改 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:10:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#撤销或重复改变"},{"categories":["技术"],"content":" 恢复删除每删除一行或多行文本的时，vi 都将内容保存在一个特殊的存储区里，编号缓冲区。vi 有 9 个缓冲区。 任何时候，都可以将编号缓冲区的内容插入到编辑缓冲区里。 \"1p 或 \"2P 即可插入 p 在当前行下，P 在当前行上 tip: 编号缓冲区只存储删除的行，单个字符或多个字符不会保存在编号缓冲区 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:11:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#恢复删除"},{"categories":["技术"],"content":" 移动文本vi 总有一个无名缓冲区，为上一次删除保存了一份副本。任何时候可以使用 p 或 P 将缓冲区的内容复制到编辑缓冲区中。 xp 调换两个字母的位置 deep 调换两个单词（光标处于第一个单词的左边） ddp 调换两行 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:12:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#移动文本"},{"categories":["技术"],"content":" 复制文本分3步 y、yy 或 Y 命令将文本由编辑缓冲区复制到无名缓冲区，但不删除原始文本。 光标移动到插入到希望插入文本的位置。 使用 p 或 P 插入。 y (yank 接出) 和 d 命令工作方式一样 yw 接出一个单词 y10W 10个单词 yb 向前接出一个单词 y2) 接出2个句子 y5} 接出5个段落 yy 接出1行 12yy 接出12行 等价于 12Y y$ 接出到行尾部 y0 接出到行开头 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:13:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#复制文本"},{"categories":["技术"],"content":" 改变字母大小写~ 改变当前字母大小写，并向后移动一个位置 7~ 重复7次~ ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:14:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#改变字母大小写"},{"categories":["技术"],"content":" 设置选项 shell :set [no]option... :set option[=value]... # 显示输入提醒 :set showmode :set noshowmode # 显示行号 :set number :set nonumber # 选项是变量的 :set tabstop=4 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:15:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#设置选项"},{"categories":["技术"],"content":" 显示选项 shell :set [option[?]... | all] # 显示所有选项的值 :set all :set number? ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:16:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#显示选项"},{"categories":["技术"],"content":" 键入过程自动换行:set wrapmargin=n / :set wm=n n 是从右边边缘算起开始换行的位置 :set wm=l 文本尽可能的长 :set wm=0 关闭自动换行 :set autoindent 文本自动缩进 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:17:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#键入过程自动换行"},{"categories":["技术"],"content":" 分隔和连接行r\u003cReturn\u003e 长行分割短行, r 可以将一个字符替换成另外一个字符，此处将 空格符 替换成 换行符 J 可以将当前行和下一行合并到一个长行 （默认会在单词之间插入一个空格，句子行尾插入两个空格） 3J 合并一下三行 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:18:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#分隔和连接行"},{"categories":["技术"],"content":" 复制和移动行指定行号的复制和移动可以使用 ex 命令 :co (copy, 复制) :m (move , 移动)。 :_x[,y]co_z x,y,z 代表行号 :_x[,y]m_z :5co10 复制5行插入到第10行下面 :6,9co15 复制第6行至第9行，插入到第15行下面 :5m10 移动第5行，插入到第10行下面 4,8m12 移动第4行至第9行，插入到第12行下面 和其他 ex 命令一样 . 代表当前行，$ 代表最后一行： :1,.m$ 将第1行至当前行，移动到编辑缓冲区末尾 :.,$m0 将当前行至末尾，移动到编辑缓冲区开头 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:19:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#复制和移动行"},{"categories":["技术"],"content":" 输出 shell 命令 :! + 希望运行的命令，vi 将命令发送给 shell 执行，当命令结束后吗，控制将返回 vi。 :!date 输出日期 :!! 重复上一条命令 有时候希望输入 不止1条 shell 命令。这时候可以启动一个 新的shell 。 :sh 启动一个新的 shell 副本，^D 或 输入 exit 命令后返回 vi 中。 :!bash 启动一个 Bash shell ，:!tcsh 启动一个 Tcsh shell ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:20:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#输出-shell-命令"},{"categories":["技术"],"content":" 将文件的数据插入编辑缓冲区:[line]r file line 行号，file 文件名称 :10r info 将 info 的文件内容插入到第 10 行之后 :0r info 0 表示开头 :$r info $ 表示结尾 r info 省略了行号，vi 将数据插入到当前行之后 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:21:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#将文件的数据插入编辑缓冲区"},{"categories":["技术"],"content":" 将shell 命令输出插入到编辑缓冲区:r !ls 将 ls 的结果插入到编辑缓冲区 :0r !date 在第一行插入当前时间和日期 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:22:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#将shell-命令输出插入到编辑缓冲区"},{"categories":["技术"],"content":" 使用程序处理数据使用 ! 和 !! 命令可以将编辑缓冲区的行发送给另一个程序。该程序的输出将替换原始行。 5!!sort 排序当前光标的下5行数据 10!!fmt 从当前行开始格式化10行 ! + 光标移动命令 + 程序名称。 !}fmt 格式化当前行至段落末尾之间的文本 gg、1G +!Gfmt 格式化整个编辑缓冲区 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:23:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#使用程序处理数据"},{"categories":["技术"],"content":" 将数据写入文件:w 数据写入原始文件 :w file 将数据写入新文件 :w! file 覆盖已有文件 :w\u003e\u003e file 追加数据到文件 :10w! file 将第10行数据写入文 :10,20w \u003e\u003e file 将10到20行数据追加到文件里 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:24:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#将数据写入文件"},{"categories":["技术"],"content":" 切换文件编辑文件时，不必退出重启 vi ，切换到另一个新文件。确保之前编辑缓冲区内容的保存（:w）。 :e document 编辑指定文件 :e! 重新编辑当前文件，忽略自动检查，即放弃未保存的修改 :e! file 编辑指定文件，忽略自动检查 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:25:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#切换文件"},{"categories":["技术"],"content":" 使用缩写为经常使用的单词或表达式，创建缩写。 :ab [short long] short 是缩写，long是对应的原文 :ab puf public function 单独键入 puf 时，vi 将会自动转换成 public function :una puf 删除缩写 :ab 查看目前所有缩写列表 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:26:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#使用缩写"},{"categories":["技术"],"content":" 宏:map 可以创建单字符命令,即宏(macro). :map [x commands] x 是单个字符, commands 是 vi 或 ex 命令. :map X dd 创建一个 X 的宏, 这个宏删除当前的行。 :map * I/* ^V\u003cEsc\u003eA */^V\u003cEsc\u003e 创建 * 的宏，将该行用 /* 和 */ 包裹起来, 即注释掉. :map 显示所有的宏列表 :unmap X 移除宏 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:27:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#宏"},{"categories":["技术"],"content":" 初始化文件vi 或 vim 启动时, 会在 home 目录查找初始化文件. 如果存在就读取并执行查找到的任何 ex 命令. vi 的初始化文件名为 .exrc (rc 代表 ‘run commands’) . vim 的初始化文件名为 .vimrc vim 首先查找 .vimrc, 如果不存在 , 则查找 .exrc . 如果同时存在, vim 只会读取 .vimrc 文件. 初始化文件, \" 字符开头的将被忽略, 可以作为注释. shell \" ====================== \"sample vi/vim init file \" ======================= \" \" 1. options set autoindent set compatible set ignorecase set showmatch set showmode set wrapmargin=6 \" \" 2. abbreviation ab puf public function ab prf private function \" \" 3. macros map K deep map X dd \" \" 4. shell commands !date; sleep 2 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:28:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#初始化文件"},{"categories":["技术"],"content":" 使用两个初始化文件vi/vim 启动时, 会在执行 home 目录的初始化文件. 然后检查 exrc 选项的状态. 如果选项打开, 那么程序将在工作目录（即当前目录）查找第二个初始化文件。 通过这种方式，可是使各个目录都有自己的初始化文件，从而组织各个文件。 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:28:1","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#使用两个初始化文件"},{"categories":["技术"],"content":" vim 增强内容 屏幕拆分 多级撤销 支持鼠标 GUI 命令行历史 命令行补全 文件名补全 搜索历史 语法高亮 高亮显示：选取文本行、文本块，然后再对文本进行操作 多缓冲区 支持宏：记录、修改和运行宏的工具 内置脚本 自动命令：自动执行的预定义命令 ","date":"2021-11-04","objectID":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/:29:0","series":null,"tags":["unix","linux","vi"],"title":"Vi 命令一览","uri":"/posts/unixs/vi-%E5%91%BD%E4%BB%A4%E4%B8%80%E8%A7%88/#vim-增强内容"},{"categories":["技术"],"content":" 镜像相关","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#镜像相关"},{"categories":["技术"],"content":" 列出镜像docker image ls 无仓库名的是悬空镜像，docker image ls -f dangling=true 查看悬空镜像， docker image prune 删除悬空镜像。 该命令默认，仅仅显示顶层镜像，docker image ls -a , 可以显示中间层镜像。 过滤参数 shell // 指定仓库名 docker image ls ubuntu // 指定仓库名和标签名 docker image ls ubuntu:18.04 // --filter 简写 -f 过滤 docker image ls -f since=mongo:3.2 docker image ls -f label=com.example.version=0.1 docker image ls -f dangling=true 显示格式 shell docker image ls -q docker image ls --format \"table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}\" --filter 配合 -q 产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker 命令行使用过程中非常常见。 ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#列出镜像"},{"categories":["技术"],"content":" 删除镜像 shell docker image rm [选项] \u003c镜像1\u003e [\u003c镜像2\u003e ...] 镜像标识可以是 长id，短id，镜像名，镜像名：标签等。 Untagged 和 Deleted，untagged 表明镜像和删除标签的关联已经被取消，可能还有其余的标签在镜像上，所有镜像没有被删除。当镜像上的所有标签都被删除后，则触发 delete 行为。 与 docker image ls -q 命令的配合 shell // 删除所有仓库名为 redis 的镜像 docker image rm $(docker image ls -q redis) // 删除所有在 mongo:3.2 之前的镜像 docker image rm $(docker image ls -q -f before=mongo:3.2) ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:2","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#删除镜像"},{"categories":["技术"],"content":" 容器相关","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#容器相关"},{"categories":["技术"],"content":" 启动 基于镜像新建一个容器并启动 shell docker run ubuntu:18.04 // -t 让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上, -i 则让容器的标准输入保持打开。 docker run -t -i ubuntu:18.04 检查本地是否存在指定的镜像，不存在就从 registry 下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 将一个终止（exited）状态的容器重新启动 shell // 传入容器id 即可 docker container start [options] containerID ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#启动"},{"categories":["技术"],"content":" 容器列表 shell // 获取正在运行的容器 docker container ls // 获取所有容器 docker container ls -a ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:2","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#容器列表"},{"categories":["技术"],"content":" 停止与重启 shell // 停止 docker container stop containerID // 重启 docker container restart containerID ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:3","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#停止与重启"},{"categories":["技术"],"content":" 进入容器-d 参数运行容器时候，容器启动后会进入后台。 shell // 方式1 ， ps：如果在 stdin 中 exit，会导致容器的停止 docker attach containerID // 方式2 ， ps： exit 不会导致容器停止 docker exec [options] containerID command ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:4","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#进入容器"},{"categories":["技术"],"content":" 查看容器信息 shell docker inspect [OPTIONS] NAME|ID [NAME|ID...] 数据卷 信息在 “Mounts” Key 下面 ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:5","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#查看容器信息"},{"categories":["技术"],"content":" 导出和导入导出 shell // 导入容器快照到本地文件 docker export 7691a814370e \u003e ubuntu.tar 导入 shell // 从容器快照导入为镜像 cat ubuntu.tar | docker import - test/ubuntu:v1.0 docker image ls // 会显示 test/ubuntu:v1.0 镜像 // 通过 url 导入 docker import http://example.com/exampleimage.tgz example/imagerepo docker load 也可以导入镜像存储文件到本地镜像库，区别在于： docker import 将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），可以重新指定标签等元数据信息。 docker load 将保存镜像的完整记录，体积也要大。 ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:6","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#导出和导入"},{"categories":["技术"],"content":" 删除 shell // 删除一个处于终止状态的容器, -f 可以删除一个运行中的容器 docker container rm [options] containerID // 清理掉所有处于终止状态的容器 docker container prune ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:7","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#删除"},{"categories":["技术"],"content":" 数据管理","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#数据管理"},{"categories":["技术"],"content":" 数据卷数据卷是可以一个或多个容器使用的特殊目录，它绕过了 UFS ， 可以提供很多特性： 容器间共享和重用 对数据卷的修改会立即生效 对数据卷的更新，不影响镜像 容器删除后，数据卷还会存在 创建数据卷 shell docker volume create my-vol 查看所有数据卷 shell docker volume ls // 查看特定数据卷信息 docker volume inspect my-vol 启动一个挂载有数据卷的容器 shell docker run -d -p --name \\ # -v my-vol:/usr/share/nginx/html \\ --mount source=my-vol,target=/usr/share/nginx/html \\ nginx:alpine 删除数据卷 shell docker volume rm my-vol // 删除容器时一起删除 docker container rm -v containerID // 批量删除无主的数据卷 docker volume prune ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#数据卷"},{"categories":["技术"],"content":" 数据卷数据卷是可以一个或多个容器使用的特殊目录，它绕过了 UFS ， 可以提供很多特性： 容器间共享和重用 对数据卷的修改会立即生效 对数据卷的更新，不影响镜像 容器删除后，数据卷还会存在 创建数据卷 shell docker volume create my-vol 查看所有数据卷 shell docker volume ls // 查看特定数据卷信息 docker volume inspect my-vol 启动一个挂载有数据卷的容器 shell docker run -d -p --name \\ # -v my-vol:/usr/share/nginx/html \\ --mount source=my-vol,target=/usr/share/nginx/html \\ nginx:alpine 删除数据卷 shell docker volume rm my-vol // 删除容器时一起删除 docker container rm -v containerID // 批量删除无主的数据卷 docker volume prune ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#创建数据卷"},{"categories":["技术"],"content":" 数据卷数据卷是可以一个或多个容器使用的特殊目录，它绕过了 UFS ， 可以提供很多特性： 容器间共享和重用 对数据卷的修改会立即生效 对数据卷的更新，不影响镜像 容器删除后，数据卷还会存在 创建数据卷 shell docker volume create my-vol 查看所有数据卷 shell docker volume ls // 查看特定数据卷信息 docker volume inspect my-vol 启动一个挂载有数据卷的容器 shell docker run -d -p --name \\ # -v my-vol:/usr/share/nginx/html \\ --mount source=my-vol,target=/usr/share/nginx/html \\ nginx:alpine 删除数据卷 shell docker volume rm my-vol // 删除容器时一起删除 docker container rm -v containerID // 批量删除无主的数据卷 docker volume prune ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#查看所有数据卷"},{"categories":["技术"],"content":" 数据卷数据卷是可以一个或多个容器使用的特殊目录，它绕过了 UFS ， 可以提供很多特性： 容器间共享和重用 对数据卷的修改会立即生效 对数据卷的更新，不影响镜像 容器删除后，数据卷还会存在 创建数据卷 shell docker volume create my-vol 查看所有数据卷 shell docker volume ls // 查看特定数据卷信息 docker volume inspect my-vol 启动一个挂载有数据卷的容器 shell docker run -d -p --name \\ # -v my-vol:/usr/share/nginx/html \\ --mount source=my-vol,target=/usr/share/nginx/html \\ nginx:alpine 删除数据卷 shell docker volume rm my-vol // 删除容器时一起删除 docker container rm -v containerID // 批量删除无主的数据卷 docker volume prune ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#删除数据卷"},{"categories":["技术"],"content":" 挂载主机目录","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:2","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#挂载主机目录"},{"categories":["技术"],"content":" 其他","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#其他"},{"categories":["技术"],"content":" 查看镜像容器和数据卷所占体积docker system df ","date":"2021-11-04","objectID":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:1","series":null,"tags":["docker"],"title":"容器常用命令","uri":"/posts/%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#查看镜像容器和数据卷所占体积"},{"categories":["技术"],"content":" 概述控制反转**（Inversion of Control, IoC）**，是面向对象编程的一种设计原则，目的是降低代码耦合。常见方式有 依赖注入 （Dependence Injection, DI） 和 依赖查找 （Dependency Lookup） 。 ","date":"2021-11-04","objectID":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/:1:0","series":null,"tags":["设计模式"],"title":"依赖注入","uri":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/#概述"},{"categories":["技术"],"content":" 技术描述Class A中用到了Class B的对象b，一般情况下，需要在A的代码中显式的new一个B的对象。 采用依赖注入技术之后，A的代码只需要定义一个私有的B对象，不需要直接new来获得这个对象，而是通过相关的容器控制程序来将B对象在外部new出来并注入到A类里的引用中。 ","date":"2021-11-04","objectID":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/:2:0","series":null,"tags":["设计模式"],"title":"依赖注入","uri":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/#技术描述"},{"categories":["技术"],"content":" 实现方式依赖注入有如下实现方式： 基于构造函数。实现特定参数的构造函数，在新建对象时传入所依赖类型的对象。 基于 set 方法。实现特定属性的public set方法，来让外部容器调用传入所依赖类型的对象。 ","date":"2021-11-04","objectID":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/:3:0","series":null,"tags":["设计模式"],"title":"依赖注入","uri":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/#实现方式"},{"categories":["技术"],"content":" Ioc 容器不管是哪一种实现，依赖单元的实例化代码是一个重复、繁琐的过程，且当系统很复杂的时候，依赖嵌套，前后关系的处理将很麻烦。所以 IoC 容器 被发明出来了。IoC Container提供了动态地创建、注入依赖单元，映射依赖关系等功能，减少了许多代码量。 ","date":"2021-11-04","objectID":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/:4:0","series":null,"tags":["设计模式"],"title":"依赖注入","uri":"/posts/php-web/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/#ioc-容器"},{"categories":["技术"],"content":" 条件判断语句 相等: ==, !=, \u003c, \u003e, \u003c=, \u003e= 正则: =(匹配正则), !(不匹配正则) 包含: in(包含), not in(不包含) 布尔操作： and(与), or(或), nand(非与), xor(非或) 一元运算符： !(取反) 复合表达式： ()(复合表达式), !()(对复合表达式结果取反) ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:1:0","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#条件判断语句"},{"categories":["技术"],"content":" 测试管道以下 generator 会触发一个生成事件，即产生一条记录。 text input { generator { lines =\u003e [\"Generated line\"] count =\u003e 1 } } ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:2:0","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#测试管道"},{"categories":["技术"],"content":" 一些 filter","date":"2021-11-04","objectID":"/posts/elastic/logstash/:3:0","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#一些-filter"},{"categories":["技术"],"content":" 使用本地Geo数据库，并添加标签 text geoip { source =\u003e \"source_ip\" target =\u003e \"source_geoip\" database =\u003e \"/usr/share/logstash/data/geolite/GeoLite2-City.mmdb\" add_tag =\u003e [\"%{[source_geoip][country_name]}\",\"%{[source_geoip][city_name]}\"] tag_on_failure =\u003e \"_geoip_source_fail\" } ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:3:1","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#使用本地geo数据库并添加标签"},{"categories":["技术"],"content":" 运行ruby代码 行内运行 text # 原数据 [{\"interface_name\":\"xxx\" ...},{...}] 修改 interface_name key名 为 IfName 等 ruby { code =\u003e ' interface_msg = event.get(\"interface_msg\") if interface_msg interface_msg.each_with_index do |interface, index| interface[\"IfName\"] = interface[\"interface_name\"] interface.delete(\"interface_name\") interface[\"IfMAC\"] = interface[\"interface_mac\"] interface.delete(\"interface_mac\") if interface[\"interface_status\"]==1 interface[\"IfStatus\"] = \"up\" else interface[\"IfStatus\"] = \"down\" end interface.delete(\"interface_status\") event.set(\"interface_msg\", interface_msg) end end ' } 单独文件 text ruby { path =\u003e \"/usr/share/logstash/script/rt_device_interface_data.rb\" } rt_device_interface_data.rb 文件内容如下： ruby def register(params) end # 将 interface_state 转为 json 字符串 # {RxDropped=0, IfMtu=1500},{RxDropped=0, IfMtu=1500} # [{\"RxDropped\":\"0\",\"IfMtu\":\"1500\"},{\"RxDropped\":\"0\",\"IfMtu\":\"1500\"}] def filter(event) str = event.get('interface_state') log_dict_list = str.split(\"},{\").map do |log| log.gsub(/[{}]/, '').split(\", \").map do |kv| k, v = kv.split(\"=\") [k.to_sym, v] end.to_h end # 将字典列表转换为JSON字符串 #log_json_string = JSON.generate(log_dict_list) event.set('interface_state', log_dict_list) return [event] end ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:3:2","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#运行ruby代码"},{"categories":["技术"],"content":" 配置管道在 logstash 配置文件 pipelines.yml 中新增管道和配置路径: text - pipeline.id: main path.config: \"/usr/share/logstash/pipeline\" - pipeline.id: dp path.config: \"/usr/share/logstash/dp_pipeline\" 管道 main 是默认的管道，我们新增了dp管道。 在配置路径下新增配置文件，配置管道处理数据的逻辑即可： text input { udp { port =\u003e 10523 type =\u003e \"gateway\" } } filter { # code more ... } output { # code more ... } ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:4:0","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#配置管道"},{"categories":["技术"],"content":" 两个管道相连我们希望数据先经过 main 管道，再经过 dp 管道，可以做如下操作： 在 main 管道 配置文件的 output 中将数据发送至一个管道虚拟地址，下例为 dp_router text output { pipeline { send_to =\u003e dp_router } } 在 dp 管道 配置文件的 input 中从虚拟地址接收数据, 然后即可处理数据： text input { pipeline { address =\u003e dp_router } } pipeline to pipeline 的常用模式 分发 (一对多) 有多种类型的数据通过单个输入传入，且每种数据都有自己复杂的处理规则集的情况下，您可以使用分发器模式。 text input { beats { port =\u003e 5044 } } output { if [type] == apache { pipeline { send_to =\u003e weblogs } } else if [type] == system { pipeline { send_to =\u003e syslog } } else { pipeline { send_to =\u003e fallback } } } 隔离 如果配置了多个输出，任意一个输出服务down了，将阻塞其他输出，使用输出隔离器模式和持久队列，即使一个输出出现故障，我们也可以继续发送到 Elasticsearch。 text # config/pipelines.yml - pipeline.id: intake config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [es, http] } } - pipeline.id: buffered-es queue.type: persisted config.string: | input { pipeline { address =\u003e es } } output { elasticsearch { } } - pipeline.id: buffered-http queue.type: persisted config.string: | input { pipeline { address =\u003e http } } output { http { } } fork 我们在自己的系统中收集完整的索引信息，但同时我们也需要将数据发送给合作伙伴，这是我们需要对索引进行额外处理（例如：删除敏感信息等） text # config/pipelines.yml - pipeline.id: intake queue.type: persisted config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [\"internal-es\", \"partner-s3\"] } } - pipeline.id: buffered-es queue.type: persisted config.string: | input { pipeline { address =\u003e \"internal-es\" } } # Index the full event output { elasticsearch { } } - pipeline.id: partner queue.type: persisted config.string: | input { pipeline { address =\u003e \"partner-s3\" } } filter { # Remove the sensitive data mutate { remove_field =\u003e 'sensitive-data' } } output { s3 { } } # Output to partner's bucket 收集 (多对一) 许多管道流入单个管道，在其中共享输出和处理。 text # config/pipelines.yml - pipeline.id: beats config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [commonOut] } } - pipeline.id: kafka config.string: | input { kafka { ... } } output { pipeline { send_to =\u003e [commonOut] } } - pipeline.id: partner # This common pipeline enforces the same logic whether data comes from Kafka or Beats config.string: | input { pipeline { address =\u003e commonOut } } filter { # Always remove sensitive data from all input sources mutate { remove_field =\u003e 'sensitive-data' } } output { elasticsearch { } } ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:4:1","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#两个管道相连"},{"categories":["技术"],"content":" 两个管道相连我们希望数据先经过 main 管道，再经过 dp 管道，可以做如下操作： 在 main 管道 配置文件的 output 中将数据发送至一个管道虚拟地址，下例为 dp_router text output { pipeline { send_to =\u003e dp_router } } 在 dp 管道 配置文件的 input 中从虚拟地址接收数据, 然后即可处理数据： text input { pipeline { address =\u003e dp_router } } pipeline to pipeline 的常用模式 分发 (一对多) 有多种类型的数据通过单个输入传入，且每种数据都有自己复杂的处理规则集的情况下，您可以使用分发器模式。 text input { beats { port =\u003e 5044 } } output { if [type] == apache { pipeline { send_to =\u003e weblogs } } else if [type] == system { pipeline { send_to =\u003e syslog } } else { pipeline { send_to =\u003e fallback } } } 隔离 如果配置了多个输出，任意一个输出服务down了，将阻塞其他输出，使用输出隔离器模式和持久队列，即使一个输出出现故障，我们也可以继续发送到 Elasticsearch。 text # config/pipelines.yml - pipeline.id: intake config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [es, http] } } - pipeline.id: buffered-es queue.type: persisted config.string: | input { pipeline { address =\u003e es } } output { elasticsearch { } } - pipeline.id: buffered-http queue.type: persisted config.string: | input { pipeline { address =\u003e http } } output { http { } } fork 我们在自己的系统中收集完整的索引信息，但同时我们也需要将数据发送给合作伙伴，这是我们需要对索引进行额外处理（例如：删除敏感信息等） text # config/pipelines.yml - pipeline.id: intake queue.type: persisted config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [\"internal-es\", \"partner-s3\"] } } - pipeline.id: buffered-es queue.type: persisted config.string: | input { pipeline { address =\u003e \"internal-es\" } } # Index the full event output { elasticsearch { } } - pipeline.id: partner queue.type: persisted config.string: | input { pipeline { address =\u003e \"partner-s3\" } } filter { # Remove the sensitive data mutate { remove_field =\u003e 'sensitive-data' } } output { s3 { } } # Output to partner's bucket 收集 (多对一) 许多管道流入单个管道，在其中共享输出和处理。 text # config/pipelines.yml - pipeline.id: beats config.string: | input { beats { port =\u003e 5044 } } output { pipeline { send_to =\u003e [commonOut] } } - pipeline.id: kafka config.string: | input { kafka { ... } } output { pipeline { send_to =\u003e [commonOut] } } - pipeline.id: partner # This common pipeline enforces the same logic whether data comes from Kafka or Beats config.string: | input { pipeline { address =\u003e commonOut } } filter { # Always remove sensitive data from all input sources mutate { remove_field =\u003e 'sensitive-data' } } output { elasticsearch { } } ","date":"2021-11-04","objectID":"/posts/elastic/logstash/:4:1","series":null,"tags":["es","logstash"],"title":"Logstash","uri":"/posts/elastic/logstash/#pipeline-to-pipeline-的常用模式"},{"categories":["技术"],"content":" RESTful 接口 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 以下通过 kibanan 操作。 ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:1:0","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#restful-接口"},{"categories":["技术"],"content":" 基本 curd 操作","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:0","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#基本-curd-操作"},{"categories":["技术"],"content":" 新增 text # post vdong/_doc/\u003cid\u003e ,不写id会自动生成随机的 _id POST vdong/_doc { \"name\": \"vodng\", \"age\": 23, \"sex\":\"男\" } GET vdong/_doc/t1pqInwB9bbxQEztANxo ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:1","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#新增"},{"categories":["技术"],"content":" 查看 text GET vdong/_doc/t1pqInwB9bbxQEztANxo // 查看 id 为 1 的文档是否存在 HEAD vdong/_doc/1 // 查看索引是否存在 HEAD vdong // 获取 vdong 索引文档个数 GET vdong/_count ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:2","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#查看"},{"categories":["技术"],"content":" 修改 text // 需要完整资源信息 PUT vdong/_doc/12 { \"name\": \"栋\", \"age\": 23, \"sex\":\"男\" } // 不需要完整资源 POST vdong/_update/12 { \"doc\": { \"name\": \"ss\" } } 宽松的修改 text // id 1002 的如果不存在，不会报错, 会新增一个文档 POST vdong/_update/1002 { \"doc\": { \"name\": \"ss\" }, \"doc_as_upsert\": true } 查询并修改 text // 查询并修改 POST vdong/_update_by_query { \"query\":{ \"match\": { \"name\": \"ccc\" } }, \"script\":{ \"source\": \"ctx._source.age=params.age;ctx._source.sex=params.sex\", \"lang\": \"painless\", \"params\": { \"age\":77, \"sex\":\"女\" } } } // 处理中文字段 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"ctx._source[\\\"年纪\\\"]=params[\\\"年纪\\\"]\", \"lang\": \"painless\", \"params\": { \"年纪\":6 } } } // ctx['_op'] 来删除文档 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"\"\" if(ctx._source[\"年纪\"]\u003c34){ ctx.op = 'delete' } \"\"\" } } ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:3","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#修改"},{"categories":["技术"],"content":" 修改 text // 需要完整资源信息 PUT vdong/_doc/12 { \"name\": \"栋\", \"age\": 23, \"sex\":\"男\" } // 不需要完整资源 POST vdong/_update/12 { \"doc\": { \"name\": \"ss\" } } 宽松的修改 text // id 1002 的如果不存在，不会报错, 会新增一个文档 POST vdong/_update/1002 { \"doc\": { \"name\": \"ss\" }, \"doc_as_upsert\": true } 查询并修改 text // 查询并修改 POST vdong/_update_by_query { \"query\":{ \"match\": { \"name\": \"ccc\" } }, \"script\":{ \"source\": \"ctx._source.age=params.age;ctx._source.sex=params.sex\", \"lang\": \"painless\", \"params\": { \"age\":77, \"sex\":\"女\" } } } // 处理中文字段 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"ctx._source[\\\"年纪\\\"]=params[\\\"年纪\\\"]\", \"lang\": \"painless\", \"params\": { \"年纪\":6 } } } // ctx['_op'] 来删除文档 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"\"\" if(ctx._source[\"年纪\"]\u003c34){ ctx.op = 'delete' } \"\"\" } } ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:3","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#宽松的修改"},{"categories":["技术"],"content":" 修改 text // 需要完整资源信息 PUT vdong/_doc/12 { \"name\": \"栋\", \"age\": 23, \"sex\":\"男\" } // 不需要完整资源 POST vdong/_update/12 { \"doc\": { \"name\": \"ss\" } } 宽松的修改 text // id 1002 的如果不存在，不会报错, 会新增一个文档 POST vdong/_update/1002 { \"doc\": { \"name\": \"ss\" }, \"doc_as_upsert\": true } 查询并修改 text // 查询并修改 POST vdong/_update_by_query { \"query\":{ \"match\": { \"name\": \"ccc\" } }, \"script\":{ \"source\": \"ctx._source.age=params.age;ctx._source.sex=params.sex\", \"lang\": \"painless\", \"params\": { \"age\":77, \"sex\":\"女\" } } } // 处理中文字段 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"ctx._source[\\\"年纪\\\"]=params[\\\"年纪\\\"]\", \"lang\": \"painless\", \"params\": { \"年纪\":6 } } } // ctx['_op'] 来删除文档 POST vdong_cn/_update_by_query { \"query\":{ \"match\": { \"姓名\": \"ss\" } }, \"script\":{ \"source\": \"\"\" if(ctx._source[\"年纪\"]\u003c34){ ctx.op = 'delete' } \"\"\" } } ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:3","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#查询并修改"},{"categories":["技术"],"content":" 删除 text // 删除文档 DELETE vdong/_doc/1002 // 删除索引 DELETE vdong // 根据搜索删除 POST vdong/_delete_by_query { \"query\": { \"term\": { \"realname\": { \"value\": \"dong\" } } } } ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:4","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#删除"},{"categories":["技术"],"content":" 批量操作 text // 创建索引 index 总会成功，如果 _id 已经存在，则更新数据 POST _bulk { \"index\" : { \"_index\" : \"vdong\", \"_id\" : \"1\" } } { \"name\" : \"小明\", \"sex\" : \"男\", \"age\" : \"18\" } { \"index\" : { \"_index\" : \"vdong\", \"_id\" : \"2\" } } { \"name\" : \"小红\", \"sex\" : \"女\", \"age\" : \"20\" } // 创建索引 create ，如果 _id 存在，该条不会成功 POST _bulk { \"create\" : { \"_index\" : \"vdong\", \"_id\" : \"2\" } } { \"name\" : \"小明\", \"sex\" : \"男\", \"age\" : \"18\" } { \"create\" : { \"_index\" : \"vdong\", \"_id\" : \"20\" } } { \"name\" : \"小红\", \"sex\" : \"女\", \"age\" : \"20\" } // 删除 { \"delete\" : { \"_index\" : \"vdong\", \"_id\" : \"2\" } } // 更新 { \"update\" : {\"_id\" : \"1\", \"_index\" : \"vdong\"} } { \"doc\" : {\"field2\" : \"value2\"} } 批量相关文档 ","date":"2021-11-04","objectID":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/:2:5","series":null,"tags":["es"],"title":"Es 索引相关","uri":"/posts/elastic/es%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3/#批量操作"},{"categories":["技术"],"content":" 集群（cluster）由一个名称标识（ elisticsearch.yml 中 cluster.name: “docker-cluster” ），由一个或多个节点（node）组成。 bash # 查看集群状态 GET _cluster/state ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:1:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#集群cluster"},{"categories":["技术"],"content":" 节点 （node）默认情况下，群集中的每个节点都可以处理 HTTP 和 Transport 流量。 Transport 层专门用于节点之间的通信。 HTTP 层由 REST 客户端使用。 所有节点都知道集群中的所有其他节点，并且可以将客户端请求转发到适当的节点。 根据作用分类： master-eligible node：主节点，管理集群，去中心化，避免单点故障，自动选举 data node ： 数据节点，保存和操作数据，如CRUD、搜索和聚合 ingest node ：数据接入节点。 角色为 ingest。 remote-eligible node ：远程合格节点。角色为 remote_cluster_client ，可以充当远程客户端。 Machine learning node ： 机器学习节点。 transform node ：转换节点。 Coordnating node 协调节点 搜索请求或批量索引请求等请求可能涉及保存在不同数据节点上的数据。 例如，搜索请求在两个阶段中执行，这两个阶段由接收客户请求的节点（即协调节点）协调。 在分散阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行该请求，并将其结果返回到协调节点。 在收集阶段，协调节点将每个数据节点的结果缩减为单个全局结果集。 每个节点都隐式地是一个协调节点。 这意味着通过 node.roles 具有明确的空角色列表的节点将仅充当协调节点，无法禁用。 结果，这样的节点需要具有足够的内存和 CPU 才能处理收集阶段。协调节点的定义为： node.roles: [] 角色： master data data_content data_hot data_warm data_cold data_frozen ingest ml remote_cluster_client transform ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:2:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#节点-node"},{"categories":["技术"],"content":" 索引（index）对应关系型数据库里的 databse 概念（但不完全相同）。一个集群，可以有多个索引。是一个逻辑名称。 索引是文档（对应关系数据库的 record ）的集合。 ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:3:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#索引index"},{"categories":["技术"],"content":" 分片 （shard）分布式的搜索引擎，索引通常被拆分为多份，这份就叫分片（shard），分布在多个节点上。es 自动管理和平衡这些分片。 因此一个索引可以存储，超过单节点硬件限制的数据。 优点： 方便水平拓展 多节点并行操作（潜在地），提高吞吐 分片的类型： primary shard ：每个文档都有一个 primary shard 。 索引文档时，它首先在 Primary shard 上编制索引，然后在此分片的所有副本上（replica）编制索引。索引可以包含一个或多个主分片。 此数字确定索引相对于索引数据大小的可伸缩性。 创建索引后，无法更改索引中的主分片数。 replica shard：每个主分片可以具有零个或多个副本分片。数量可以动态修改（默认1个）。有两个目的： 1.增加故障转移：如果主要故障，可以将副本分片提升为主分片。即使你失去了一个 node，那么副本分片还是拥有所有的数据。 2.提高性能：get 和 search 请求可以由主 shard 或副本 shard 处理。 主分片 和 副本分片 的区别：只有主分片可以接受索引请求。副本分片和主分片都可以提供查询请求。 shard 健康状态 红色：集群中至少有一个主分片未分配 黄色：已分配所有主分片，但至少一个副分片未分配 绿色：分配所有分片 ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:4:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#分片-shard"},{"categories":["技术"],"content":" 文档 （document）es 索引或搜索的最小数据单元是文档，对应关系数据库的 record。 特点： 独立 可以嵌套的 schemaless 不需要预先定义模式，可以动态调整 文档通常是数据的 JSON 表示形式。JSON over HTTP 是与 Elasticsearch 进行通信的最广泛使用的方式。 ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:5:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#文档-document"},{"categories":["技术"],"content":" 相互关系每个 Index 由一个或许多的 documents 组成，并且这些 documents 可以分布于不同的 shard 之中，一个 node 里可以放零个或多个 shard （取决与 node 的种类），一个或多个节点（node）组成集群。 如：一个 index 有5个 shard 及1个 replica 这些 shard 分布于不同的物理机器上 ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/:6:0","series":null,"tags":["es"],"title":"Es 基本概念","uri":"/posts/elastic/es%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/#相互关系"},{"categories":["技术"],"content":" Elasticsearch text # 安装 docker pull elasticsearch:7.14.0 shell docker network create elk_net # 指定内存运行 docker run -d -e ES_JAVA_OPTS=\"-Xms256m -Xmx256m\" -h elasticsearch --name elasticsearch --net elk_net -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.14.0 # 访问 http://localhost:9200 or http://host-ip:9200 head 插件（可选） shell docker pull mobz/elasticsearch-head:5 docker run -d -p 9100:9100 docker.io/mobz/elasticsearch-head:5 # 访问 http://localhost:9100 or http://host-ip:9100 # 跨域问题 # 进入 elasticsearch 容器内部，修改配置文件 ./config/elasticsearch.yml # 增加 http.cors.enabled: true http.cors.allow-origin: \"*\" # 重启 docker restart elasticsearch # ElasticSearch-head 查询报 406错误码 # {\"error\":\"Content-Type header [application/x-www-form-urlencoded] is not supported\",\"status\":406} # _site 目录下 sed -i '6886c contentType: \"application/json;charset=UTF-8\",' vendor.js sed -i '7573c var inspectData = s.contentType === \"application/json;charset=UTF-8\" \u0026\u0026' vendor.js ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%AE%89%E8%A3%85/:1:0","series":null,"tags":["es"],"title":"Es 安装","uri":"/posts/elastic/es%E5%AE%89%E8%A3%85/#elasticsearch"},{"categories":["技术"],"content":" Kibana shell docker pull kibana:7.14.0 docker run -d -h kibana --name kibana --net elk_net -p 5601:5601 kibana:7.14.0 # 访问 http://localhost:5601 or http://host-ip:5601 ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%AE%89%E8%A3%85/:2:0","series":null,"tags":["es"],"title":"Es 安装","uri":"/posts/elastic/es%E5%AE%89%E8%A3%85/#kibana"},{"categories":["技术"],"content":" Logstash shell docker pull logstash:7.14.0 1. 编辑文件 logstash.conf 文件 docker run -h logstash --name logstash --network elk_net -it --rm -v /usr/local/logstash/config/pipeline:/usr/local/logstash/config/pipeline logstash:7.14.0 -f /usr/local/logstash/config/pipeline/logstash.conf ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%AE%89%E8%A3%85/:3:0","series":null,"tags":["es"],"title":"Es 安装","uri":"/posts/elastic/es%E5%AE%89%E8%A3%85/#logstash"},{"categories":["技术"],"content":" 网络问题 shell docker network create elk_net // 将运行中的 容器 containerName 连接到网络 networkName 中 docker network connect networkName containerName // 将容器移除网络 docker network disconnect networkName containerName ","date":"2021-11-04","objectID":"/posts/elastic/es%E5%AE%89%E8%A3%85/:4:0","series":null,"tags":["es"],"title":"Es 安装","uri":"/posts/elastic/es%E5%AE%89%E8%A3%85/#网络问题"},{"categories":["技术"],"content":"这是一个静态 bolg 网站。 记录搭建过程。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:0:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#"},{"categories":["技术"],"content":" 技术栈：hugo , git , github 。 hugo：是一个开源的 web 框架，使用 go 语言开发，可以将 markdown 文件快速的建构成静态网站。 git: 作为版本管理工具。 github: 作为代码仓库，使用 github 的 actions 做自动的建构和部署，分别部署在 github pages 和 阿里云上。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:1:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#技术栈"},{"categories":["技术"],"content":" 搭建过程","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#搭建过程"},{"categories":["技术"],"content":" 本地安装 hugo不同操作系统安装差异略有不同，具体请参考：安装文档 本人使用如下命令安装 shell # Hugo extended 支持 Sass/SCSS scoop install hugo-extended ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:1","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#本地安装-hugo"},{"categories":["技术"],"content":" 创建站点，测试参考快速开始创建站点，并测试。 本人选择的主题是 Doit，并作简单配置（修改 config.toml 文件）即可，先跑起来。 至此博客已经搭建完成，你可以添加内容，在本地浏览博客，生成静态文件（默认 public/ 目录下），上传 public/ 目录的下的内容到服务器，就是一个静态的blog 网站。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:2","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#创建站点测试"},{"categories":["技术"],"content":" 添加版本管理本地：进入站点目录，git init 将该目录变成一个仓库；之后正常做版本管理。 github线上: 创建仓库，推荐命名 USER_NAME.github.io （之后设置 github pages 时，网址没有路径）。推送站点仓库到 github 。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:3","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#添加版本管理"},{"categories":["技术"],"content":" 配置持续集成使用 github actions ，自动建构 hugo 并发布到 github pages 和 第三方服务器。 github actions ： 是一些持续集成的操作，包括但不局限于：抓取代码、运行测试、登录远程服务器，发布到第三方服务等。github 允许开发者，把每个操作协程独立的脚本，存放到代码仓库，供他人调用。官方市场，awesome-actions github pages ：是一个静态网址托管服务，github 允许你为你的每一个仓库，制作一个静态网页。 配置 github actions你可以在 actions 选项卡内新建 workflow ；也可以在项目根目录手动创建.github\\workflows\\XXX.yml 然后提交， action 的配置文件叫做 workflow 文件。 以 .yml 为后缀。 yml name: Hugo build and deploy on: # 当 main 分支被 push 时，此 workflow 被触发 push: branches: [ main ] # 允许你在 actions 选项卡中手动执行此 workflow workflow_dispatch: jobs: Hugo-build-deploy: runs-on: ubuntu-latest # 基于 ubuntu 最新版 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v2 # 检出代码 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo # 安装 hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build # 建构 hugo run: hugo --minify - name: Deploy Github Pages # 部署到 github pages uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public - name: 📂 Sync files # ftp 同步到 第三方服务器，如无第三方服务器可以省略此步。 uses: SamKirkland/FTP-Deploy-Action@4.1.0 with: server: ${{ secrets.FTP_URL }} username: ${{ secrets.FTP_USER }} password: ${{ secrets.FTP_PWD }} local-dir: ./public/ ps: 敏感字段需要在 Settings/Secrets 选项卡中设置，如上的 ${{ secrets.FTP_PWD }} 等。 ps：可以在 actions 选项卡中查看 action 运行状态，并调试错误。 配置 github pages当以上 actions 成功运行一次后（push 操作后，或 手动运行），你会发现多了一个 gh-pages 分支。 进入选项卡 Settings/Pages 配置 source ，branch 为 gh-pages，目录为 /(root)。 成功后会提示你 Your site is published at https://sunvdong.github.io/ （如果你的仓库名是 你的用户名.github.io，则网址后没有路径）。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:4","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#配置持续集成"},{"categories":["技术"],"content":" 配置持续集成使用 github actions ，自动建构 hugo 并发布到 github pages 和 第三方服务器。 github actions ： 是一些持续集成的操作，包括但不局限于：抓取代码、运行测试、登录远程服务器，发布到第三方服务等。github 允许开发者，把每个操作协程独立的脚本，存放到代码仓库，供他人调用。官方市场，awesome-actions github pages ：是一个静态网址托管服务，github 允许你为你的每一个仓库，制作一个静态网页。 配置 github actions你可以在 actions 选项卡内新建 workflow ；也可以在项目根目录手动创建.github\\workflows\\XXX.yml 然后提交， action 的配置文件叫做 workflow 文件。 以 .yml 为后缀。 yml name: Hugo build and deploy on: # 当 main 分支被 push 时，此 workflow 被触发 push: branches: [ main ] # 允许你在 actions 选项卡中手动执行此 workflow workflow_dispatch: jobs: Hugo-build-deploy: runs-on: ubuntu-latest # 基于 ubuntu 最新版 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v2 # 检出代码 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo # 安装 hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build # 建构 hugo run: hugo --minify - name: Deploy Github Pages # 部署到 github pages uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public - name: 📂 Sync files # ftp 同步到 第三方服务器，如无第三方服务器可以省略此步。 uses: SamKirkland/FTP-Deploy-Action@4.1.0 with: server: ${{ secrets.FTP_URL }} username: ${{ secrets.FTP_USER }} password: ${{ secrets.FTP_PWD }} local-dir: ./public/ ps: 敏感字段需要在 Settings/Secrets 选项卡中设置，如上的 ${{ secrets.FTP_PWD }} 等。 ps：可以在 actions 选项卡中查看 action 运行状态，并调试错误。 配置 github pages当以上 actions 成功运行一次后（push 操作后，或 手动运行），你会发现多了一个 gh-pages 分支。 进入选项卡 Settings/Pages 配置 source ，branch 为 gh-pages，目录为 /(root)。 成功后会提示你 Your site is published at https://sunvdong.github.io/ （如果你的仓库名是 你的用户名.github.io，则网址后没有路径）。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:4","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#配置-github-actions"},{"categories":["技术"],"content":" 配置持续集成使用 github actions ，自动建构 hugo 并发布到 github pages 和 第三方服务器。 github actions ： 是一些持续集成的操作，包括但不局限于：抓取代码、运行测试、登录远程服务器，发布到第三方服务等。github 允许开发者，把每个操作协程独立的脚本，存放到代码仓库，供他人调用。官方市场，awesome-actions github pages ：是一个静态网址托管服务，github 允许你为你的每一个仓库，制作一个静态网页。 配置 github actions你可以在 actions 选项卡内新建 workflow ；也可以在项目根目录手动创建.github\\workflows\\XXX.yml 然后提交， action 的配置文件叫做 workflow 文件。 以 .yml 为后缀。 yml name: Hugo build and deploy on: # 当 main 分支被 push 时，此 workflow 被触发 push: branches: [ main ] # 允许你在 actions 选项卡中手动执行此 workflow workflow_dispatch: jobs: Hugo-build-deploy: runs-on: ubuntu-latest # 基于 ubuntu 最新版 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - uses: actions/checkout@v2 # 检出代码 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo # 安装 hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build # 建构 hugo run: hugo --minify - name: Deploy Github Pages # 部署到 github pages uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == 'refs/heads/main' }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public - name: 📂 Sync files # ftp 同步到 第三方服务器，如无第三方服务器可以省略此步。 uses: SamKirkland/FTP-Deploy-Action@4.1.0 with: server: ${{ secrets.FTP_URL }} username: ${{ secrets.FTP_USER }} password: ${{ secrets.FTP_PWD }} local-dir: ./public/ ps: 敏感字段需要在 Settings/Secrets 选项卡中设置，如上的 ${{ secrets.FTP_PWD }} 等。 ps：可以在 actions 选项卡中查看 action 运行状态，并调试错误。 配置 github pages当以上 actions 成功运行一次后（push 操作后，或 手动运行），你会发现多了一个 gh-pages 分支。 进入选项卡 Settings/Pages 配置 source ，branch 为 gh-pages，目录为 /(root)。 成功后会提示你 Your site is published at https://sunvdong.github.io/ （如果你的仓库名是 你的用户名.github.io，则网址后没有路径）。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:4","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#配置-github-pages"},{"categories":["技术"],"content":" 增加评论功能参考官方文档：Waline 快速上手 LeanCloud 设置 (准备数据库) 注册 LeanCloud 国际版 （用于存储数据） 在 LeanCloud 创建应用 记录 id ，密钥等。 Vercel 部署 (服务端) github 登录 Vercel 新建项目，初始化仓库 设置环境变量，步骤1（第3小步）中记录的id等 重新部署，使环境变量生效 配置域名（默认域名 xxxxxx.vercel.app 国内不能访问） 域名服务器商处添加新的 CNAME 解析记录 修改 hugo 配置文件 config.toml： toml [params.page] # 评论系统设置 [params.page.comment] enable = true # Waline 评论系统设置 (https://waline.js.org) [params.page.comment.waline] enable = true serverURL = \"https://comment.vdong.xyz/\" # 此处为上边配置的域名 pageview = true comment = true emoji = ['https://i.whaoot.com/emojis/qq/'] # emoji 使用了个人七牛云存储 # emoji = ['https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo'] # meta = ['nick', 'mail', 'link'] # requiredMeta = [] # login = 'enable' # wordLimit = 0 # pageSize = 10 # imageUploader = false # highlighter = false # texRenderer = false ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:5","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#增加评论功能"},{"categories":["技术"],"content":" 增加搜索功能DoIt 支持多种搜索方式，这里使用 Fuse.js ,只需要修改 hugo 的配置文教 config.toml 即可： toml [params.search] enable = true # 搜索引擎的类型 (\"lunr\", \"algolia\", \"fuse\") type = \"fuse\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"搜索文章标题或内容...\" # DoIt 新增 | 0.2.1 最大结果数目 maxResultLength = 10 # DoIt 新增 | 0.2.3 结果内容片段长度 snippetLength = 50 # DoIt 新增 | 0.2.1 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # DoIt 新增 | 0.2.4 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.fuse] # DoIt 新增 | 0.2.12 https://fusejs.io/api/options.html isCaseSensitive = false minMatchCharLength = 2 findAllMatches = false location = 0 threshold = 0.3 distance = 100 ignoreLocation = false useExtendedSearch = false ignoreFieldNorm = false ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:6","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#增加搜索功能"},{"categories":["技术"],"content":" 配置文章修改时间hugo 配置文件中新增如下： :git 从文件的 git 提交记录获取 lastmod 从文件中的 lastmod 字段获取 :fileModTime 从文件修改时间获取 toml enableGitInfo = true # 获取每个文件的git修改信息 [frontmatter] lastmod = [':git', 'lastmod', ':fileModTime', ':default'] ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:2:7","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#配置文章修改时间"},{"categories":["技术"],"content":" 问题记录","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:3:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#问题记录"},{"categories":["技术"],"content":" 子模块未初始化或更新导致，hugo server -D 不能预览网站问题原因是：子模块未初始化，导致模板缺失，从而不能生成 html 文件。 shell # 初始化子模块 git submodule init # 更新子模块 git submodule update --remote # 查看更新状态 git submodule status ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:3:1","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#子模块未初始化或更新导致hugo-server--d-不能预览网站问题"},{"categories":["技术"],"content":" 使用 Typora 编辑 md 文件时的图片设置问题 文件 -\u003e 偏好设置 -\u003e 图像 -\u003e 插入图片时.. 选择 复制到指定路径，并指定路径为，hugo\\static\\imgs\\${filename}\\ 。 格式( Format ) -\u003e 图像( Image ) -\u003e 设置图片根目录( Use Image Root Path )，目录选择到 imgs 的上一级 static 目录，这时 Typora 会增加一个 Markdown 文件顶部的元数据（yml格式） typora-root-url: \"..\\\\..\\\\..\\\\static\"（每个文件都得设置，不推荐）。 直接修改，archetypes/default.md 模板文件，增加元数据 yml格式：typora-root-url: \"..\\\\..\\\\..\\\\static\" 或 toml格式：typora-root-url = \"..\\\\..\\\\..\\\\static\"，之后在新建 md 文件时就自动增加了。 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:3:2","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#使用-typora-编辑-md-文件时的图片设置问题"},{"categories":["技术"],"content":" 成果源码：https://github.com/SunVdong/sunvdong.github.io 博客地址: https://www.vdong.xyz or https://sunvdong.github.io ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:4:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#成果"},{"categories":["技术"],"content":" 参考Hugo官网 GitHub Actions 入门教程 使用 Hugo + Github 搭建个人博客 hugo与typora图片路径统一问题 ","date":"2021-11-03","objectID":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/:5:0","series":null,"tags":["hugo","折腾"],"title":"博客搭建记录","uri":"/posts/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/#参考"},{"categories":["生活"],"content":"Hello world! ","date":"2021-11-01","objectID":"/posts/hello-world/:0:0","series":null,"tags":["日常","折腾"],"title":"Hello world","uri":"/posts/hello-world/#"},{"categories":null,"content":" 你没有连接至互联网, 只有缓存的页面可用. ","date":"0001-01-01","objectID":"/offline/:0:0","series":null,"tags":null,"title":"Offline","uri":"/offline/#"}]