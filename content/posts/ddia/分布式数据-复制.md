---
title: "分布式数据-复制"
date: "2024-04-15T18:00:06+08:00"
draft: true
author: "vdong"
description: "分布式数据主从复制、多主复制、复制延迟等介绍"
categories:
  - "生活"
  - "技术"
tags:
  - "日常"
  - "折腾"
typora-root-url: ..\..\..\static
---

复制的原因：

- 使数据和用户在地理位置上更近（降低延迟）
- 使系统即使一部分故障，也能正常工作（提高可用性）
- 伸缩可接受读请求的机器数量（提高读取吞吐量）

复制的困难：在于处理数据的变更。变更复制算法：**单主（singer leader ，单领导者）**，**多主（multi leader，多领导者）**，**无主（leaderless ，无领导者）**。

复制时需要权衡的问题：1. 同步还是异步？2. 如何处理失败的副本？...

## 领导者和追随者

存储了数据库拷贝的每个节点被称为 **副本（replica）**。

每一次向数据库的写入操作都需要传播到所有副本上，否则副本就会包含不一样的数据。解决方案：基于领导者的复制，主从。原理：

1. 其中一个副本被指定为 **领导者（leader）**，也称为 **主库（master|primary）** 。当客户端要向数据库写入时，它必须将请求发送给该 **领导者**，其会将新数据写入其本地存储。
2. 其他副本被称为 **追随者（followers）**，亦称为 **只读副本（read replicas）**、**从库（slaves）**、**备库（ secondaries）** 或 **热备（hot-standby）**。每当领导者将新数据写入本地存储时，它也会将数据变更发送给所有的追随者，称之为 **复制日志（replication log）** 或 **变更流（change stream）**。每个跟随者从领导者拉取日志，并相应更新其本地数据库副本，方法是按照与领导者相同的处理顺序来进行所有写入。
3. 当客户想要从数据库中读取数据时，它可以向领导者或任一追随者进行查询。但只有领导者才能接受写入操作（从客户端的角度来看从库都是只读的）。

这种复制模式是很多关系型数据库的内置功能，也被用于一些非关系数据库（如：MongoDB、RethinkDB 和 Espresso），甚至它不仅限于数据库：Kafka、RabbitMQ 等分布式消息代理也使用它。某些网络文件系统，如 DRBD 这样的块复制设备也类似。

### 同步复制or异步复制

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-2.png)

Follower1 是同步复制，主库等待从库确认后，才向用户报告写入成功。

Follower2 是异步复制，主库不等待从库的响应。

通常从库的复制是很快的：大多数数据库能在不到1秒中完成同步。但是，这是没有保证的。有些情况从库可能落后主库几分钟，如：从库正在从失败中恢复；系统正在最大负载附近工作；或者节点间的网络问题。

同步复制的优点：副本被保障拥有领导者最新且一致的数据，当领导者挂了，副本可以直接上。缺点：如果从库（crash 或者网络问题等）没有响应，主库就无法处理写入操作。

因此，将所有从库都设置为同步的是不切实际的：任何一个节点的中断都会导致整个系统停滞不前。实际上，如果在数据库上启用同步复制，通常意味着其中 **一个** 从库是同步的，而其他的从库则是异步的。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库。 这种配置有时也被称为 **半同步（semi-synchronous）**。

异步复制缺点：主库失效且不可恢复，未复制给从库的写入将丢失。优点：即使所有从库都落后了，主库也可以继续处理写入。弱的持久化。

### 设置新从库

数据不断变化，简单复制是不行的（复制过程中也可能有数据变化），如果先锁定数据库再复制，违背了高可用性的目标，所以复制过程通常如下：

1. 在某个时刻获取主库一致性快照。如：mysql 的 innobackupex 和 XtraBackup。
2. 将快照复制到新的从库节点。
3. 从库连接主库，拉取快照之后发生的数据变更。前提要求是快照和主库复制日志中位置精确对应。这个位置有不同的名称，如 PostgreSQL 中叫 **日志序列号（log sequence nubmer，LSN）**， MySQL 中叫 **二进制日志坐标（binlog coordinates）**。
4. 当从库处理完快照之后积累的数据变更，从库 **赶上（caught up）**了主库。

### 处理节点宕机

任何节点都可能宕机，例如意外故障，计划内的维护。我们的目标是即使个别节点失效，也能保持整个系统的运行，尽可能控制停机带来的影响。基于领导者的复制是怎么做到高可用的呢？

#### 从库失效：追赶恢复

在从库的本地磁盘中，每个从库都保存着它从主库收到的数据变更日志。如果一个从库崩溃并重启，或者主库和从库的网络临时中断，从库可以很容易的恢复：它从日志中可以知道故障发生前，已经处理的最后一个事务。因此，从库连上主库，请求从库断开期间的所有数据变更。

#### 主库失效：故障切换

主库失效处理起来非常棘手： 其中一个从库会被提升为主库，需要重新配置客户端，以将其写操作发送给新的主库，其余从库需要开始拉取新主库的数据变更。这个过程被称为 **故障切换（failover）**。

故障切换可以手动执行，也可以自动执行。自动执行的步骤通常如下：

1. 确认主库失效。通常采用超时来确认失效，节点频繁的互相来回传递消息。如果一个节点一段时间（如30s）内没有响应，则认为它挂了。
2. 选择一个新的主库。这可以通过选举过程（主库由剩余副本以多数选举产生）来完成，或者可以由选定的 **控制器节点（controller node）** 来指定新的主库。主库的最佳候选人通常是拥有旧主库最新数据副本的从库（以最小化数据损失）。涉及 共识 问题。
3. 重新配置系统以启用新的从库。客户端现在需要将它们的写入请求发送给新的主库（请求路由）。如果旧主库恢复，可能仍然认为自己是主库，而没有意识到其他副本眼看让他失去来领导权。系统需要确保旧的主库，意识到新主库的存在，并成为一个从库。

故障切换有很多地方容易出错：

1.  如果使用异步复制，则新主库可能没有收到老主库宕机前的写入操作。选出新主库后，如果老的主库重新加入集群，新主库在此期间可能收到冲突的写入，这些写入该怎么处理呢？常见的解决方案是，简单丢弃老主库未复制的写入，这可能打破客户对数据持久化的期望。
2. 如果数据库需要和其他外部存储相协调，丢弃写入内容是极其危险的操作。例如在 GitHub 的一场事故中，一个过时的 MySQL 从库被提升为主库。数据库使用自增 ID 作为主键，因为新主库的计数器落后于老主库的计数器，所以新主库重新分配了一些已经被老主库分配掉的 ID 作为主键。这些主键也在 Redis 中使用，主键重用使得 MySQL 和 Redis 中的数据产生不一致，最后导致一些私有数据泄漏到错误的用户手中。
3. 发生某些故障时，可能两个节点都认为自己是主库，称为 **脑裂（split brain）**： 两个主库都可以接受写操作，却没有冲突解决机制，那么数据就会丢失或损坏，某些系统采取来安全防范措施： 当检测到两个主库节点同时存在时，会关闭其中一个节点（ 这种机制叫 **屏障 fencing**，或者充满感情的术语是 ： **爆彼之头 Shoot The Other Node In The Head， STONITH** ），但是粗糙的设计可能导致两个节点都被关闭。
4. 主库被宣告死亡的超时应该怎么设置呢？主库失效的情况下，超时时间越长，则意味着恢复的时间也越长。如果超时时间设置的太短，则可能会出现不必要的切换。例如，临时的负载峰值可能导致节点的响应时间增加到超出超时时间，或者网络故障也可能导致数据包延迟。如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕。

这些问题没有简单的解决方案。因此，即使软件支持自动故障切换，不少运维团队还是更愿意手动执行故障切换。

节点故障、不可靠的网络、对副本一致性、持久性、可用性和延迟的权衡，这些问题实际上是分布式系统中的基本问题。

### 复制日志的实现

#### 基于语句的复制

最简单的场景下，主库会记录下它执行的每个写请求（**语句**，statement），并将该语句日志发送给从库。对关系型数据库而言，每个 INSERT，UPDATE，DELETE 语句都被发送到从库，从库解析并执行这些语句，就像他是从客户端收到的一样。

这种方式听起来很合理，但是还是有一些场景，可以导致这种复制方式出现问题。

1. 语句中包含不确定的函数，如 `NOW()` 、 `RAND()` 等，每个副本都会产生不同的值。
2. 如果一个语句使用自增序列，或者依赖数据库中存在的数据（如 UPDATE ... WHERE <some condition>）, 在每个副本中，这些语句必须按照相同的顺序执行，否则就可能产生不同的效果。这在有多个并发事务时，会是一种限制。
3. 有副作用的语句（如： trigger，存储过程，用户定义的函数 ）可能在每个副本上产生不同的副作用，除非副作用是确定的。

当然这些问题可以绕开，例如，当主库语句日志记录时，替换不确定的函数调用 为 函数调用的返回值，这样每个从库将得到相同的值。但是，由于边界情况太多了，通常选择其他的复制方法。

#### 预写入日志传输 （Write-ahead log）

数据的存储和检索中，存储引擎的写操作通常都是追加到日志中：

- 对于日志结构的存储引擎（SSTable 和 LSM tree），日志是主要的存储位置，日志段在后台被压实和垃圾回收。
- 对于覆写单个磁盘块的 B-tree ，每次修改都会先写入  **预写式日志（Write Ahead Log, WAL）** ，以便崩溃后索引可以恢复到一个一致的状态。

在任一情况下，日志都是一个只追加的字节序列，包含了对数据库的所有写操作。我们可以使用完全相同的日志来在另一个节点上构建一个副本：主库除了将日志写入磁盘外，还将其通过网络发送给其从库。

追随者处理这些日志，可以构建一个与主库一模一样的数据结构拷贝。

这种复制方法在 PostgreSQL 和 Oracle 等产品中被使用到，缺点，日志记录数据非常底层： WAL 包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。如果数据库将其存储格式从一个版本更改为另一个版本，通常不可能在主库和从库上运行不同版本的数据库软件。

看上去这可能只是一个小的实现细节，但却可能产生巨大的操作影响。如果复制协议允许从库使用比主库更新的软件版本，则可以先升级从库，然后执行故障切换，使升级后的节点之一成为新的主库，从而允许数据库软件的零停机升级。如果复制协议不允许版本不匹配（传输 WAL 经常出现这种情况），则此类升级需要停机。

#### 逻辑日志复制（基于行）

复制时使用和存储引擎不一样的格式，这样可以是复制日志从存储引擎中解耦出来。这种类型的复制日志被称为 **逻辑日志**。

关系型数据库的逻辑日志通常是以行为颗粒度描述对数据库表的写入记录序列：

- 对于插入的行，日志包含所有新值。
- 对于删除的行，日志包含足够的信息来唯一标识被删除的行。通常是主键，但是如果表上没有主键，则需要记录所有列的旧值。
- 对于更新的行，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少所有已更改的列的新值）。

修改多行的事务会生成多个这样的日志记录，后边跟着一条记录，指出事务已经提交。MySQL的二进制日志（当配置基于行的复制时）使用这种方法。

由于逻辑日志和存储引擎内部分离，因此可以更容易的保持向后兼容，从而是领导者和跟随者可以运行不同的数据库软件版本，甚至使用不同送存储引擎。

对于外部应用程序来说，逻辑日志格式也很容易解析。如果要将数据库的内容发送至外部系统（如用于离线分析或建立自定义索引和缓存的数据仓库），这种特性将很有用。这种技术称为 **数据变更捕获**。

#### 基于触发器的复制

以上复制都是数据库系统实现的，不涉及应用代码，但是有时，你可能希望更灵活，比如你希望复制一个数据子集，或者从一种数据库复制到另一种数据库，或者你需要自定义冲突解决的逻辑，则可能需要将复制移动到应用程序层。

一些工具，如 Oracle Golden Gate 可以通过读取数据库日志，使得应用程序可以使用数据。 另一种方法是使用关系数据库自带的功能： 触发器和存储过程。

触发器允许您注册在数据库系统中发生数据更改（写入事务）时自动执行的自定义应用程序代码。触发器有机会将更改记录到一个单独的表中，使用外部程序读取这个表，再加上任何业务逻辑处理，会后将数据变更复制到另一个系统去。例如，Databus for Oracle 和 Bucardo for Postgres 就是这样工作的。

基于触发器的复制通常比其他复制方法具有更高的开销，并且比数据库的内置复制更容易出错，也有很多限制。然而由于其灵活性，仍然是很有用的。


## 复制延迟问题

对于读多写少的场景，读伸缩（read-scaling）的体系结构，通常使用异步复制，将读请求分散到从库上，减少主库的负载。

不幸的是，当应用程序从异步从库读取时，如果从库落后，它可能会看到过时的信息。这会导致数据库中出现明显的不一致：同时对主库和从库执行相同的查询，可能得到不同的结果，因为并非所有的写入都反映在从库中。这种不一致只是一个暂时的状态 —— 如果停止写入数据库并等待一段时间，从库最终会赶上并与主库保持一致。出于这个原因，这种效应被称为 最终一致性（eventual consistency）。

最终一致性的 “最终” 是模糊化的，复制延迟（replication lag），反应了写入到主库和在从库中反应之间的延迟，可能仅仅是几分之一秒，在实践中并不显眼。但如果系统在接近极限的情况下运行，或网络中存在问题时，延迟可以轻而易举地超过几秒，甚至达到几分钟。

复制延迟可能造成一些问题

### 读己之写

如下：用户在写入后马上就查看数据，则新数据可能尚未到达副本。对用户而言，看起来好像是刚提交的数据丢失了。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-3.png)

这种情况下，需要 **写后读一致性（read-after-write consistency）** ，也叫 **读己之写一致性（read）**。如果用户重新加载页面，他们总会看到他们自己提交的任何更新。它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到。它保证用户自己的输入已被正确保存。

基于领导者的复制怎么实现写后读一致性呢：

- 对于用户 **可能修改过** 的内容，总是从主库读取；这要求，需要有办法不通过实际查询就可以知道用户是否修改了某些东西。例如：社交网络的个人信息只能本人维护。因此，总是从主库读取用户自己的个人信息，如果读取其他用户的就去从库。
- 如果应用中的大部分内容都可能被用户自己编辑，那上边的方法就失效了，因为大部分内容必须从主库读取（读伸缩就没有效果了）。这种情况下需要别的标准来决定是否从主库读取。如，跟踪上次更新时间，在上次更新时间一分钟以内，从主库读。还可以监控从库的复制延迟，防止向延迟超过一分钟的从库发出查询。
- 客户端记录最近一次写入的时间戳，系统需要确保从库在处理该用户读取请求时，该时间戳前的变更已经被应用到了该从库中。如果当前从库不够新，在可以从另一个从库中获取，或者等待主库赶上来。这里的时间戳可以是逻辑时间戳（表示写入顺序的东西，如日志序列号），也可以是实际系统时间（这时，时钟同步变得至关重要）。
- 如果你的副本分布在多个数据中心（为了地理上接近用户或者高可用目的），还会有额外的复杂性：任何需要主库提供服务的请求，都需要路由到包含该主库的数据中心。

另一种复杂的情况，同一个用户从多个设备（如，桌面浏览器和mobile app）上请求服务。这时需要跨设备的写后一致性：如果用户在一个设备上输入了一些信息，然后在另一个设备上查看，则应该看到他们刚输入的信息。

这种情况下，需要考虑一些问题：

- 记住用户的更新时间戳变得困难，因为一个设备上运行的程序不知道另一个设备上发生了什么，需要对这些元数据进行中心化存储。
- 如果副本分布在不同的数据中心，很难保证来自不同设备的的连接路由到同一个数据中心。（如，台式计算机使用的是家庭宽带，而移动设备使用的是蜂窝数据，设备的网络路由可能完全不同）。如果你的请求需要读取主库，可能首先需要把来自该用户所有设备的请求都路由到同一个数据中心。

### 单调读

从从库中异步读取时，可能发生的第二种异常情况是，用户可能遇到时光倒流（moving backward in time）。

如果用户从不同从库进行多次读取，就可能发生这种情况。如下：

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-4.png)

用户首先从新副本读取，然后从旧副本读取。时间看上去回退了。为了防止这种异常，我们需要单调的读取。

**单调读（monotonic reads）**可以保证这种异常不会发生，这个是比 **强一致性（strong consistency）** 更弱，比 **最终一致性（eventual consistency）** 更强的保证。你可能读取到旧值；单调读意味着，如果一个用户按顺序进行多次读取，不会出现时间倒流的情况，也就是说，如果已经读到了数据，后续再读到的数据都是比之前的读到的数据更新。

单调读的一种实现方式是确保某个用户总是从同一个副本读取（不同的用户可以从不同的副本）。例如，基于用户 ID 散列来选择副本，而不是随机选择副本。但，如果该副本出现故障，用户的查询需要重新路由到另一个副本。

### 一致前缀读

第三种复制延迟异常的例子是违反了因果律。

现在，想象第三个人正在通过从库来听一个对话。 Cake 夫人回答的内容是从一个延迟很低的从库读取的，但 Poons 先生所问的问题，从库的延迟要大的多（如下图）。于是，这个观察者会先听到问题的答案，然后才听到问题。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-5.png)

为防止这种异常，需要另一种类型的保证：**一致前缀读（consistent prefix reads）**。 即：如果一系列写入是按某个顺序发生的，那么任何人在读取时，也需要以相同的顺序读取。

这是 **分区（partitioned）** 或 **分片（sharded）** 数据库中的一个特殊问题。如果数据库总是以相同的顺序应用写入，而读取总是看到一致的前缀，那么这种异常不会发生。但是在许多分布式数据库中，不同的分区独立运行，因此不存在 **全局的写入顺序**：当用户从数据库中读取数据时，可能会看到数据库的某些部分处于较旧的状态，而某些则处于较新的状态。

一种解决方案是，确保任何因果相关的写入都写入相同的分区，但在一些应用中可能无法高效地完成这种操作。还有一些显式跟踪因果依赖关系的算法，待续。

### 复制延迟的解决方案

当使用最终一致性系统时，延迟可能有几分钟，甚至几个小时，如果对于业务来说可以接受，那很好，如果不能，在设计系统时就需要设计更强的保证，如 **写后读**。假装复制是同步的，实际上它是异步的，这种做法注定会在将来引发问题。

如上述讨论，在应用层面可以有一些方法提供一些更强的保证，如，从主库进行某些读取。然而在应用层代码中处理这些问题是复杂且易错的。

如果应用开发者不用担心这些微妙的复制问题，而是相信他们的数据库可以 “做正确的事情”，将更好。这就是事务存在的原因。

单节点的事务存在很长时间了。然而，到了分布式（包含复制和分片）数据库，许多系统放弃了他们，声称事务在性能和可用性上代价太高，并在一个可伸缩系统中，最终一致性是不可避免的。这种表述有一定道理，但是过于简单。待续。。。

## 多主复制

单主复制有一个缺点：只有一个主库，所有的写操作都得经过它。如果你由于某些原因不能连接到主库了，那么你就不能向数据库写入了，如网络中断。

一个很自然的拓展是，允许多个节点接受写操作。复制仍向之前一样：每个处理写操作的节点，都必须将数据变化传递给其他节点。我们将其称之为 **多领导者配置**（multi-leader configuration，也称多主、多活复制，即 master-master replication 或 active/active replication）。在这种情况下，每个主库同时是其他主库的从库。

### 多主复制的应用场景

单个数据中心内部使用多主配置，通常没有太大意义，其导致的复杂度超过了其带来的好处。应用场景如下：

#### 运维多个数据中心

如果你有一个数据库，副本分散在多个不同的数据中心（目的，容忍单个数据中心的故障，地理上更接近用户）。如果是单领导者的复制设置，主库必须位于一个数据中心上，所有的写入都经过了该数据中心。

多主配置可以在每个数据中心都有主库。如下图：每个数据中心内使用常规的单主主从复制；在数据中心之间，每个数据中心的主库会将其更改复制到其他数据中心的主库中。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-6.png)

对比单主和多主：

- 性能

  单主，每个写入都必须穿过互联网，进入主库所在的数据中心。增加了写入时间，违背了设置多个数据中心的初衷。

  多主，每个写操作可以在本地的数据中心中进行处理，与其他数据中心异步复制。因此，数据中心之间的网络延迟对用户是透明的，这意味着感觉到的性能可能会更好。

- 容忍数据中心停机

  单主，入职主库所在的数据中心发生故障，故障切换必须使另一个数据中心里的从库称为主库。

  多主，每个数据中心都可以独立于其他数据中心继续运行，并且当发生故障的数据中心归队时，复制会自动追赶。

- 容忍网络问题

  数据中心直线的通信通常穿越公共互联网，这可能不如数据中心的本地网络可靠。单主配置对数据中心之间的连接问题非常敏感，因为通过这个连接进行的写操作是同步的；采用异步复制的功能的多主配置通常可以更好的承受网络问题：临时的网络中断不会妨碍正在处理的写入。

有些数据库默认支持多主复制，有些使用外部工具实现，如 MySQL 的 Tungsten Replicator ，PostgreSQL  的 BDR 和 Oracle 的 GoldenGate 。

多主复制的缺点，两个不同的数据中心可能同时修改相同的数据，写冲突是必须解决的。

由于多主复制在许多数据库中都属于改装的功能，所以常常存在微妙的配置缺陷，且经常与其他数据库功能之间出现意外的反应。比如自增主键、触发器、完整性约束等都可能会有麻烦。因此，多主复制往往被认为是危险的领域，应尽可能避免。

#### 离线操作的客户端

多主复制的另一个适用场景是：应用程序在断网之后仍需要继续工作。

如，手机、笔记本或其他设备上的日历应用。无论设备目前是否联网，你都应该可以随时查看你的会议（读取请求），输入新的会议（写入请求）。如果在离线状态下进行的任何更改，在设备下次上线时，需要与服务器和其他设备同步。

在这种情况下，每个设备都相当于一个充当主库的本地数据库（接受写入请求），并在所有设备的日历副本之前同步，存在异步的多主复制过程。复制的延迟可能是几小时或几天，取决于什么时候连接互联网。

从架构的角度来看，这种设置实际上与数据中心之间的多主复制类似，每个设备都是一个 “数据中心”，而它们之间的网络连接是极度不可靠的。从历史上各类日历同步功能的破烂实现可以看出，想把多主复制用好是多么困难的一件事。

有一些工具旨在使这种多主配置更容易。例如，CouchDB 就是为这种操作模式而设计的。

#### 协同编辑

实时的协作编辑应用程序允许多人同时编辑文档。我们通常不会将协作式编辑视为数据库复制问题，但它与前面提到的离线编辑用例有许多相似之处。当一个用户编辑文档时，所做的更改将立即应用到其本地副本（Web 浏览器或客户端应用程序中的文档状态），并异步复制到服务器和编辑同一文档的任何其他用户。

如果要保证不会发生编辑冲突，则应用程序必须先取得文档的锁定，然后用户才能对其进行编辑。如果另一个用户想要编辑同一个文档，他们首先必须等到第一个用户提交修改并释放锁定。这种协作模式相当于主从复制模型下在主节点上执行事务操作。

但是，为了加速协作，你可能希望将更改的单位设置得非常小（例如单次按键），并避免锁定。这种方法允许多个用户同时进行编辑，但同时也带来了多主复制的所有挑战，包括需要解决冲突。

### 处理写入冲突

多主复制的最大问题是可能发生写入冲突，这意味着需要解决冲突。

如下：

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-7.png)

两个主库同时更新同一记录引起的写入冲突。

#### 同步与异步的冲突检测

在单主数据库中，要么第二个写入被阻塞等待第一个写入完成，要么中止第二个写入事务并强制用户重试。但是在多主配置中，两个写入都是成功的，在稍后的某个时间点，才能异步检测到冲突，这时再要求用户解决冲突可能为时已晚。

原则上，你可以让冲突检测变成同步的——即，等待写入被复制到所有的副本时，再告诉用户写入成功。但是，这样做你将失去多主复制的主要优点：允许每个副本独立的接受写入。如果你想要同步冲突检测，你不妨只使用单主复制。

#### 避免冲突

处理冲突的最简单策略是避免他们：如果应用程序可以确保针对特定记录的写入都流入同一个主节点，那么冲突就不会发生。由于许多多主复制的实现对冲突处理得非常不好，避免冲突是一种推荐的方法。

例如，在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心，并使用该数据中心的主库进行读写。不同的用户可能有不同的 “主” 数据中心（可能根据用户的地理位置选择），但从任何一位用户的角度来看，本质上就是单主配置了。

但是，有时你可能需要更改被指定的主库 —— 可能是因为某个数据中心出现故障，你需要将流量重新路由到另一个数据中心，或者可能是因为用户已经迁移到另一个位置，现在更接近其它的数据中心。在这种情况下，冲突避免将失效，你必须处理不同主库同时写入的可能性。

#### 收敛至一致的状态

单主的数据库按顺序进行写操作：如果对同一字段有多个更新，则最后一个更新将决定字段的最终值。

多主复制中，没有明确的写入顺序。如上图，title 最终是 B 还是 C 是不确定的。

如果每个副本只按照它看到的写入顺序写入，那么数据库最终将处于不一致的状态：最终值将是在主库 1 的 C 和主库 2 的 B。这是不可接受的，每个复制方案都必须确保数据最终在所有副本中都是相同的。因此，数据库必须以一种 **收敛（convergent）** 的方式解决冲突，这意味着所有副本必须在所有变更复制完成时收敛至一个相同的最终值。

实现冲突合并解决有多种途径：

- 给每个写入一个唯一 ID （如：时间戳，长随机数，UUID 或键和值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为 **最后写入胜利（LWW, last write wins）**。虽然这种方法很流行，但是很容易造成数据丢失。
- 为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级。这种方法也意味着数据丢失。
- 以某种方式将这些值合并在一起 - 例如，按字母顺序排序，然后连接它们（在上例中，合并的标题可能类似于 “B/C”）。
- 用一种可保留所有信息的显式数据结构来记录冲突，并编写解决冲突的应用程序代码（也许通过提示用户的方式）。

#### 自定义冲突解决逻辑

冲突解决的最合适的方式可能取决于应用程序，大多数复制工具允许应用程序编写处理冲突解决的逻辑。该代码可以在写入时执行也可以在读取时执行：

- 写时执行

  只要数据库系统检测到复制变更日志中存在冲突，则调用冲突处理程序。例如，Bucardo 允许你为此编写一段 Perl 代码。这个处理程序通常不能提示用户 —— 它在后台进程中运行，并且必须快速执行。

- 读时执行

  当检测到冲突时，所有冲突写入被存储。下一次读取数据时，将这多个版本的数据返回给应用程序。应用程序提示用户解决冲突，并将结果写回数据库。例如 CouchDB 就以这种方式工作。

注意，冲突解决通常适用于单行记录或单个文档层面，而不是整个事务。因此，如果你有一个事务会以原子性地进行几次写入，对于冲突解决而言，每个写入仍然需单独分开考虑。

> #### 冲突的自动解决
>
> 冲突解决规则很容易越来越复杂，自定义代码还可能出错。
>
> 已经有一些有趣的研究，关于自动解决冲突：
>
> - **无冲突复制数据类型（Conflict-free replicated datatypes，CRDT）**是可以由多个用户同时编辑的集合、映射、有序列表、计数器等一系列数据结构，它们以合理的方式自动解决冲突。一些 CRDT 已经在 Riak 2.0 中实现。
> - **可合并的持久数据结构（Mergeable persistent data structures）** 显式跟踪历史记录，类似于 Git 版本控制系统，并使用三向合并功能（而 CRDT 使用双向合并）。
> - **操作转换（operational transformation）**是 Etherpad 和 Google Docs 【31】等协同编辑应用背后的冲突解决算法。它是专为有序列表的并发编辑而设计的，例如构成文本文档的字符列表。

#### 什么是冲突

有些冲突显而易见，如，两个写操作并发地修改了同一条记录中的同一个字段，并将其设置为两个不同的值。毫无疑问这是一个冲突。

也有一些冲突更微妙且难以发现，如，考虑一个会议室预订系统：它记录谁订了哪个时间段的哪个房间。应用程序需要确保每个房间在任意时刻都只能被一组人进行预定（即不得有相同房间的重叠预订）。在这种情况下，如果为同一个房间同时创建两个不同的预订，则可能会发生冲突。即使应用程序在允许用户进行预订之前先检查会议室的可用性，如果两次预订是由两个不同的主库进行的，则仍然可能会有冲突。

### 多主复制拓扑

**复制拓扑** 用来描述写入操作从一个节点传播到另一个节点的通信路径。如有两个以上的主库，则可能存在多种拓扑。如下：

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-8.png)

最常见的拓扑是上图的 c ，全部到全部（all-to-all），每个主库都将写入发送给其他主库。其他拓扑也可能被用到。如，MySQL 仅仅支持 **环形拓扑（Circular topology）**，其中每个节点都从一个节点接受写入，并将这些写入（加上自己的写入）转发至另一个节点。另一种类型的拓扑是星形拓扑（star topilogy）：一个指定的根节点将写入转发至其他节点。星形拓扑可以推广为树。

在星形拓扑和环形拓扑中，写入需要在到达所有副本前通过多个节点。因此，节点需要转发从其他节点收到的数据变更。为了避免无限循环，每个节点被赋予了一个唯一的标识符，并且在复制日志中，每次写入都会使用其经过的所有节点的标识进行标识。当一个节点收到用自己的标识符标记的数据变更时，该数据变更将被忽略，因为节点知道它应经被处理过了。

环形和星形拓扑的问题是，如果只有一个节点发生故障，则可能会中断其他节点之间的复制消息流，导致它们无法通信，除非节点被修复。拓扑结构可以重新配置为跳过发生故障的节点，但在大多数部署中，这种重新配置必须手动完成。更密集连接的拓扑结构（例如，全部到全部拓扑）的容错性更好，因为它允许消息沿着不同的路径传播，可以避免单点故障。

但是，全部到全部的拓扑也可能存在问题。特别是，一些网络链接可能比其他网络链接更快（如网络阻塞），结果是一些消息可能 “超越” 其他消息。如下：

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-9.png)

如上：客户端 A 向主库 1 的表中插入一行，客户端 B 在主库 3 上更新该行。然而，主库 2 可以以不同的顺序接收写入：它可能先接收到更新（从它的角度来看，是对数据库中不存在的行的更新），稍后才接收到相应的插入（其应该在更新之前）。

这是一个因果关系问题，更新取决于先前的插入，所以我们需要保证所有的节点先处理插入，后处理更新。而且仅仅在每次写入时添加一个时间戳是不够的，因为时钟不可能充分的同步，所以主库无法正确的对这些事件进行排序。

要正确的处理这些事件，可以使用一种称为 **版本向量（version vectors）** 的技术。

如果你正在使用基于多主复制的系统，那么你应该多了解这些问题，仔细阅读文档，并彻底测试你的数据库，以确保它确实提供了你想要的保证

## 无主复制

单主、多主的复制逻辑是：客户端向一个节点（主库）发送写请求，数据库系统小心的将这次写操作复制到其他副本节点。主库决定写入的顺序，从库按照相同的顺序应用从库的写入。

一些数据存储系统采用不同的方法，放弃主库的概念，并允许任何副本直接接受来自客户端的写入。Riak，Cassandra 和 Voldemort 是受 Dynamo（ 亚马逊应用于其内部的系统 ） 启发的无主复制模型的开源数据存储，所以这类数据库也被称为 *Dynamo 风格*。

在一些无主复制的实现中，客户端直接将写入发送到几个副本中，而在另一些实现中，由一个 **协调者（coordinator）** 节点代表客户端进行写入的发送。但与主库数据库不同，该协调器不会强制施加特定的写入顺序。

### 当节点故障时写入数据库

假设你有一个带有三个副本的数据库，而其中一个副本目前不可用，或许正在重新启动以安装系统更新。在基于领导者的配置中，如果要继续处理写入，则可能需要执行故障切换。

在无主配置中，不存在故障转移。下图 演示了会发生了什么事情：客户端（用户 1234）并行发送写入到所有三个副本，并且两个可用副本接受写入，但是不可用副本错过了它。假设三个副本中的两个承认写入是足够的：在用户 1234 已经收到两个确定的响应之后，我们认为写入成功。客户简单地忽略了其中一个副本错过了写入的事实。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-10.png)

现在想象一下，不可用的节点重新联机，客户端开始读取它。节点关闭期间发生的任何写入都不在该节点上。因此，如果你从该节点读取数据，则可能会从响应中拿到陈旧的（过时的）值。

为了解决这个问题，当一个客户端从数据库中读取数据时，它不仅仅把它的请求发送到一个副本：读请求将被并行地发送到多个节点。客户可能会从不同的节点获得不同的响应，即来自一个节点的最新值和来自另一个节点的陈旧值。可以使用版本号确定哪个值是新的。

#### 读修复和反熵

复制方案保障了最终将所有数据复制到每个副本中。在一个不可用的节点重新联机后，它如何赶上它错过的写入呢？

在 Dynamo 风格的数据存储中经常使用两种机制：

- 读修复（ Read repair ）

  当客户端并行读取多个节点时，它可以监测到陈旧的响应。如果客户端发现了副本中陈旧的值，就将新值写回到该副本。适用于读频繁的值。

- 反熵过程（ Anti-entropy process）

  此外，一些数据存储系统有一个后台进程，该进程不断的查找副本之间的数据差异，并将任何缺失的数据从一个副本复制到另一个副本。与基于领导者的复制中的复制日志不同，这种反熵过程并不会按照特定顺序复制写入的数据，并且在数据被复制之前可能会有显著的延迟。

并不是所有的系统都实现了这两种机制，例如，Voldemort 目前没有反熵过程。请注意，如果没有反熵过程，很少被读取的值可能会从某些副本中丢失，从而降低了持久性，因为只有在应用程序读取值时才执行读修复。

#### 读写的法定人数

在上边的例子中，我们认为三个副本中的两个被处理，就是写入成功的。如果三个副本中只有一个被处理，我们能接受这个写入吗?

假如一个成功的写操作意味着三个副本中至少保障两个，即最多有一个副本可能是陈旧的。所以，如果我们至少从两个副本中读取，可以保证两个副本中至少有一个是最新的。如果第三个副本停机或响应速度缓慢，则读取仍可以继续返回最新值。

如果有 n 个副本，每个写入必须由 w 个节点确认才能被认为是成功的，并且我们必须至少为每个读取查询 r 个节点。 （在我们的例子中，𝑛=3，𝑤=2，𝑟=2）。只要 𝑤+𝑟>𝑛，我们可以预期在读取时能获得最新的值，因为 r 个读取中至少有一个节点是最新的。遵循这些 r 值和 w 值的读写称为 **法定人数（quorum）**的读和写。你可以认为，r 和 w 是有效读写所需的最低票数。

在 Dynamo 风格的数据库中，参数 n、w 和 r 通常是可配置的。一个常见的选择是使 n 为奇数（通常为 3 或 5）并设置 𝑤=𝑟=(𝑛+1)/2（向上取整）。但是你可以根据需要更改数字。例如，写入次数较少且读取次数较多的工作负载可以设置 𝑤=𝑛w=n 和 𝑟=1。这会使得读取速度更快，但缺点是只要有一个不可用的节点就会导致所有的数据库写入都失败。

法定人数条件 𝑤+𝑟>𝑛 允许系统容忍不可用的节点，如下所示：

- 如果 𝑤<𝑛，当节点不可用时，我们仍然可以处理写入。
- 如果 𝑟<𝑛，当节点不可用时，我们仍然可以处理读取。
- 对于 𝑛=3，𝑤=2，𝑟=2，我们可以容忍一个不可用的节点。
- 对于 𝑛=5，𝑤=3，𝑟=3，我们可以容忍两个不可用的节点，如下图：。
- 通常，读取和写入操作始终并行发送到所有 n 个副本。参数 w 和 r 决定我们等待多少个节点，即在我们认为读或写成功之前，有多少个节点需要报告成功。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-11.png)

**如果 𝑤+𝑟>𝑛，读取 r 个副本，至少有一个副本必然包含了最近的成功写入。**

如果可用的节点少于所需的 w 或 r 节点数，写入或读取将返回错误。节点可能由于多种原因不可用：节点宕机（崩溃、断电），由于执行操作时出错（因为磁盘已满而无法写入），客户端和节点之间的网络中断，或者其他任何原因。我们只关心节点是否返回了成功的响应，而不需要区分不同类型的故障。

### 法定人数一致性的局限性

如果你有 n 个副本，并且你选择了满足 w+r>n 的 w 和 r，你通常可以期望每次读取都能返回最近写入的值。情况就是这样，因为你写入的节点集合和你读取的节点集合必然有重叠。也就是说，你读取的节点中必然至少有一个节点具有最新值。

通常 r 和 w 被选为多数（超过 𝑛/2 ）节点，因为这确保了 𝑤+𝑟>𝑛，同时仍然容忍多达 𝑛/2 个节点故障。但是，法定人数不一定必须是大多数，重要的是读写使用的节点至少有一个节点的交集。其他法定人数的配置是可能的，这使得分布式算法的设计有一定的灵活性。

如果 w+r <= n (即法定条件不满足)。在这种条件下，读取和写入操作仍将被发送到 n 个节点，但操作成功只需要少量的成功响应。

较小的 w 和 r 更有可能读取到陈旧的数据，因为你的读取可能未包含具有最新值的节点。但是，这种配置允许更低的延迟和更高的可用性：如果存在网络中断，并且更多的副本变得无法访问，则有更大的机会可以继续处理读取和写入。只有当可达的副本数量低于 w 或 r 时， 数据库才变得不可读取和写入。

但是即使在 w+r > n 的情况下，也存在返回陈旧值的边缘情况。可能的情况如下：

- 如果使用  宽松的法定人数，w个写入 和 r个读取可能落到不同的节点上，读取节点和写入节点不能保证有重叠节点。
- 如果两个写入同时发生，不清楚哪一个先发生。这中情况下，唯一安全的策略是合并并发写入。如果根据时间戳（最后写入胜利）挑选出一个胜利者，则写入可能由于时钟偏差而丢失。
- 如果写操作和读操作同事发生，写操作可能仅反映在部分副本上。这种情况下，无法确定读取返回的是旧值还是新值。
- 如果一个写入在某些副本上成功但在其他副本上失败（例如，因为某些节点的磁盘已满），并且总体上在少于 w 个副本上成功，那么它不会在成功的副本上回滚。这意味着，如果一个写入被报告为失败，后续的读取可能会也可能不会返回该写入的值。
- 如果携带新值的节点发生故障，则需要从一个带有旧值的节点进行恢复，那么携带新值的节点数目可能小于 w， 法定人数将被打破。
- 即使一切都工作正常，有时你也可能会遇到一些因为时序（timing）问题导致的极端情况。

因此，尽管法定人数似乎保证读取返回最新的写入值，但在实践中并不那么简单。 Dynamo 风格的数据库通常针对可以忍受最终一致性的用例进行优化。你可以通过参数 w 和 r 来调整读取到陈旧值的概率，但把它们当成绝对的保证是不明智的。

#### 监控陈旧度

从运维的角度来说，监控你的数据库是否返回最新的结果很重要。即使你的应用程序可以容忍陈旧的读取，你也需要了解副本节点的健康状况。如果显著落后了，它应该提醒你，方便你进行原因的调查（网络问题或节点过载了）。

对于基于领导者的复制，数据库通常会公开复制延迟的指标，您可以将其输入到监控系统中。这是可能的，因为写入操作以相同的顺序应用于领导者和追随者，并且每个节点在复制日志中都有一个位置（它在本地应用的写入数量）。通过将追随者的当前位置减去领导者的当前位置，您可以衡量复制延迟的量。

然而，在无主复制的系统中，没有固定的写入顺序，这使得监控变得更加困难。而且，如果数据库只使用读修复（没有反熵过程），那么对于一个值可能会有多陈旧其实是没有限制的 - 如果一个值很少被读取，那么由一个陈旧副本返回的值可能是古老的。

已经有一些关于衡量无主复制数据库中的复制陈旧度的研究，并根据参数 n、w 和 r 来预测陈旧读取的预期百分比。不幸的是，这还不是很常见的做法，但是将陈旧测量值包含在数据库的标准度量集中是一件好事。虽然最终一致性是一种有意模糊的保证，但是从可操作性角度来说，能够量化 “最终” 也是很重要的。

### 宽松的法定人数与提示移交

Sloppy Quorums and Hinted Handoff

合理配置的法定人数可以使数据库无需故障切换即可容忍个别节点的故障。它也可以容忍个别节点变慢，因为请求不必等待所有 n 个节点响应 —— 当 w 或 r 个节点响应时它们就可以返回。对于需要高可用、低延时、且能够容忍偶尔读到陈旧值的应用场景来说，这些特性使无主复制的数据库很有吸引力。

然而，法定人数（截至目前所描述的）并没有达到其应有的容错能力。网络中断很容易将客户端与大量数据库节点隔离。但是那些节点仍然活着，其他客户端可能能连接到它们，但对于被数据库节点隔离的客户端而言，它们可能就像死了一样。在这种情况下，很可能剩余的可访问节点少于 w 或 r，因此客户端无法再达到法定人数。

在一个大型集群中（节点数量显著多于 n），客户端在网络中断期间很可能能够连接到一些另外的数据库节点，但无法连接到它需要的用于组成特定值的法定人数的节点。在这种情况下，数据库设计人员面临一个权衡：

- 对于所有无法达到 w 或 r 个节点法定人数的请求，是否返回错误是更好的？
- 还是应该无论如何都接受写入，并将它们写入一些可访问的节点，即使这些节点不在通常用于满足法定节点个数 n 的节点中？

后者被认为是一个 **宽松的法定人数（sloppy quorum）** ：写和读仍然需要 w 和 r 个成功的响应，但这些响应可能来自不在指定的 n 个 “主” 节点中的其它节点。就好比说，如果你把自己锁在房子外面了，你可能会去敲开邻居的门，问是否可以暂时呆在他们的沙发上。

一旦网络中断得到解决，一个节点代表另一个节点临时接受的任何写入都将被发送到适当的 “主” 节点。这就是所谓的 **提示移交（hinted handoff）**（一旦你再次找到你的房子的钥匙，你的邻居可以礼貌地要求你离开沙发回家）。

宽松的法定人数对写入可用性的提高特别有用：只要有任何 w 个节点可用，数据库就可以接受写入。然而，这意味着即使当 w+r>n 时，也不能确保读取到某个键的最新值，因为最新的值可能已经临时写入了 n 之外的某些节点。

因此，在传统意义上，宽松的法定人数实际上并不是法定人数。它只是一个持久性的保证，即数据已存储在某处的 w 个节点。但不能保证 r 个节点的读取能看到它，除非提示移交已经完成。

在所有常见的 Dynamo 实现中，宽松的法定人数是可选的。在 Riak 中，它们默认是启用的，而在 Cassandra 和 Voldemort 中它们默认是禁用的。

### 检测并发写入

Dynamo 风格的数据库允许多个客户端同时写入相同的键（Key），这意味着即使使用严格的法定人数也会发生冲突。这个情况类似于多领导者复制，而且冲突也可能在 **读取修复** 或 **提示移交** 期间发生。

其问题在于，由于可变的网络延迟和部分节点的故障，事件可能以不同的顺序到达不同的节点。如下图，显示了两个客户机 A 和 B 同时写入三节点数据存储中的键 X：

- 节点 1 接收来自 A 的写入，但由于暂时中断，未接收到来自 B 的写入。
- 节点 2 首先接收来自 A 的写入，然后接收来自 B 的写入。
- 节点 3 首先接收来自 B 的写入，然后从 A 写入。

![img](/imgs/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE-%E5%A4%8D%E5%88%B6/fig5-12.png)

如果每个节点只要接收到来自客户端的写入请求就简单地覆写某个键值，那么节点就会永久地不一致，如上图中的最终获取请求所示：节点 2 认为 X 的最终值是 B，而其他节点认为值是 A 。

为了最终达到一致性，副本应该趋于相同的值。有人可能希望复制的数据库能够自动处理，但不幸的是，大多数的实现都很糟糕

上文的 处理冲突写入 小节，已经建议介绍了一些解决冲突的技术。下面更详细的介绍：

#### 最后写入胜利（丢弃并发写入）

实现最终收敛的一种方法是声明每个副本只需要存储 **“最近”** 的值，并允许 **“更旧”** 的值被覆盖和抛弃。然后，只要我们有一种明确的方式来确定哪个写是 “最近的”，并且每个写入最终都被复制到每个副本，那么复制最终会收敛到相同的值。

正如 **“最近”** 的引号所表明的，这个想法其实颇具误导性。在上图的例子中，当客户端向数据库节点发送写入请求时，两个客户端都不知道另一个客户端，因此不清楚哪一个先发送请求。事实上，说这两种情况谁先发送请求是没有意义的：既然我们说写入是 **并发（concurrent）** 的，那么它们的顺序就是不确定的。

即使写入没有自然的排序，我们也可以强制进行排序。例如，可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 **“最近的”**，并丢弃具有较早时间戳的任何写入。这种冲突解决算法被称为 **最后写入胜利（LWW, last write wins）**，是 Cassandra 唯一支持的冲突解决方法，也是 Riak 中的一个可选特征。

LWW 实现了最终收敛的目标，但以 **持久性** 为代价：如果同一个键有多个并发写入，即使它们反馈给客户端的结果都是成功的（因为它们被写入 w 个副本），也只有一个写入将被保留，而其他写入将被默默地丢弃。此外，LWW 甚至可能会丢弃不是并发的写入。

在类似缓存的一些情况下，写入丢失可能是可以接受的。但如果数据丢失不可接受，LWW 是解决冲突的一个很烂的选择。

在数据库中使用 LWW 的唯一安全方法是确保一个键只写入一次，然后视为不可变，从而避免对同一个键进行并发更新。例如，Cassandra 推荐使用的方法是使用 UUID 作为键，从而为每个写操作提供一个唯一的**键**。

